[
  {
    "objectID": "02.2_dataset.html",
    "href": "02.2_dataset.html",
    "title": "Smartphone and smartwatch HAR dataset",
    "section": "",
    "text": "Description of the data collection process",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Smartphone and smartwatch HAR dataset</span>"
    ]
  },
  {
    "objectID": "02.2_dataset.html#description-of-the-data-collection-process",
    "href": "02.2_dataset.html#description-of-the-data-collection-process",
    "title": "Smartphone and smartwatch HAR dataset",
    "section": "",
    "text": "Subjects\nTable 4.1 shows the details of the subjects and the number of activity sequences executed. Twenty-three physically healthy, white caucasian subjects (thirteen male, ten female) voluntarily participated in the data collection procedure. The age of the participants ranged from \\(23\\) to \\(66\\) years old (\\(\\mu = 44.3 \\pm 14.3\\)), where the ratio of male/female participants was \\(56\\%/44\\%\\) (Table 4.2 and Table 4.3). Informed written consent was obtained from all participants, and the data collection was approved by the ethics committee of the Universitat Jaume I (reference No. CD/88/2022) and carried out in accordance with the Declaration of Helsinki.\n\n\nCode\nsubjects_info = load_subjects_info()\nsubjects_info\n\n\n\n\nTable 4.1: Subject’s information\n\n\n\n\n\n\n\n\n\n\nsubject_id\nage\ngender\nheight\nweight\ndominant_hand\nexecutions\n\n\n\n\n0\ns01\n54\nM\n190\n83\nR\n6\n\n\n1\ns02\n31\nM\n171\n71\nR\n9\n\n\n2\ns03\n24\nF\n161\n62\nR\n10\n\n\n3\ns04\n51\nM\n174\n60\nR\n10\n\n\n4\ns05\n54\nM\n172\n85\nR\n10\n\n\n5\ns06\n53\nM\n179\n110\nR\n10\n\n\n6\ns07\n49\nM\n176\n88\nR\n11\n\n\n7\ns08\n63\nM\n165\n89\nR\n9\n\n\n8\ns09\n28\nF\n164\n49\nR\n10\n\n\n9\ns10\n66\nF\n165\n72\nR\n10\n\n\n10\ns11\n50\nM\n181\n70\nR\n10\n\n\n11\ns12\n46\nM\n181\n90\nR\n10\n\n\n12\ns13\n26\nM\n170\n65\nR\n10\n\n\n13\ns14\n34\nM\n170\n65\nR\n10\n\n\n14\ns15\n23\nF\n166\n60\nR\n10\n\n\n15\ns16\n25\nM\n173\n64\nL\n10\n\n\n16\ns17\n58\nF\n156\n53\nR\n10\n\n\n17\ns18\n61\nM\n172\n97\nR\n10\n\n\n18\ns19\n30\nF\n160\n58\nR\n10\n\n\n19\ns20\n58\nF\n160\n60\nR\n10\n\n\n20\ns21\n56\nF\n160\n55\nR\n10\n\n\n21\ns22\n31\nF\n162\n70\nR\n9\n\n\n22\ns23\n48\nF\n174\n78\nR\n9\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsubjects_age_range(subjects_info)\n\n\n\n\nTable 4.2: Subject’s statistics\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nage\n23.0\n44.304348\n14.293784\n23.0\n30.5\n49.0\n55.0\n66.0\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsubjects_age_range_by_gender(subjects_info)\n\n\n\n\nTable 4.3: Subject’s statistics by gender\n\n\n\n\n\n\n\n\n\n\nage\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ngender\n\n\n\n\n\n\n\n\n\n\n\n\nF\n10.0\n42.200000\n16.551603\n23.0\n28.5\n39.5\n57.5\n66.0\n\n\nM\n13.0\n45.923077\n12.750566\n25.0\n34.0\n50.0\n54.0\n63.0\n\n\n\n\n\n\n\n\n\n\n\n\nDevices\nA Xiaomi Poco X3 Pro smartphone (M2102J20SG) and a TicWatch Pro 3 GPS smartwatch (WH12018), both equipped with an STMicroelectronics LSM6DSO IMU sensor 1, were used to collect accelerometer and gyroscope data. The devices had a custom application installed —smartphone app (Matey-Sanz and González-Pérez 2022a), smartwatch app (Matey-Sanz and González-Pérez 2022b)— to collect sensor samples at \\(100\\)Hz. These apps were developed using the libraries described in Data collection libraries on top of the AwarNS Framework. The smartwatch was worn on the left wrist; the smartphone was carried in the front left trousers pocket, letting the participants choose the device orientation in their pockets (see Figure 4.1).\n1 IMU Specification\n\nSmartphone specs:\n\nAccelerometer: range (\\(\\pm8g\\)), accuracy (\\(\\pm0.02g\\))\nGyroscope: range (\\(\\pm1000dpi\\)), accuracy (\\(\\pm1dpi\\))\n\nSmartwatch specs:\n\nAccelerometer: range (\\(\\pm8g\\)), accuracy (\\(\\pm0.02g\\))\nGyroscope: range (\\(\\pm2000dpi\\)), accuracy (\\(\\pm1dpi\\))\n\n\n\n\n\n\n\n\nFigure 4.1: Different orientations of the smartphone placed in the pocket\n\n\n\nAnother device, a Xiaomi Poco F2 Pro smartphone (M2004J11G), was used to video-record the subjects while performing the data collection procedure at \\(60\\) frames per second for data labelling (i.e. ground truth) purposes.\n\n\nCollection environment\nThe data collection was executed in a research laboratory at Universitat Jaume I. An obstacle-free, three-meter-long and two-meter-wide area with a flat ceramic floor and a combination of natural and artificial light was prepared to carry out the collection.\nAn armless chair was placed in one longitudinal extreme of the area and a visible floor mark was put in the opposite extreme. Thus, the chair and the floor mark were separated by three meters.\nThe environment was only occupied by a participant and a researcher to avoid any distraction or interference during the data collection. In addition to the smartphone used to video-record the collection and the personal devices of the participant, no other devices were enabled in the environment that could interfere with the data collection process.\n\n\nExperimental procedure\nEach participant was asked to perform a specific sequence of activities that corresponds with the TUG test – a well-known mobility test typically used for fall risk assessment (Podsiadlo and Richardson 1991). The test starts from a seated position on a chair and each subject was then instructed to perform the following sequence of activities: standing up from the chair, walking three meters (indicated with a mark on the ground), turning around (\\(180º\\)), walking back to the chair, turning around (\\(180º\\)), and sitting down on the chair. The participants were free to choose the direction of their turns (i.e., left or right). In summary, five unique activities were performed: SEATED, STANDING_UP, WALKING, TURNING and SITTING_DOWN.\nEach subject was instructed to perform the sequence of activities ten times, although some sequence executions were discarded due to non-compliance with the procedure (e.g., incorrect start of data collection, poor sequence execution, etc.). A total amount of \\(223\\) executions (Table 4.4) compose the dataset.\nEach activity sequence was video-recorded by a researcher. Then, each video was manually analyzed at frame level to determine the transitions between the executed activities and label the collected samples with the corresponding activity to establish the groundtruth.\n\n\nCode\nexecutions_by_gender(subjects_info)\n\n\n\n\nTable 4.4: Total amount of executions\n\n\n\n\n\n\n\n\n\ngender\nF\nM\nTotal\n\n\n\n\nexecutions\n98\n125\n223",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Smartphone and smartwatch HAR dataset</span>"
    ]
  },
  {
    "objectID": "02.2_dataset.html#collected-dataset",
    "href": "02.2_dataset.html#collected-dataset",
    "title": "Smartphone and smartwatch HAR dataset",
    "section": "Collected dataset",
    "text": "Collected dataset\n\nData description\n\n\nCode\ndata = load_data()\n\n\nThe collected dataset contains raw (i.e., no preprocessing steps applied) accelerometer and gyroscope samples from a smartphone and a smartwatch labelled with a certain human activity. Even though the devices’ clocks are synchronised, the samples of each device are not synchronized with each other since the data collection on both devices are independent processes. In other words, a data sample at timestamp \\(X\\) in the smartphone data might not have an equivalent sample exactly at the same timestamp \\(X\\) in the smartwatch data.\nThe dataset is organized in CSV files named using the XX_YY_DEV.csv pattern, where XX is the id of the subject, YY is the execution number and DEV is the device used to collect the data contained in the file (i.e., sp or sw). Then, each row of the CSV file contains an accelerometer and gyroscope sample labelled with an activity and annotated with a timestamp.\nTable 4.5 contains the number of collected samples for each activity. Even though the sampling rate used in the data collection applications was set to \\(100\\)Hz, Android applications are not always able to apply the requested sampling rate, resulting in an average sampling rate of \\(102\\)Hz for the smartphone and \\(104\\)Hz for the smartwatch.\n\n\nCode\ncount_samples(data)\n\n\n\n\nTable 4.5: Number of collected samples\n\n\n\n\n\n\n\n\n\n\nSEATED\nSTANDING_UP\nWALKING\nTURNING\nSITTING_DOWN\nTOTAL\n\n\n\n\nsp\n32764\n27303\n115069\n52209\n31868\n259213\n\n\nsw\n32025\n27765\n117126\n53180\n32457\n262553\n\n\n\n\n\n\n\n\n\n\nAs an example of the type of data captured for one subject, Figure 4.2 and Figure 4.3 show a plot of the accelerometer and gyroscope samples collected respectively from the smartphone and the smartwatch by the subject s16 on his first execution (i.e., files s16_01_sp.csv and s16_01_sw.csv).\n\n\nCode\ns16_01_sp = plot_execution(data, 's16_01_sp')\ns16_01_sp.show()\n\n\n\n\n                                                \n\n\nFigure 4.2: Sample of smartphone collected accelerometer (top) and gyroscope (bottom) data.\n\n\n\n\n\n\nCode\ns16_01_sw = plot_execution(data, 's16_01_sw')\ns16_01_sw.show()\n\n\n\n\n                                                \n\n\nFigure 4.3: Sample of smartwatch collected accelerometer (top) and gyroscope (bottom) data.\n\n\n\n\nFinally, Table 4.6 contains information about each execution. In particular, it contains the phone orientation (see Figure 4.1) and the turning direction (left or right) for each execution.\n\n\nCode\nfrom itables import show\n\nexecutions_info = load_executions_info()\nshow(executions_info)\n\n\n\n\nTable 4.6: Metadata of each execution.\n\n\n\n\n\n    \n      \n      execution_id\n      orientation\n      first_turn\n      second_turn\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef verbatim(x):\n    return f'Verb|{x}|'\n\nwith open('executions_info_latex', 'w') as f:\n    executions_info.to_latex(f, formatters={'execution_id': verbatim, 'orientation': verbatim, 'first_turn': verbatim, 'second_turn': verbatim})\n\n\nFrom the above table, some statistics can be extracted, such as the amount of times each orientation has been employed (Figure 4.4), the number of right and left turns in first_turn and second_turn (Figure 4.5), or the direction of the first_turn and second_turn as a sequence (Figure 4.6).\n\n\nCode\nplot_orientation_stats(executions_info)\n\n\n\n\n                                                \n\n\nFigure 4.4: Phone orientation in the pocket.\n\n\n\n\n\n\nCode\nplot_turn_direction_stats(executions_info)\n\n\n\n\n                                                \n\n\nFigure 4.5: Direction of turns in the first_turn and second_turn activities.\n\n\n\n\n\n\nCode\nplot_turn_direction_combined_stats(executions_info)\n\n\n\n\n                                                \n\n\nFigure 4.6: Combined direction of turns in each sequence.\n\n\n\n\n\n\nLimitations\nThe main technical limitation of the data described in this section resides in the data labelling procedure. Data labelling was performed by visual inspection of videos recorded at \\(60\\) frames per second, which implies that the time resolution of the video was \\(16.6\\)ms. However, due to hardware limitations, sometimes two adjacent frames were repeated, reducing the time resolution to \\(33.2\\)ms in specific time frames. On the other hand, the resolution of the sensors used for data collection was about \\(10\\)ms. Due to this resolution mismatch, there is a possible drift of up to three sensor samples, compared to the video recording. Therefore, some samples collected during the transition between activities might be mislabeled (e.g., a WALKING sample might be mislabelled as a TURNING sample).\nIn addition, unintentional errors could have been introduced during the manual video-recording inspection and corresponding labelling process. Concerning the sampling rate, we note some minor variability which is imposed by the Android operating system and thus represents a real-world data collection process.\nFinally, while user heterogeneity regarding age and gender was ensured, there is an imbalance in handedness with a majority (\\(22\\) out of \\(23\\)) of participants being right-handed.",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Smartphone and smartwatch HAR dataset</span>"
    ]
  },
  {
    "objectID": "02.2_dataset.html#comparison-with-other-datasets",
    "href": "02.2_dataset.html#comparison-with-other-datasets",
    "title": "Smartphone and smartwatch HAR dataset",
    "section": "Comparison with other datasets",
    "text": "Comparison with other datasets\nTable 4.7 compares the datasets taking into account the number of activities, the number, age and gender distribution of subjects, and the number of different devices employed in the data collection, and the device position in subjects’ body. In terms of the number of activities, the collected dataset is limited compared to ExtraSensory, WISDM and DOMINO, being its weakest point of comparison. ExtraSensory contains up to \\(10\\) times more activities since the authors performed a data collection process in an uncontrolled manner because data labelling was delegated to participating users.\nRegarding the subjects, the collected dataset presents the most variate sample in terms of age, and similar gender balance as ExtraSensory and RealWorld, while having a decent amount of participants, but still far from ExtraSensory and WISDM numbers. Finally, the collected dataset is also limited in terms of devices since we only used a smartphone and a smartwatch for collecting data, while several device models were used in HHAR (four smartphones and two smartwatch models), ExtraSensory (fifteen smartphone models, since each participant used its own smartphone) and WISDM (two smartphone models).\n\n\n\nTable 4.7: Comparison with related datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nActivities\nSubjects\nAge range\nGender\nDevices\nPositionon body\n\n\n\n\nHHAR\n5\n9\n25-30—\n—\n4 smartphones2 smartwatches\nWaist beltArm\n\n\nRealWorld\n8\n15\n—(31.9 \\(\\pm\\) 12.4)\n53% male47% female\n6 smartphones1 smartwatch\nScattered2Wrist\n\n\nExtraSensory\n51\n60\n18-42(24.7 \\(\\pm\\) 5.6)\n44% male56% female\n15 smartphones1 smartwatch\n—Wrist\n\n\nWISDM\n18\n52\n18-25—\n—\n2 smartphones1 smartwatch\nPocketWrist\n\n\nDomino\n14\n25\n20-59(26.6 \\(\\pm\\) 9.8)\n68% female32% male\n1 smartphone1 smartwatch\nPocketWrist\n\n\nCollecteddataset\n5\n23\n23-66(44.3 \\(\\pm\\) 14.3)\n56% male44% female\n1 smartphone1 smartwatch\nPocketWrist\n\n\n\n2 Head, chest, upper arm, waist, thigh and shin.",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Smartphone and smartwatch HAR dataset</span>"
    ]
  },
  {
    "objectID": "02.2_dataset.html#code-reference",
    "href": "02.2_dataset.html#code-reference",
    "title": "Smartphone and smartwatch HAR dataset",
    "section": "Code reference",
    "text": "Code reference\n\n\n\n\n\n\nTip\n\n\n\nThe documentation of the Python functions employed in this section can be found in Chapter 2 reference:\n\ndata_loading:\n\nload_data\nload_executions_info\nload_subjects_info\n\nexploration:\n\ncount_samples\nexecutions_by_gender\nsubjects_age_range\nsubjects_age_range_by_gender\n\nvisualization:\n\nplot_execution\nplot_orientation_stats\nplot_turn_direction_stats\nplot_turn_direction_combined_stats\n\n\n\n\n\n\n\n\nArrotta, Luca, Gabriele Civitarese, Riccardo Presotto, and Claudio Bettini. 2023. “DOMINO: A Dataset for Context-Aware Human Activity Recognition Using Mobile Devices.” In 2023 24th IEEE International Conference on Mobile Data Management (MDM), 346–51. IEEE. https://doi.org/10.1109/MDM58254.2023.00063.\n\n\nMatey-Sanz, Miguel, Sven Casteleyn, and Carlos Granell. 2023a. “Dataset of Inertial Measurements of Smartphones and Smartwatches for Human Activity Recognition.” Data in Brief 51: 109809. https://doi.org/10.1016/j.dib.2023.109809.\n\n\n———. 2023b. “Smartphone and smartwatch inertial measurements from heterogeneous subjects for human activity recognition.” Zenodo. https://doi.org/10.5281/zenodo.8398688.\n\n\nMatey-Sanz, Miguel, and Alberto González-Pérez. 2022a. “TUG Test Smartphone Application.” Zenodo. https://doi.org/10.5281/zenodo.7456835.\n\n\n———. 2022b. “TUG Test Smartwatch Application.” Zenodo. https://doi.org/10.5281/zenodo.7457098.\n\n\nPodsiadlo, Diane, and Sandra Richardson. 1991. “The Timed ‘up & Go’: A Test of Basic Functional Mobility for Frail Elderly Persons.” Journal of the American Geriatrics Society 39 (2): 142–48. https://doi.org/10.1111/j.1532-5415.1991.tb01616.x.\n\n\nStisen, Allan, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. 2015. “Smart Devices Are Different: Assessing and Mitigating Mobile Sensing Heterogeneities for Activity Recognition.” In Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems, 127–40. https://doi.org/10.1145/2809695.2809718.\n\n\nSztyler, Timo, and Heiner Stuckenschmidt. 2016. “On-Body Localization of Wearable Devices: An Investigation of Position-Aware Activity Recognition.” In 2016 IEEE International Conference on Pervasive Computing and Communications (PerCom), 1–9. https://doi.org/10.1109/PERCOM.2016.7456521.\n\n\nVaizman, Yonatan, Katherine Ellis, and Gert Lanckriet. 2017. “Recognizing Detailed Human Context in the Wild from Smartphones and Smartwatches.” IEEE Pervasive Computing 16 (4): 62–74. https://doi.org/10.1109/MPRV.2017.3971131.\n\n\nWeiss, Gary M, Kenichi Yoneda, and Thaier Hayajneh. 2019. “Smartphone and Smartwatch-Based Biometrics Using Activities of Daily Living.” IEEE Access 7: 133190–202. https://doi.org/10.1109/ACCESS.2019.2940729.",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Smartphone and smartwatch HAR dataset</span>"
    ]
  },
  {
    "objectID": "03.1_training-data.html",
    "href": "03.1_training-data.html",
    "title": "Impact of the amount of training data",
    "section": "",
    "text": "Visual inspection\nFigure 6.1, 6.2, 6.3, 6.4 show the accuracy evolution regarding the increasing training set size in the MLP, CNN, LSTM and CNN-LSTM models, respectively.\nIn every model, a clear tendency can be observed: the models trained with smartphone or fused data present a drastic increase of the model accuracy with low amounts of data, i.e., \\(n \\leq 7\\). On the other side, the models trained with smartwatch data have a smoother evolution.\nAnother tendency is observed with medium amounts of data, i.e., \\(8 \\leq n \\leq 12\\): the evolution in all models and data sources tends to stabilize with very slight improvements in accuracy. In the case of the models trained with smartphone and the fused data, this stabilization threshold is around \\(n \\in [10,11]\\), while with the smartwatch-trained models the threshold is a bit lower, around \\(n \\in [8,9]\\).\nIt can also be observed that the models trained with the smartwatch data provide the most stable results, having the lowest SD and range, except in the MLP model, where the smartphone and fused data obtain better results. In general, the evolution tendency shown by the smartphone and the fused data is similar, although the fused data obtains slightly better results in all metrics.\nCode\nplot_evolution(\n    reports, \n    SOURCES, \n    Filter(Model.MLP, None, TargetFilter.MODEL, ModelMetric.ACCURACY),\n    EVOLUTION_PLOT_TITLES\n)\n\n\n\n\n                                                \n\n\nFigure 6.1: MLP accuracy evolution for smartphone, smartwatch and fused data sources.\nCode\nplot_evolution(\n    reports, \n    SOURCES, \n    Filter(Model.CNN, None, TargetFilter.MODEL, ModelMetric.ACCURACY),\n    EVOLUTION_PLOT_TITLES\n)\n\n\n\n\n                                                \n\n\nFigure 6.2: CNN accuracy evolution for smartphone, smartwatch and fused data sources.\nCode\nplot_evolution(\n    reports, \n    SOURCES, \n    Filter(Model.LSTM, None, TargetFilter.MODEL, ModelMetric.ACCURACY),\n    EVOLUTION_PLOT_TITLES\n)\n\n\n\n\n                                                \n\n\nFigure 6.3: LSTM accuracy evolution for smartphone, smartwatch and fused data sources.\nCode\nplot_evolution(\n    reports, \n    SOURCES, \n    Filter(Model.CNN_LSTM, None, TargetFilter.MODEL, ModelMetric.ACCURACY),\n    EVOLUTION_PLOT_TITLES\n)\n\n\n\n\n                                                \n\n\nFigure 6.4: CNN-LSTM accuracy evolution for smartphone, smartwatch and fused data sources.\nRegarding individual activities, Figure 6.5, 6.6, 6.7, 6.8, 6.9 show the F1-score evolution produced by increasing the training set size in the SEATED, STANDING_UP, WALKING, TURNING and SITTING_DOWN activities, respectively.\nIn the SEATED activity, the bad performance of the models trained with smartphone data stands out. These models show F1-scores \\(0\\) or close to \\(0\\) with any amount of data, except the CNN-LSTM model. This behaviour is also reproduced in the STANDING_UP and SITTING_DOWN activities. In these activities, the bad performance also transfers to the models trained with fused data when few data is used for training, but in part mitigated with higher amounts of data. However, in any case, the models trained with smartwatch data show a better evolution for these activities. Notwithstanding, while the metrics of the models in the SEATED activity are quite stable, for the STANDING_UP and SITTING_DOWN activities the evolution curve in the smartphone and fused data suggests the evaluation metrics could have been improved with more data.\nIn the WALKING and TURNING activities, it is remarkable the enormous evolution with few data (\\(n \\leq 3\\)) in the smartphone and fused models compared with the other activities. For instance, to reach similar performance levels (F1-score \\(\\geq 0.6\\)), the SEATED activity requires \\(n \\geq 7\\), or the STANDING_UP and SITTING_DOWN activities \\(n \\geq 9\\). This rapid evolution contrasts with the small increase in the evolution on the smartwatch-trained models, which already perform well with very few data. It is also noticeable the good performance of the smartphone- and fused-trained models with the highest amounts of data, which improve the models trained with smartwatch data. This clearly contrasts with the other activities, where the smartwatch-trained models show the best performance.\nCode\nplot_comparison(\n    reports, \n    MODELS,\n    SOURCES, \n    Filter(None, None, TargetFilter.SEATED, ActivityMetric.F1), \n    SOURCES_PRINT\n)\n\n\n\n\n                                                \n\n\nFigure 6.5: F1-score evolution of the SEATED activity across models and data sources.\nCode\nplot_comparison(\n    reports, \n    MODELS,\n    SOURCES, \n    Filter(None, None, TargetFilter.STANDING_UP, ActivityMetric.F1), \n    SOURCES_PRINT\n)\n\n\n\n\n                                                \n\n\nFigure 6.6: F1-score evolution of the STANDING_UP activity across models and data sources.\nCode\nplot_comparison(\n    reports, \n    MODELS,\n    SOURCES, \n    Filter(None, None, TargetFilter.WALKING, ActivityMetric.F1), \n    SOURCES_PRINT\n)\n\n\n\n\n                                                \n\n\nFigure 6.7: F1-score evolution of the WALKING activity across models and data sources.\nCode\nplot_comparison(\n    reports, \n    MODELS,\n    SOURCES, \n    Filter(None, None, TargetFilter.TURNING, ActivityMetric.F1), \n    SOURCES_PRINT\n)\n\n\n\n\n                                                \n\n\nFigure 6.8: F1-score evolution of the TURNING activity across models and data sources.\nCode\nplot_comparison(\n    reports, \n    MODELS,\n    SOURCES, \n    Filter(None, None, TargetFilter.SITTING_DOWN, ActivityMetric.F1), \n    SOURCES_PRINT\n)\n\n\n\n\n                                                \n\n\nFigure 6.9: F1-score evolution of the SITTING_DOWN activity across models and data sources.",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Impact of the amount of training data</span>"
    ]
  },
  {
    "objectID": "03.1_training-data.html#statistical-analysis",
    "href": "03.1_training-data.html#statistical-analysis",
    "title": "Impact of the amount of training data",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nFigure 6.10, 6.11, 6.12, 6.13 depict the statistical tests executed to determine the significance of the difference in accuracy caused by increasing training data in the MLP, CNN, LSTM and CNN-LSTM models respectively. In practice, the accuracy obtained from training models with \\(n_1\\) subjects is compared with the accuracies obtained in models trained with \\(n_2\\) subjects, where \\(n_2 &gt; n_1\\) (i.e., does the accuracy statistically improve when training with more subjects?).\n\n\n\n\n\n\n\n\nNote\n\n\n\nPrior to executing the statistical tests, the normality of the data was tested in order to use parametric or non-parametric tests. The results determined the non-normality of the data. The normality was tested using the is_parametric_data.\n\n\nOverall, when the training data increases the accuracy increases too (with some isolated exceptions) in any model and data source, although this increase is not always significant. In every model and data source, improvements of the type \\(n_1 &lt; n_2 + 1\\) (i.e., adding the data of one subject more) start to be not significant around \\(n_1 \\in [4,6]\\).\nThen, significant improvements seem to appear after 1) a certain threshold and 2) a specific amount of extra data (i.e., \\(n_1 &lt; n_2 + x\\)). The first case is observed in the CNN and LSTM models. In both models, improvements are obtained when \\(n_1 \\in [9,12]\\) and \\(n_2 \\in [13,14]\\) across all data sources. In the MLP, there are improvements on \\(n_2 = 22\\) when \\(n_1 \\in [14,16]\\) and \\([17,18]\\) with smartwatch and fused data, respectively; in the LSTM models, when \\(n_1 \\in [14,16]\\) improvements are observed when \\(n_2 = 20\\) with any data source; and in the CNN-LSTM model, when \\(n_1 \\in [16,18]\\) improvements appear in \\(n_2 = 22\\) with the fused data.\nExamples of the second case can be noticed in the CNN model with smartphone and smartwatch data, where improvements to \\(n_1 \\in [15, 17]\\) and \\(n_1 \\in [14,16]\\), respectively, require the addition of data from \\(5\\) subjects. The CNN-LSTM model also shows this behaviour with smartphone data, where improvements to \\(n_1 \\in [9, 17]\\) require the addition of data from \\([3,5]\\) subjects; the smartwatch, which requires data from \\([4,6]\\) subjects to obtain improvements when \\(n_1 \\in [9,16]\\); and with fused data, where improvements to \\(n_1 \\in [9,13]\\) are obtained after adding data from \\(2\\) subjects.\nFinally, no significant changes are observed after a certain range of values for \\(n_1\\) is reached. In addition, these values seem constant across model architectures. For example, in models trained with smartphone data, no differences are observed after \\(n_1 \\geq [17, 18]\\). In the case of the smartwatch data, there are no improvements after \\(n_1 \\geq 17\\). These values are a bit higher for the models trained with fused data, around \\(n_1 \\in [19,20]\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe following figures are the visual representation of the output from pairwise_n_comparision.\n\n\n\n\nCode\nplot_pairwise_comparision(\n    reports, \n    SOURCES, \n    Filter(Model.MLP, None, TargetFilter.MODEL, ModelMetric.ACCURACY), \n    SOURCES_PRINT, \n    stars=True, \n    parametric=False, \n    alternative='two-sided'\n)\n\n\n\n\n                                                \n\n\nFigure 6.10: Statistical comparison using MWU tests over MLP models accuracy with smartphone, smartwatch and fused data sources.\n\n\n\n\n\n\nCode\nplot_pairwise_comparision(\n    reports, \n    SOURCES, \n    Filter(Model.CNN, None, TargetFilter.MODEL, ModelMetric.ACCURACY), \n    SOURCES_PRINT, \n    stars=True, \n    parametric=False, \n    alternative='two-sided'\n)\n\n\n\n\n                                                \n\n\nFigure 6.11: Statistical comparison using MWU tests over CNN models accuracy with smartphone, smartwatch and fused data sources.\n\n\n\n\n\n\nCode\nplot_pairwise_comparision(\n    reports, \n    SOURCES, \n    Filter(Model.LSTM, None, TargetFilter.MODEL, ModelMetric.ACCURACY), \n    SOURCES_PRINT, \n    stars=True, \n    parametric=False, \n    alternative='two-sided'\n)\n\n\n\n\n                                                \n\n\nFigure 6.12: Statistical comparison using MWU tests over LSTM models accuracy with smartphone, smartwatch and fused data sources.\n\n\n\n\n\n\nCode\nplot_pairwise_comparision(\n    reports, \n    SOURCES, \n    Filter(Model.CNN_LSTM, None, TargetFilter.MODEL, ModelMetric.ACCURACY), \n    SOURCES_PRINT, \n    stars=True, \n    parametric=False, \n    alternative='two-sided'\n)\n\n\n\n\n                                                \n\n\nFigure 6.13: Statistical comparison using MWU tests over CNN-LSTM models accuracy with smartphone, smartwatch and fused data sources.",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Impact of the amount of training data</span>"
    ]
  },
  {
    "objectID": "03.1_training-data.html#summary",
    "href": "03.1_training-data.html#summary",
    "title": "Impact of the amount of training data",
    "section": "Summary",
    "text": "Summary\nThe presented analyses on the effect of the training set size go beyond the existing related works in the literature. We statistically analysed how the models’ performance evolves regarding the amount of data while also comparing these evolutions with several data sources and taking into account activity-wise performance.\nA first visual inspection clearly showed a fast evolution in the models’ accuracy and activities F1-scores with a low amount of data going towards stabilization with higher amounts of data. In addition, this evolution was more abrupt for the models trained with smartphone and fused data since their initial performance was much lower than that of the smartwatch-trained models (i.e., they had more room for improvement). Moreover, a threshold was identified around \\(n \\in [10,11]\\) for smartphone- and fused-trained models, and \\(n \\in [8,9]\\) for smartwatch-trained models, in which the improvement speed of the models seemed to stabilize.\nThese insights were confirmed after the statistical analysis of the models’ evolutions. We observed that with \\(n \\in [4,6]\\) the addition of data from one subject did not involve significant improvements. This lack of improvement was further deepened for \\(n \\geq 9\\), where the addition of several subjects’ data was required for significant improvements.\nWe also identified that significant improvements after this threshold appear at a later threshold(s) or after the addition of specific amounts of data and that this behaviour depends on the data source and the model architecture. For instance, MLP showed improvements at \\(n=17\\) with the smartwatch dataset and the LSTM at \\(n=14\\) with the fused dataset, while the CNN and CNN-LSTM obtained improvements at certain thresholds or after the addition of the data from \\(5\\) (CNN with smartphone or smartwatch datasets), \\([4,6]\\) (CNN-LSTM with smartwatch datasets), or \\(2\\) (CNN-LSTM with fused dataset) subjects.\nFinally, no significant improvements were consistently found across model architectures after using \\(n \\in [17,18]\\) for -trained models, \\(n = 17\\) for smartwatch-trained models and \\(n \\in [19,20]\\) for fused-trained models. However, this lack of improvement is probably caused by the limited amount of data, i.e., \\(n = 22\\).\nIn summary, the results have shown that the models improve fast when the available data is limited, but tend to stabilize with higher amounts of data. Among the selected models, the improvement on the MLP and the LSTM seems to be conditioned by reaching certain thresholds, while the CNN and CNN-LSTM can be constantly improved by the addition of specific amounts of data. Among the data sources, the smartwatch-trained models present a good performance from low amounts of data but show a slower evolution than the smartphone- and fused-trained models. In addition, with high amounts of data, the fused-trained models seem to require a smaller amount of data to keep improving.",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Impact of the amount of training data</span>"
    ]
  },
  {
    "objectID": "03.1_training-data.html#code-reference",
    "href": "03.1_training-data.html#code-reference",
    "title": "Impact of the amount of training data",
    "section": "Code reference",
    "text": "Code reference\n\n\n\n\n\n\nTip\n\n\n\nThe documentation of the Python functions employed in this section can be found in Chapter 3 reference:\n\ndata_loading:\n\nload_reports\n\nmodel:\n\nActivityMetric\nFilter\nModel\nModelMetric\nSource\nTargetFilter\n\nvisualization:\n\nplot_comparison\nplot_evolution\nplot_pairwise_comparision",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Impact of the amount of training data</span>"
    ]
  },
  {
    "objectID": "03.2_data-sources.html",
    "href": "03.2_data-sources.html",
    "title": "Statistical comparison of data sources performance",
    "section": "",
    "text": "Overall performance\nRegarding the accuracy of the models (Table 7.1), the smartwatch dataset always presents the best performance with few amounts of data (i.e., \\(n \\in [3, 4]\\)), while the fused dataset is the best with high amounts of data across all models. When comparing smartphone and smartwatch in the highest amounts of data, the smartphone is superior in the MLP models, while the smartwatch is better in the LSTM models, but no significant differences are appreciated in the CNN and CNN-LSTM models.\nThe post-hoc tests can be found in Table 7.2.\nCode\ndatasource_overlall_tests, datasource_overall_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.MODEL, ModelMetric.ACCURACY), \n    MODELS, \n    SOURCES\n)\ndatasource_overlall_tests\n\n\n\n\nTable 7.1: Statistical comparison of overall accuracies obtained by the data sources for each model.\n\n\n\n\n\n\n\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\n\n\n\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.202\n0.528\n0.242\n147.696\n0.000\n0.390\n0.678\n0.511\n111.888\n0.000\n0.283\n0.650\n0.437\n119.849\n0.000\n0.426\n0.624\n0.475\n67.193\n0.000\n\n\n2\n0.491\n0.679\n0.491\n71.194\n0.000\n0.659\n0.755\n0.716\n33.542\n0.000\n0.583\n0.728\n0.661\n47.641\n0.000\n0.682\n0.739\n0.654\n20.235\n0.000\n\n\n3\n0.609\n0.710\n0.624\n36.563\n0.000\n0.719\n0.788\n0.787\n36.940\n0.000\n0.665\n0.776\n0.739\n55.392\n0.000\n0.710\n0.774\n0.767\n15.792\n0.000\n\n\n4\n0.700\n0.736\n0.729\n10.771\n0.005\n0.776\n0.807\n0.821\n16.354\n0.000\n0.694\n0.791\n0.792\n45.746\n0.000\n0.759\n0.801\n0.799\n9.945\n0.007\n\n\n5\n0.743\n0.766\n0.759\n2.758\n0.252\n0.811\n0.829\n0.838\n15.710\n0.000\n0.753\n0.812\n0.796\n42.896\n0.000\n0.800\n0.816\n0.830\n14.262\n0.001\n\n\n6\n0.739\n0.777\n0.788\n8.468\n0.014\n0.798\n0.834\n0.849\n26.631\n0.000\n0.758\n0.829\n0.812\n61.456\n0.000\n0.793\n0.823\n0.839\n24.431\n0.000\n\n\n7\n0.782\n0.778\n0.795\n3.121\n0.210\n0.837\n0.838\n0.852\n13.777\n0.001\n0.771\n0.831\n0.821\n56.416\n0.000\n0.819\n0.823\n0.847\n16.876\n0.000\n\n\n8\n0.789\n0.795\n0.802\n4.698\n0.095\n0.836\n0.840\n0.858\n15.600\n0.000\n0.786\n0.842\n0.832\n47.311\n0.000\n0.821\n0.833\n0.852\n26.122\n0.000\n\n\n9\n0.813\n0.803\n0.819\n16.174\n0.000\n0.843\n0.857\n0.872\n18.848\n0.000\n0.813\n0.846\n0.849\n22.774\n0.000\n0.823\n0.852\n0.869\n36.807\n0.000\n\n\n10\n0.819\n0.807\n0.840\n24.966\n0.000\n0.854\n0.858\n0.875\n17.747\n0.000\n0.826\n0.851\n0.860\n24.270\n0.000\n0.838\n0.849\n0.874\n35.307\n0.000\n\n\n11\n0.825\n0.803\n0.840\n32.831\n0.000\n0.856\n0.854\n0.874\n21.060\n0.000\n0.826\n0.850\n0.861\n22.877\n0.000\n0.838\n0.849\n0.870\n32.273\n0.000\n\n\n12\n0.831\n0.803\n0.847\n48.143\n0.000\n0.855\n0.858\n0.877\n19.258\n0.000\n0.830\n0.852\n0.866\n24.382\n0.000\n0.835\n0.854\n0.883\n54.700\n0.000\n\n\n13\n0.837\n0.810\n0.849\n64.810\n0.000\n0.861\n0.859\n0.887\n34.245\n0.000\n0.846\n0.856\n0.867\n13.828\n0.001\n0.851\n0.856\n0.888\n66.836\n0.000\n\n\n14\n0.841\n0.815\n0.855\n62.088\n0.000\n0.869\n0.863\n0.889\n36.509\n0.000\n0.848\n0.861\n0.875\n22.951\n0.000\n0.854\n0.856\n0.886\n49.737\n0.000\n\n\n15\n0.842\n0.815\n0.861\n93.565\n0.000\n0.869\n0.864\n0.889\n32.564\n0.000\n0.850\n0.863\n0.875\n16.633\n0.000\n0.852\n0.858\n0.890\n72.258\n0.000\n\n\n16\n0.841\n0.814\n0.864\n88.822\n0.000\n0.870\n0.868\n0.890\n31.497\n0.000\n0.850\n0.866\n0.882\n33.638\n0.000\n0.853\n0.860\n0.894\n90.418\n0.000\n\n\n17\n0.851\n0.816\n0.867\n106.636\n0.000\n0.877\n0.867\n0.895\n36.744\n0.000\n0.855\n0.867\n0.878\n14.331\n0.001\n0.856\n0.861\n0.894\n75.250\n0.000\n\n\n18\n0.846\n0.815\n0.867\n107.590\n0.000\n0.877\n0.867\n0.896\n40.226\n0.000\n0.854\n0.869\n0.879\n17.394\n0.000\n0.858\n0.864\n0.894\n78.713\n0.000\n\n\n19\n0.856\n0.821\n0.872\n123.329\n0.000\n0.873\n0.869\n0.896\n40.944\n0.000\n0.855\n0.870\n0.884\n20.879\n0.000\n0.859\n0.868\n0.896\n87.531\n0.000\n\n\n20\n0.852\n0.817\n0.876\n136.192\n0.000\n0.880\n0.872\n0.898\n51.361\n0.000\n0.864\n0.872\n0.886\n32.094\n0.000\n0.859\n0.866\n0.897\n88.692\n0.000\n\n\n21\n0.854\n0.824\n0.873\n121.986\n0.000\n0.884\n0.871\n0.897\n46.619\n0.000\n0.864\n0.871\n0.885\n15.898\n0.000\n0.863\n0.867\n0.900\n84.458\n0.000\n\n\n22\n0.861\n0.822\n0.879\n144.736\n0.000\n0.886\n0.874\n0.902\n50.409\n0.000\n0.865\n0.875\n0.889\n27.551\n0.000\n0.868\n0.869\n0.901\n89.274\n0.000\nCode\nshow(datasource_overall_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 7.2: Post-hoc tests for Table 7.1 to determine the best-performant data sources.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical comparison of data sources performance</span>"
    ]
  },
  {
    "objectID": "03.2_data-sources.html#activitiy-wise-performance",
    "href": "03.2_data-sources.html#activitiy-wise-performance",
    "title": "Statistical comparison of data sources performance",
    "section": "Activitiy-wise performance",
    "text": "Activitiy-wise performance\nNext, we focus on how each data source affects the performance of individual activities with each selected model.\n\nSEATED\nResults from Table 7.3 and Table 7.4 (post-hoc) show that the smartwatch-trained models obtain the best scores for the SEATED activity with any amount of data and across all the models.\nIn the case of the CNN and CNN-LSTM models in \\(n \\geq 10\\), the fused-trained models also achieve the best results, with no significant differences with the smartwatch-trained models. Therefore, the smartphone-trained models achieve the worst results in these models.\nRegarding the MLP and LSTM models, when trained with smartphone and fused data their performance is significantly worse than when trained with smartwatch data. Between the smartphone- and fused-trained models, no significant differences exist in the MLP model with high amounts of data, although differences are observed in the LSTM model in favour of the fused-trained models.\n\n\nCode\ndatasource_seated_tests, datasource_seated_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.SEATED, ActivityMetric.F1),\n    MODELS, \n    SOURCES\n)\ndatasource_seated_tests\n\n\n\n\nTable 7.3: Statistical comparison of SEATED performance obtained by the data sources for each model.\n\n\n\n\n\n\n\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\n\n\n\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.065\n0.397\n0.116\n84.149\n0.0\n0.000\n0.772\n0.388\n216.958\n0.0\n0.083\n0.749\n0.316\n258.576\n0.0\n0.084\n0.644\n0.136\n58.067\n0.0\n\n\n2\n0.210\n0.706\n0.456\n121.818\n0.0\n0.273\n0.830\n0.716\n204.815\n0.0\n0.176\n0.823\n0.650\n248.303\n0.0\n0.737\n0.826\n0.705\n63.866\n0.0\n\n\n3\n0.245\n0.805\n0.543\n185.279\n0.0\n0.593\n0.849\n0.800\n163.509\n0.0\n0.431\n0.844\n0.741\n201.328\n0.0\n0.788\n0.850\n0.816\n35.338\n0.0\n\n\n4\n0.498\n0.840\n0.673\n161.477\n0.0\n0.733\n0.864\n0.823\n110.338\n0.0\n0.587\n0.851\n0.800\n176.345\n0.0\n0.800\n0.858\n0.827\n42.377\n0.0\n\n\n5\n0.660\n0.841\n0.685\n143.959\n0.0\n0.783\n0.868\n0.839\n85.974\n0.0\n0.653\n0.856\n0.800\n148.075\n0.0\n0.812\n0.858\n0.853\n42.617\n0.0\n\n\n6\n0.645\n0.850\n0.720\n152.244\n0.0\n0.774\n0.869\n0.839\n104.495\n0.0\n0.669\n0.857\n0.800\n186.511\n0.0\n0.797\n0.875\n0.849\n78.750\n0.0\n\n\n7\n0.727\n0.849\n0.702\n129.278\n0.0\n0.794\n0.866\n0.854\n68.601\n0.0\n0.735\n0.862\n0.828\n138.187\n0.0\n0.821\n0.872\n0.868\n42.119\n0.0\n\n\n8\n0.738\n0.868\n0.717\n164.616\n0.0\n0.810\n0.873\n0.861\n66.231\n0.0\n0.758\n0.867\n0.833\n121.404\n0.0\n0.811\n0.877\n0.870\n53.550\n0.0\n\n\n9\n0.778\n0.871\n0.793\n117.282\n0.0\n0.833\n0.880\n0.861\n54.662\n0.0\n0.797\n0.870\n0.845\n68.948\n0.0\n0.836\n0.879\n0.870\n45.063\n0.0\n\n\n10\n0.767\n0.872\n0.787\n108.284\n0.0\n0.830\n0.877\n0.872\n58.546\n0.0\n0.794\n0.871\n0.844\n92.679\n0.0\n0.833\n0.881\n0.875\n51.054\n0.0\n\n\n11\n0.780\n0.870\n0.798\n103.231\n0.0\n0.827\n0.878\n0.867\n52.735\n0.0\n0.799\n0.866\n0.844\n79.081\n0.0\n0.830\n0.878\n0.865\n42.425\n0.0\n\n\n12\n0.787\n0.876\n0.808\n109.682\n0.0\n0.833\n0.880\n0.876\n48.220\n0.0\n0.811\n0.876\n0.862\n81.305\n0.0\n0.833\n0.882\n0.875\n49.322\n0.0\n\n\n13\n0.807\n0.877\n0.816\n81.070\n0.0\n0.831\n0.884\n0.871\n60.920\n0.0\n0.806\n0.877\n0.847\n84.146\n0.0\n0.831\n0.878\n0.874\n55.169\n0.0\n\n\n14\n0.802\n0.877\n0.813\n91.533\n0.0\n0.842\n0.885\n0.877\n45.794\n0.0\n0.828\n0.872\n0.863\n49.187\n0.0\n0.828\n0.879\n0.874\n56.553\n0.0\n\n\n15\n0.801\n0.873\n0.835\n78.908\n0.0\n0.837\n0.882\n0.872\n42.675\n0.0\n0.830\n0.876\n0.858\n52.279\n0.0\n0.833\n0.882\n0.880\n60.678\n0.0\n\n\n16\n0.809\n0.875\n0.841\n72.361\n0.0\n0.847\n0.885\n0.876\n36.096\n0.0\n0.823\n0.887\n0.857\n71.288\n0.0\n0.843\n0.886\n0.880\n52.916\n0.0\n\n\n17\n0.807\n0.881\n0.833\n70.534\n0.0\n0.848\n0.880\n0.877\n38.414\n0.0\n0.830\n0.876\n0.865\n53.088\n0.0\n0.836\n0.881\n0.880\n50.416\n0.0\n\n\n18\n0.807\n0.882\n0.838\n76.805\n0.0\n0.842\n0.885\n0.875\n39.895\n0.0\n0.837\n0.886\n0.861\n60.194\n0.0\n0.843\n0.882\n0.880\n55.166\n0.0\n\n\n19\n0.812\n0.880\n0.846\n67.328\n0.0\n0.842\n0.889\n0.875\n51.272\n0.0\n0.829\n0.878\n0.865\n52.793\n0.0\n0.837\n0.881\n0.885\n61.821\n0.0\n\n\n20\n0.813\n0.885\n0.842\n69.846\n0.0\n0.851\n0.884\n0.877\n29.107\n0.0\n0.830\n0.883\n0.868\n59.387\n0.0\n0.844\n0.884\n0.883\n53.625\n0.0\n\n\n21\n0.814\n0.875\n0.844\n57.710\n0.0\n0.854\n0.881\n0.875\n27.843\n0.0\n0.842\n0.886\n0.867\n41.392\n0.0\n0.837\n0.883\n0.885\n67.289\n0.0\n\n\n22\n0.826\n0.879\n0.845\n59.162\n0.0\n0.850\n0.887\n0.875\n34.946\n0.0\n0.851\n0.887\n0.867\n44.372\n0.0\n0.842\n0.883\n0.885\n59.259\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nshow(datasource_seated_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 7.4: Post-hoc tests for Table 7.3 to determine the best-performant data sources.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nSTANDING_UP\nTable 7.5, 7.6 show a similar pattern across all models, where the smartwatch-trained models produce the best results with low and medium amounts of data while the fused-trained models also are the best-performing with medium and high amounts of data.\nThis pattern can be observed in the MLP, CNN and CNN-LSTM models, although the value of \\(n\\) where the fused-trained models start to outperform the smartwatch-trained models varies. In the case of the CNN and CNN-LSTM, the models trained with smartwatch data are significantly better than ones trained with smartphone data, while the contrary occurs in the MLP model. In the remaining model, the LSTM, since no differences exist between the smartwatch- and fused-trained models, the smartphone-trained models provide the worst results.\n\n\nCode\ndatasource_standing_tests, datasource_standing_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.STANDING_UP, ActivityMetric.F1),\n    MODELS, \n    SOURCES\n)\ndatasource_standing_tests\n\n\n\n\nTable 7.5: Statistical comparison of STANDING_UP performance obtained by the data sources for each model.\n\n\n\n\n\n\n\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\n\n\n\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.060\n0.434\n0.141\n157.367\n0.000\n0.130\n0.583\n0.320\n180.914\n0.0\n0.093\n0.546\n0.290\n204.095\n0.0\n0.139\n0.517\n0.275\n165.336\n0.0\n\n\n2\n0.209\n0.555\n0.320\n113.170\n0.000\n0.362\n0.675\n0.563\n95.268\n0.0\n0.272\n0.632\n0.481\n120.194\n0.0\n0.318\n0.656\n0.487\n114.961\n0.0\n\n\n3\n0.324\n0.563\n0.469\n75.872\n0.000\n0.482\n0.716\n0.667\n81.443\n0.0\n0.369\n0.667\n0.550\n111.214\n0.0\n0.494\n0.681\n0.646\n73.127\n0.0\n\n\n4\n0.485\n0.613\n0.594\n43.798\n0.000\n0.609\n0.767\n0.738\n62.602\n0.0\n0.470\n0.717\n0.670\n91.930\n0.0\n0.600\n0.730\n0.722\n56.548\n0.0\n\n\n5\n0.496\n0.634\n0.614\n31.293\n0.000\n0.657\n0.781\n0.749\n58.233\n0.0\n0.541\n0.753\n0.699\n102.212\n0.0\n0.638\n0.760\n0.742\n58.544\n0.0\n\n\n6\n0.514\n0.637\n0.626\n26.012\n0.000\n0.643\n0.800\n0.766\n77.163\n0.0\n0.528\n0.771\n0.701\n109.180\n0.0\n0.626\n0.772\n0.760\n61.746\n0.0\n\n\n7\n0.590\n0.658\n0.667\n17.192\n0.000\n0.693\n0.809\n0.777\n59.785\n0.0\n0.564\n0.768\n0.713\n99.197\n0.0\n0.699\n0.786\n0.777\n39.730\n0.0\n\n\n8\n0.607\n0.674\n0.678\n13.616\n0.001\n0.713\n0.804\n0.793\n47.931\n0.0\n0.642\n0.796\n0.732\n91.323\n0.0\n0.699\n0.788\n0.793\n51.247\n0.0\n\n\n9\n0.651\n0.694\n0.731\n15.759\n0.000\n0.745\n0.823\n0.813\n51.888\n0.0\n0.704\n0.811\n0.777\n52.882\n0.0\n0.726\n0.805\n0.809\n59.026\n0.0\n\n\n10\n0.643\n0.698\n0.732\n18.579\n0.000\n0.769\n0.830\n0.818\n35.898\n0.0\n0.707\n0.824\n0.789\n70.136\n0.0\n0.743\n0.818\n0.822\n45.094\n0.0\n\n\n11\n0.681\n0.696\n0.735\n11.883\n0.003\n0.775\n0.823\n0.827\n38.852\n0.0\n0.724\n0.814\n0.791\n56.135\n0.0\n0.771\n0.814\n0.824\n39.276\n0.0\n\n\n12\n0.675\n0.688\n0.740\n10.138\n0.006\n0.748\n0.830\n0.825\n50.593\n0.0\n0.721\n0.825\n0.786\n57.273\n0.0\n0.756\n0.827\n0.840\n45.765\n0.0\n\n\n13\n0.696\n0.693\n0.766\n19.114\n0.000\n0.775\n0.833\n0.841\n49.029\n0.0\n0.762\n0.832\n0.800\n48.930\n0.0\n0.771\n0.819\n0.847\n54.329\n0.0\n\n\n14\n0.712\n0.697\n0.755\n17.164\n0.000\n0.788\n0.832\n0.838\n39.853\n0.0\n0.765\n0.835\n0.813\n58.304\n0.0\n0.777\n0.826\n0.839\n39.972\n0.0\n\n\n15\n0.721\n0.703\n0.769\n27.619\n0.000\n0.792\n0.845\n0.848\n42.921\n0.0\n0.765\n0.842\n0.821\n56.332\n0.0\n0.769\n0.839\n0.851\n54.770\n0.0\n\n\n16\n0.726\n0.715\n0.779\n20.737\n0.000\n0.800\n0.843\n0.857\n47.573\n0.0\n0.765\n0.841\n0.830\n58.934\n0.0\n0.791\n0.836\n0.861\n59.137\n0.0\n\n\n17\n0.738\n0.712\n0.778\n27.224\n0.000\n0.791\n0.842\n0.857\n50.423\n0.0\n0.780\n0.843\n0.825\n44.072\n0.0\n0.783\n0.832\n0.857\n51.329\n0.0\n\n\n18\n0.724\n0.716\n0.789\n29.998\n0.000\n0.805\n0.843\n0.859\n36.824\n0.0\n0.779\n0.854\n0.825\n50.413\n0.0\n0.786\n0.833\n0.860\n51.852\n0.0\n\n\n19\n0.753\n0.712\n0.794\n36.159\n0.000\n0.809\n0.847\n0.860\n40.092\n0.0\n0.788\n0.846\n0.844\n43.787\n0.0\n0.798\n0.831\n0.870\n56.029\n0.0\n\n\n20\n0.749\n0.714\n0.798\n44.079\n0.000\n0.805\n0.850\n0.869\n57.402\n0.0\n0.789\n0.852\n0.845\n56.008\n0.0\n0.800\n0.838\n0.862\n45.284\n0.0\n\n\n21\n0.753\n0.717\n0.800\n41.653\n0.000\n0.809\n0.847\n0.871\n44.334\n0.0\n0.800\n0.846\n0.847\n36.116\n0.0\n0.802\n0.837\n0.865\n56.834\n0.0\n\n\n22\n0.745\n0.714\n0.800\n50.242\n0.000\n0.821\n0.857\n0.871\n41.424\n0.0\n0.792\n0.851\n0.859\n47.474\n0.0\n0.807\n0.844\n0.872\n58.018\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nshow(datasource_standing_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 7.6: Post-hoc tests for Table 7.5 to determine the best-performant data sources.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nWALKING\nTable 7.7, 7.8 show different patterns regarding the model employed. For the MLP and CNN models, the smartwatch models are the best with few quantities of data, but no significant differences among data sources are appreciated in \\(n \\in [4,5]\\). After \\(n \\geq 6\\), the smartphone- and fused-trained models obtain the best results.\nIn the LSTM and the CNN-LSTM, the smartwatch-trained models are the best-performing with low amounts of data, while with medium and high quantities of data, the fused-trained models are the best. Regarding the models trained with smartphone and smartwatch data, significant differences exist in the CNN-LSTM models in favour of the smartphone-trained models, but not on the LSTM.\n\n\nCode\ndatasource_walking_tests, datasource_walking_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.WALKING, ActivityMetric.F1),\n    MODELS, \n    SOURCES\n)\ndatasource_walking_tests\n\n\n\n\nTable 7.7: Statistical comparison of WALKING performance obtained by the data sources for each model.\n\n\n\n\n\n\n\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\n\n\n\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.027\n0.679\n0.071\n142.659\n0.000\n0.471\n0.765\n0.620\n65.950\n0.000\n0.347\n0.734\n0.576\n91.566\n0.000\n0.509\n0.736\n0.603\n47.913\n0.000\n\n\n2\n0.599\n0.778\n0.667\n48.257\n0.000\n0.775\n0.833\n0.811\n10.585\n0.005\n0.728\n0.801\n0.759\n21.695\n0.000\n0.794\n0.810\n0.768\n7.564\n0.023\n\n\n3\n0.764\n0.795\n0.780\n7.010\n0.030\n0.826\n0.852\n0.861\n12.450\n0.002\n0.784\n0.841\n0.803\n20.031\n0.000\n0.820\n0.843\n0.836\n1.628\n0.443\n\n\n4\n0.817\n0.815\n0.813\n0.201\n0.904\n0.859\n0.865\n0.875\n4.437\n0.109\n0.799\n0.860\n0.847\n20.265\n0.000\n0.847\n0.861\n0.853\n1.700\n0.428\n\n\n5\n0.856\n0.835\n0.847\n3.367\n0.186\n0.889\n0.883\n0.888\n4.750\n0.093\n0.838\n0.870\n0.853\n14.631\n0.001\n0.874\n0.867\n0.883\n9.651\n0.008\n\n\n6\n0.845\n0.848\n0.856\n0.333\n0.847\n0.874\n0.886\n0.893\n7.039\n0.030\n0.833\n0.877\n0.860\n27.379\n0.000\n0.874\n0.870\n0.885\n7.491\n0.024\n\n\n7\n0.874\n0.849\n0.876\n12.934\n0.002\n0.899\n0.887\n0.901\n10.127\n0.006\n0.842\n0.883\n0.880\n30.691\n0.000\n0.898\n0.871\n0.894\n15.912\n0.000\n\n\n8\n0.870\n0.855\n0.882\n16.806\n0.000\n0.895\n0.886\n0.904\n10.418\n0.005\n0.848\n0.889\n0.877\n30.343\n0.000\n0.884\n0.879\n0.898\n15.809\n0.000\n\n\n9\n0.893\n0.859\n0.887\n35.791\n0.000\n0.905\n0.900\n0.915\n14.265\n0.001\n0.882\n0.893\n0.890\n7.437\n0.024\n0.887\n0.893\n0.914\n25.310\n0.000\n\n\n10\n0.898\n0.865\n0.899\n41.057\n0.000\n0.910\n0.901\n0.915\n14.139\n0.001\n0.884\n0.894\n0.904\n10.507\n0.005\n0.904\n0.892\n0.913\n29.421\n0.000\n\n\n11\n0.898\n0.868\n0.899\n49.683\n0.000\n0.907\n0.897\n0.912\n11.350\n0.003\n0.879\n0.894\n0.902\n12.538\n0.002\n0.899\n0.892\n0.912\n20.069\n0.000\n\n\n12\n0.905\n0.866\n0.904\n73.540\n0.000\n0.911\n0.901\n0.914\n17.755\n0.000\n0.887\n0.899\n0.904\n8.186\n0.017\n0.896\n0.893\n0.919\n41.693\n0.000\n\n\n13\n0.905\n0.869\n0.908\n91.948\n0.000\n0.917\n0.901\n0.924\n36.930\n0.000\n0.900\n0.898\n0.907\n5.347\n0.069\n0.906\n0.896\n0.925\n62.834\n0.000\n\n\n14\n0.909\n0.870\n0.908\n85.674\n0.000\n0.916\n0.907\n0.925\n28.561\n0.000\n0.903\n0.906\n0.917\n10.336\n0.006\n0.910\n0.898\n0.925\n52.277\n0.000\n\n\n15\n0.909\n0.874\n0.915\n109.023\n0.000\n0.922\n0.908\n0.925\n37.932\n0.000\n0.902\n0.905\n0.914\n5.706\n0.058\n0.908\n0.900\n0.927\n60.843\n0.000\n\n\n16\n0.910\n0.869\n0.914\n100.301\n0.000\n0.925\n0.909\n0.925\n38.392\n0.000\n0.904\n0.905\n0.919\n18.383\n0.000\n0.909\n0.899\n0.930\n83.681\n0.000\n\n\n17\n0.914\n0.871\n0.918\n121.710\n0.000\n0.925\n0.908\n0.928\n37.226\n0.000\n0.903\n0.909\n0.916\n4.562\n0.102\n0.910\n0.902\n0.928\n66.523\n0.000\n\n\n18\n0.913\n0.873\n0.916\n128.281\n0.000\n0.928\n0.908\n0.928\n48.604\n0.000\n0.909\n0.906\n0.918\n10.981\n0.004\n0.914\n0.902\n0.933\n82.070\n0.000\n\n\n19\n0.916\n0.875\n0.918\n148.882\n0.000\n0.927\n0.908\n0.929\n51.017\n0.000\n0.905\n0.906\n0.919\n13.093\n0.001\n0.918\n0.904\n0.932\n84.062\n0.000\n\n\n20\n0.916\n0.874\n0.922\n145.974\n0.000\n0.928\n0.912\n0.930\n48.167\n0.000\n0.912\n0.910\n0.925\n17.917\n0.000\n0.917\n0.906\n0.932\n79.506\n0.000\n\n\n21\n0.917\n0.880\n0.923\n134.989\n0.000\n0.931\n0.913\n0.931\n62.152\n0.000\n0.915\n0.913\n0.925\n9.854\n0.007\n0.918\n0.905\n0.933\n75.168\n0.000\n\n\n22\n0.918\n0.877\n0.923\n169.960\n0.000\n0.932\n0.913\n0.933\n69.003\n0.000\n0.912\n0.911\n0.925\n19.418\n0.000\n0.921\n0.907\n0.935\n88.114\n0.000\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nshow(datasource_walking_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 7.8: Post-hoc tests for Table 7.7 to determine the best-performant data sources.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nTURNING\nThe results presented in Table 7.9, 7.10 indicate that the smartphone- and fused-trained models obtain the best metrics in almost any case.\nThe smartphone-trained models consistently obtain the best results with any amount of data across all models. On the other hand, the fused-trained models require medium amounts of data to equal the smartphone results in the MLP and CNN-LSTM. The smartwatch-trained models only perform well when the minimum amount of training data is used. For any other quantities, they provide the worst results.\n\n\nCode\ndatasource_turning_tests, datasource_turning_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.TURNING, ActivityMetric.F1),\n    MODELS, \n    SOURCES\n)\ndatasource_turning_tests\n\n\n\n\nTable 7.9: Statistical comparison of TURNING performance obtained by the data sources for each model.\n\n\n\n\n\n\n\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\n\n\n\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.151\n0.412\n0.344\n23.926\n0.000\n0.589\n0.648\n0.534\n18.943\n0.000\n0.402\n0.514\n0.478\n11.056\n0.004\n0.568\n0.574\n0.497\n7.935\n0.019\n\n\n2\n0.636\n0.614\n0.451\n22.429\n0.000\n0.779\n0.692\n0.761\n12.047\n0.002\n0.744\n0.642\n0.768\n31.916\n0.000\n0.763\n0.663\n0.682\n21.681\n0.000\n\n\n3\n0.733\n0.674\n0.648\n14.444\n0.001\n0.796\n0.728\n0.796\n29.461\n0.000\n0.784\n0.715\n0.783\n39.760\n0.000\n0.773\n0.709\n0.759\n22.098\n0.000\n\n\n4\n0.783\n0.670\n0.738\n32.274\n0.000\n0.820\n0.742\n0.810\n39.893\n0.000\n0.813\n0.728\n0.818\n56.020\n0.000\n0.818\n0.727\n0.785\n39.105\n0.000\n\n\n5\n0.829\n0.701\n0.784\n71.408\n0.000\n0.826\n0.753\n0.827\n51.768\n0.000\n0.811\n0.746\n0.835\n91.561\n0.000\n0.839\n0.741\n0.809\n77.591\n0.000\n\n\n6\n0.821\n0.714\n0.797\n55.737\n0.000\n0.827\n0.766\n0.823\n32.806\n0.000\n0.812\n0.761\n0.827\n44.326\n0.000\n0.827\n0.757\n0.822\n43.343\n0.000\n\n\n7\n0.840\n0.716\n0.828\n100.433\n0.000\n0.842\n0.773\n0.837\n55.313\n0.000\n0.825\n0.763\n0.835\n56.482\n0.000\n0.844\n0.758\n0.830\n61.091\n0.000\n\n\n8\n0.836\n0.723\n0.834\n148.564\n0.000\n0.849\n0.772\n0.839\n68.741\n0.000\n0.832\n0.780\n0.841\n51.793\n0.000\n0.846\n0.759\n0.832\n70.827\n0.000\n\n\n9\n0.847\n0.728\n0.836\n170.804\n0.000\n0.843\n0.786\n0.849\n66.251\n0.000\n0.838\n0.773\n0.845\n79.757\n0.000\n0.839\n0.781\n0.840\n63.635\n0.000\n\n\n10\n0.854\n0.734\n0.846\n173.315\n0.000\n0.854\n0.793\n0.849\n70.227\n0.000\n0.851\n0.786\n0.851\n67.842\n0.000\n0.844\n0.786\n0.847\n65.421\n0.000\n\n\n11\n0.845\n0.728\n0.850\n191.693\n0.000\n0.852\n0.784\n0.849\n76.988\n0.000\n0.845\n0.784\n0.850\n78.618\n0.000\n0.846\n0.782\n0.839\n51.429\n0.000\n\n\n12\n0.853\n0.726\n0.859\n217.040\n0.000\n0.859\n0.794\n0.849\n79.960\n0.000\n0.848\n0.786\n0.855\n61.309\n0.000\n0.848\n0.790\n0.854\n73.268\n0.000\n\n\n13\n0.856\n0.736\n0.859\n226.242\n0.000\n0.864\n0.799\n0.860\n104.813\n0.000\n0.852\n0.788\n0.851\n79.701\n0.000\n0.860\n0.785\n0.859\n82.505\n0.000\n\n\n14\n0.856\n0.743\n0.862\n229.817\n0.000\n0.860\n0.797\n0.862\n96.266\n0.000\n0.850\n0.802\n0.855\n79.526\n0.000\n0.856\n0.791\n0.855\n78.966\n0.000\n\n\n15\n0.861\n0.745\n0.865\n274.935\n0.000\n0.867\n0.802\n0.860\n108.313\n0.000\n0.854\n0.799\n0.860\n78.817\n0.000\n0.857\n0.795\n0.856\n77.251\n0.000\n\n\n16\n0.862\n0.738\n0.863\n261.219\n0.000\n0.867\n0.802\n0.862\n95.275\n0.000\n0.860\n0.807\n0.860\n65.828\n0.000\n0.856\n0.794\n0.857\n87.502\n0.000\n\n\n17\n0.866\n0.735\n0.868\n283.156\n0.000\n0.871\n0.801\n0.862\n112.284\n0.000\n0.855\n0.806\n0.859\n61.410\n0.000\n0.860\n0.806\n0.860\n87.450\n0.000\n\n\n18\n0.864\n0.743\n0.862\n272.812\n0.000\n0.866\n0.802\n0.867\n97.861\n0.000\n0.858\n0.805\n0.860\n66.906\n0.000\n0.858\n0.800\n0.862\n80.067\n0.000\n\n\n19\n0.866\n0.742\n0.864\n296.156\n0.000\n0.869\n0.807\n0.869\n105.937\n0.000\n0.853\n0.812\n0.859\n65.296\n0.000\n0.860\n0.805\n0.863\n87.774\n0.000\n\n\n20\n0.864\n0.742\n0.868\n312.355\n0.000\n0.870\n0.812\n0.868\n112.034\n0.000\n0.859\n0.815\n0.860\n62.550\n0.000\n0.864\n0.799\n0.866\n95.319\n0.000\n\n\n21\n0.865\n0.750\n0.870\n312.882\n0.000\n0.874\n0.819\n0.869\n111.421\n0.000\n0.860\n0.812\n0.859\n69.801\n0.000\n0.862\n0.803\n0.864\n84.037\n0.000\n\n\n22\n0.864\n0.743\n0.868\n305.136\n0.000\n0.870\n0.812\n0.870\n111.311\n0.000\n0.861\n0.817\n0.861\n67.998\n0.000\n0.865\n0.807\n0.866\n87.498\n0.000\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nshow(datasource_turning_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 7.10: Post-hoc tests for Table 7.9 to determine the best-performant data sources.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nSITTING_DOWN\nThe results shown in Table 7.11, 7.12 present similar patterns to those of the STANDING_UP activity. On the one hand, in the MLP, CNN and CNN-LSTM, the smartwatch-trained models are the best-performing with low and medium amounts of data while the fused-trained models provide the best metrics with medium and high amounts of data. On the other hand, the models trained with smartwatch and fused data are the best on the LSTM models.\nAs in the STANDING_UP activity, aside from the superiority of the fused-trained models, the smartwatch-trained models outperform the smartphone-trained models in the CNN and the CNN-LSTM, while the opposite applies for MLP models.\n\n\nCode\ndatasource_sitting_tests, datasource_sitting_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.SITTING_DOWN, ActivityMetric.F1),\n    MODELS, \n    SOURCES\n)\ndatasource_sitting_tests\n\n\n\n\nTable 7.11: Statistical comparison of SITTING_DOWN performance obtained by the data sources for each model.\n\n\n\n\n\n\n\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\n\n\n\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\nsp\nsw\nfused\nH(2)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.108\n0.368\n0.100\n125.001\n0.0\n0.102\n0.583\n0.238\n185.743\n0.0\n0.094\n0.499\n0.252\n190.167\n0.0\n0.152\n0.479\n0.168\n164.098\n0.0\n\n\n2\n0.207\n0.458\n0.346\n62.538\n0.0\n0.363\n0.657\n0.581\n83.232\n0.0\n0.305\n0.649\n0.511\n108.515\n0.0\n0.265\n0.624\n0.478\n80.936\n0.0\n\n\n3\n0.286\n0.535\n0.443\n94.438\n0.0\n0.473\n0.725\n0.671\n82.024\n0.0\n0.406\n0.691\n0.610\n123.758\n0.0\n0.471\n0.717\n0.620\n91.994\n0.0\n\n\n4\n0.420\n0.603\n0.569\n59.284\n0.0\n0.619\n0.754\n0.712\n43.731\n0.0\n0.502\n0.718\n0.680\n92.819\n0.0\n0.572\n0.730\n0.709\n60.015\n0.0\n\n\n5\n0.507\n0.632\n0.612\n37.638\n0.0\n0.651\n0.768\n0.753\n53.262\n0.0\n0.547\n0.750\n0.654\n90.598\n0.0\n0.613\n0.748\n0.754\n62.946\n0.0\n\n\n6\n0.518\n0.633\n0.642\n42.385\n0.0\n0.650\n0.786\n0.779\n53.718\n0.0\n0.617\n0.766\n0.711\n87.575\n0.0\n0.632\n0.766\n0.780\n73.800\n0.0\n\n\n7\n0.594\n0.659\n0.658\n24.641\n0.0\n0.704\n0.793\n0.800\n46.566\n0.0\n0.597\n0.778\n0.738\n85.029\n0.0\n0.680\n0.778\n0.812\n64.448\n0.0\n\n\n8\n0.612\n0.667\n0.679\n25.609\n0.0\n0.705\n0.806\n0.811\n47.750\n0.0\n0.634\n0.794\n0.754\n76.895\n0.0\n0.693\n0.786\n0.807\n67.641\n0.0\n\n\n9\n0.659\n0.688\n0.698\n15.948\n0.0\n0.738\n0.822\n0.827\n42.565\n0.0\n0.689\n0.808\n0.785\n47.875\n0.0\n0.692\n0.806\n0.815\n68.162\n0.0\n\n\n10\n0.660\n0.695\n0.731\n25.724\n0.0\n0.751\n0.829\n0.841\n55.646\n0.0\n0.736\n0.815\n0.792\n45.626\n0.0\n0.724\n0.802\n0.828\n56.691\n0.0\n\n\n11\n0.669\n0.695\n0.728\n19.301\n0.0\n0.766\n0.828\n0.828\n60.149\n0.0\n0.739\n0.809\n0.786\n52.023\n0.0\n0.727\n0.811\n0.828\n78.245\n0.0\n\n\n12\n0.687\n0.697\n0.743\n23.797\n0.0\n0.765\n0.824\n0.843\n46.111\n0.0\n0.743\n0.818\n0.814\n41.332\n0.0\n0.736\n0.805\n0.842\n71.700\n0.0\n\n\n13\n0.710\n0.701\n0.765\n26.949\n0.0\n0.774\n0.827\n0.850\n51.775\n0.0\n0.753\n0.817\n0.809\n30.991\n0.0\n0.756\n0.804\n0.856\n82.744\n0.0\n\n\n14\n0.717\n0.706\n0.762\n23.874\n0.0\n0.784\n0.832\n0.860\n50.096\n0.0\n0.765\n0.823\n0.831\n32.109\n0.0\n0.753\n0.811\n0.849\n83.008\n0.0\n\n\n15\n0.713\n0.714\n0.783\n45.473\n0.0\n0.787\n0.834\n0.857\n48.778\n0.0\n0.778\n0.828\n0.824\n22.071\n0.0\n0.761\n0.814\n0.857\n85.721\n0.0\n\n\n16\n0.714\n0.704\n0.785\n57.454\n0.0\n0.791\n0.845\n0.848\n38.998\n0.0\n0.783\n0.831\n0.838\n35.341\n0.0\n0.759\n0.823\n0.865\n104.325\n0.0\n\n\n17\n0.734\n0.720\n0.786\n48.910\n0.0\n0.809\n0.837\n0.857\n41.412\n0.0\n0.796\n0.833\n0.832\n17.900\n0.0\n0.774\n0.822\n0.860\n71.749\n0.0\n\n\n18\n0.739\n0.715\n0.785\n47.594\n0.0\n0.800\n0.846\n0.869\n59.795\n0.0\n0.795\n0.833\n0.837\n21.110\n0.0\n0.768\n0.824\n0.866\n87.083\n0.0\n\n\n19\n0.743\n0.722\n0.800\n62.711\n0.0\n0.806\n0.845\n0.873\n54.034\n0.0\n0.786\n0.844\n0.839\n24.038\n0.0\n0.769\n0.823\n0.870\n93.933\n0.0\n\n\n20\n0.750\n0.714\n0.802\n72.130\n0.0\n0.807\n0.838\n0.873\n58.636\n0.0\n0.796\n0.833\n0.850\n31.334\n0.0\n0.783\n0.822\n0.876\n98.424\n0.0\n\n\n21\n0.744\n0.730\n0.813\n61.236\n0.0\n0.816\n0.840\n0.872\n47.147\n0.0\n0.806\n0.841\n0.840\n16.009\n0.0\n0.788\n0.822\n0.871\n92.640\n0.0\n\n\n22\n0.768\n0.726\n0.810\n87.104\n0.0\n0.821\n0.850\n0.879\n51.039\n0.0\n0.800\n0.842\n0.846\n23.175\n0.0\n0.790\n0.829\n0.876\n91.725\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nshow(datasource_sitting_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 7.12: Post-hoc tests for Table 7.11 to determine the best-performant data sources.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical comparison of data sources performance</span>"
    ]
  },
  {
    "objectID": "03.2_data-sources.html#summary",
    "href": "03.2_data-sources.html#summary",
    "title": "Statistical comparison of data sources performance",
    "section": "Summary",
    "text": "Summary\nThe obtained results show that the smartwatch-trained models have the best overall accuracy and activities F1-score across all models with low amounts of data. On the other side, models trained with the fused dataset present the best overall accuracy results with higher amounts of data.\nWhen focusing on individual activities, the smartwatch-trained models are the best to recognize the SEATED activity. The smartwatch-trained models are also the best for the STANDING_UP activity with low and medium amounts of data, while with higher amounts the fused-trained models show better results. In the WALKING activity, the fused-trained models obtain the best results across all models, while the smartphone-trained models dataset joins them in the MLP and CNN models. Similarly, the models trained with the smartphone and the fused datasets are the best in the TURNING activity. For the SITTING_DOWN activity, the smartwatch-trained models are good with a low quantity of data while the fused-trained models are the best with medium and higher quantities. It is worth noting that the patterns observed in the STANDING_UP and SITTING_DOWN activities are very similar, which can be explained due to the inverse nature of these movements.\nWhile the models trained with the fused dataset usually show the best results, sometimes they are not statistically better than the results obtained with the other datasets. In other words, the fact that the best results are presented by the fused-trained models and also by the smartphone or smartwatch ones implies that the fusion of the data is not always worth it. For example, in the TURNING activity, the smartphone- and the fused-trained models are always the best, which indicates that the fusion of smartphone and smartwatch data does not improve the smartphone results. The same occurs in the WALKING activity with the MLP and CNN models. However, the fusion is worth it for the remaining models in that activity and the STANDING_UP or SITTING_DOWN activities.\nThese results are visually summarized and simplified in Figure 7.1, 7.2. Following, some examples are given to show how to interpret the figure: in the WALKING activity and the CNN model, for \\(n=1\\), the statistically best metrics are obtained with the smartwatch dataset; for \\(n=2\\), the best metrics are obtained with the smartwatch dataset, although not statistically better compared with another dataset (whether smartphone or fused, should be determined by checking Table 7.7), and for \\(n=4\\), no significant difference is observed between data sources.\nGiven the obtained results, it is not possible to determine a clear winner. In the end, the most suitable data source will depend on the amount of data that can be collected, the target activities and the selected model – in line with . We could determine that the fused dataset would be the best option with any model and a moderate amount of data, while the smartwatch dataset would be good for the SEATED activity and the smartphone dataset works fine with the TURNING activity. The smartwatch dataset would also be the preferred choice for the STANDING_UP and SITTING_DOWN activities using the LSTM model.\n\n\nCode\nsources_results = {\n    TargetFilter.MODEL: datasource_overlall_tests, \n    TargetFilter.SEATED: datasource_seated_tests, \n    TargetFilter.STANDING_UP: datasource_standing_tests, \n    TargetFilter.WALKING: datasource_walking_tests, \n    TargetFilter.TURNING: datasource_turning_tests, \n    TargetFilter.SITTING_DOWN: datasource_sitting_tests\n}\n\nbest_sources = obtain_best_items(sources_results, SOURCES, MODELS)\nsignificance_sources = load_best_significant(SIGNIFICANCE_FILE)\n\nplot_visual_comparison(best_sources, significance_sources, SOURCES, MODELS)\n\n\n\n\n                                                \n\n\nFigure 7.1: Graphical representation of best data sources for each metric, model and amount of data combination. Symbology: ■ (SP), ◆ (SW) and ● (FUSED).\n\n\n\n\n\n\nCode\nplot_visual_ties(best_sources, significance_sources, SOURCES, MODELS)\n\n\n\n\n                                                \n\n\nFigure 7.2: Visual representation of performance ties. The plot indicates the amount of times a specific data source tied with each other.",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical comparison of data sources performance</span>"
    ]
  },
  {
    "objectID": "03.2_data-sources.html#code-reference",
    "href": "03.2_data-sources.html#code-reference",
    "title": "Statistical comparison of data sources performance",
    "section": "Code reference",
    "text": "Code reference\n\n\n\n\n\n\nTip\n\n\n\nThe documentation of the Python functions employed in this section can be found in Chapter 3 reference:\n\ndata_loading:\n\nload_reports\nload_best_significant\n\nmodel:\n\nActivityMetric\nModel\nModelMetric\nSource\nTargetFilter\nobtain_best_items\n\nstatistical_tests:\n\nstatistical_comparison\n\nvisualization:\n\nplot_visual_comparison\nplot_visual_ties",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical comparison of data sources performance</span>"
    ]
  },
  {
    "objectID": "03.3_models.html",
    "href": "03.3_models.html",
    "title": "Statistical comparison of models performance",
    "section": "",
    "text": "Overall performance\nTable 8.1, 8.2 show that the CNN is the best-performant model in any dataset and any amount of data.\nIn the smartphone dataset, the CNN-LSTM also performs well for low amounts of data, while with higher quantities of data, there are no statistical differences among the MLP, LSTM and CNN-LSTM. Regarding the smartwatch dataset, the LSTM and CNN-LSTM perform best with medium and higher amounts of data, along with the CNN model.\nFor the fused dataset, the CNN and the CNN-LSTM show the best accuracies with any amount of data, while the MLP model significantly shows the worst results.\nCode\nmodels_overlall_tests, models_overall_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.MODEL, ModelMetric.ACCURACY), \n    SOURCES, \n    MODELS\n)\nmodels_overlall_tests\n\n\n\n\nTable 8.1: Statistical comparison of overall accuracies obtained by the models for each data source.\n\n\n\n\n\n\n\n\n\n\nsp\nsw\nfused\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.202\n0.390\n0.283\n0.426\n63.138\n0.0\n0.528\n0.678\n0.650\n0.624\n132.544\n0.0\n0.242\n0.511\n0.437\n0.475\n66.589\n0.0\n\n\n2\n0.491\n0.659\n0.583\n0.682\n49.458\n0.0\n0.679\n0.755\n0.728\n0.739\n71.994\n0.0\n0.491\n0.716\n0.661\n0.654\n69.982\n0.0\n\n\n3\n0.609\n0.719\n0.665\n0.710\n54.824\n0.0\n0.710\n0.788\n0.776\n0.774\n89.297\n0.0\n0.624\n0.787\n0.739\n0.767\n108.814\n0.0\n\n\n4\n0.700\n0.776\n0.694\n0.759\n44.015\n0.0\n0.736\n0.807\n0.791\n0.801\n108.119\n0.0\n0.729\n0.821\n0.792\n0.799\n65.616\n0.0\n\n\n5\n0.743\n0.811\n0.753\n0.800\n45.246\n0.0\n0.766\n0.829\n0.812\n0.816\n90.656\n0.0\n0.759\n0.838\n0.796\n0.830\n81.758\n0.0\n\n\n6\n0.739\n0.798\n0.758\n0.793\n30.480\n0.0\n0.777\n0.834\n0.829\n0.823\n94.456\n0.0\n0.788\n0.849\n0.812\n0.839\n70.833\n0.0\n\n\n7\n0.782\n0.837\n0.771\n0.819\n44.531\n0.0\n0.778\n0.838\n0.831\n0.823\n113.771\n0.0\n0.795\n0.852\n0.821\n0.847\n75.129\n0.0\n\n\n8\n0.789\n0.836\n0.786\n0.821\n38.073\n0.0\n0.795\n0.840\n0.842\n0.833\n107.121\n0.0\n0.802\n0.858\n0.832\n0.852\n70.929\n0.0\n\n\n9\n0.813\n0.843\n0.813\n0.823\n24.215\n0.0\n0.803\n0.857\n0.846\n0.852\n128.881\n0.0\n0.819\n0.872\n0.849\n0.869\n78.168\n0.0\n\n\n10\n0.819\n0.854\n0.826\n0.838\n30.906\n0.0\n0.807\n0.858\n0.851\n0.849\n122.873\n0.0\n0.840\n0.875\n0.860\n0.874\n60.891\n0.0\n\n\n11\n0.825\n0.856\n0.826\n0.838\n22.820\n0.0\n0.803\n0.854\n0.850\n0.849\n123.208\n0.0\n0.840\n0.874\n0.861\n0.870\n49.824\n0.0\n\n\n12\n0.831\n0.855\n0.830\n0.835\n17.955\n0.0\n0.803\n0.858\n0.852\n0.854\n142.441\n0.0\n0.847\n0.877\n0.866\n0.883\n61.943\n0.0\n\n\n13\n0.837\n0.861\n0.846\n0.851\n18.860\n0.0\n0.810\n0.859\n0.856\n0.856\n149.958\n0.0\n0.849\n0.887\n0.867\n0.888\n87.474\n0.0\n\n\n14\n0.841\n0.869\n0.848\n0.854\n19.856\n0.0\n0.815\n0.863\n0.861\n0.856\n159.488\n0.0\n0.855\n0.889\n0.875\n0.886\n64.162\n0.0\n\n\n15\n0.842\n0.869\n0.850\n0.852\n24.013\n0.0\n0.815\n0.864\n0.863\n0.858\n158.026\n0.0\n0.861\n0.889\n0.875\n0.890\n59.296\n0.0\n\n\n16\n0.841\n0.870\n0.850\n0.853\n31.019\n0.0\n0.814\n0.868\n0.866\n0.860\n168.872\n0.0\n0.864\n0.890\n0.882\n0.894\n75.081\n0.0\n\n\n17\n0.851\n0.877\n0.855\n0.856\n24.430\n0.0\n0.816\n0.867\n0.867\n0.861\n170.798\n0.0\n0.867\n0.895\n0.878\n0.894\n72.092\n0.0\n\n\n18\n0.846\n0.877\n0.854\n0.858\n27.748\n0.0\n0.815\n0.867\n0.869\n0.864\n170.957\n0.0\n0.867\n0.896\n0.879\n0.894\n72.640\n0.0\n\n\n19\n0.856\n0.873\n0.855\n0.859\n23.246\n0.0\n0.821\n0.869\n0.870\n0.868\n175.237\n0.0\n0.872\n0.896\n0.884\n0.896\n69.911\n0.0\n\n\n20\n0.852\n0.880\n0.864\n0.859\n27.445\n0.0\n0.817\n0.872\n0.872\n0.866\n178.248\n0.0\n0.876\n0.898\n0.886\n0.897\n66.134\n0.0\n\n\n21\n0.854\n0.884\n0.864\n0.863\n38.568\n0.0\n0.824\n0.871\n0.871\n0.867\n170.556\n0.0\n0.873\n0.897\n0.885\n0.900\n58.449\n0.0\n\n\n22\n0.861\n0.886\n0.865\n0.868\n34.760\n0.0\n0.822\n0.874\n0.875\n0.869\n198.656\n0.0\n0.879\n0.902\n0.889\n0.901\n65.920\n0.0\nCode\nshow(models_overall_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 8.2: Post-hoc tests for Table 8.1 to determine the best-performant models.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical comparison of models performance</span>"
    ]
  },
  {
    "objectID": "03.3_models.html#activity-wise-performance",
    "href": "03.3_models.html#activity-wise-performance",
    "title": "Statistical comparison of models performance",
    "section": "Activity-wise performance",
    "text": "Activity-wise performance\nThe following sections address the performance of the selected models in each activity and data source.\n\nSEATED\nTable 8.3, 8.4 show different results regarding the data source.\nUsing the smartphone dataset, the CNN-LSTM seems to perform well with low and high quantities of data, while the CNN and LSTM are also the best with high amounts of data. With the smartwatch dataset, the best results are obtained by the CNN, LSTM and CNN-LSTM with low amounts of data, while with higher amounts no significant differences are observed among models. Regarding the fused dataset, the CNN and the CNN-LSTM are the best-performing models, followed by the LSTM. The MLP provides the worst results.\n\n\nCode\nmodels_seated_tests, models_seated_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.SEATED, ActivityMetric.F1), \n    SOURCES, \n    MODELS\n)\nmodels_seated_tests\n\n\n\n\nTable 8.3: Statistical comparison of SEATED performance obtained by the models for each data source.\n\n\n\n\n\n\n\n\n\n\nsp\nsw\nfused\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.065\n0.000\n0.083\n0.084\n11.680\n0.009\n0.397\n0.772\n0.749\n0.644\n149.050\n0.000\n0.116\n0.388\n0.316\n0.136\n15.160\n0.002\n\n\n2\n0.210\n0.273\n0.176\n0.737\n65.804\n0.000\n0.706\n0.830\n0.823\n0.826\n85.527\n0.000\n0.456\n0.716\n0.650\n0.705\n31.707\n0.000\n\n\n3\n0.245\n0.593\n0.431\n0.788\n160.718\n0.000\n0.805\n0.849\n0.844\n0.850\n44.039\n0.000\n0.543\n0.800\n0.741\n0.816\n117.621\n0.000\n\n\n4\n0.498\n0.733\n0.587\n0.800\n79.843\n0.000\n0.840\n0.864\n0.851\n0.858\n19.856\n0.000\n0.673\n0.823\n0.800\n0.827\n79.399\n0.000\n\n\n5\n0.660\n0.783\n0.653\n0.812\n82.175\n0.000\n0.841\n0.868\n0.856\n0.858\n17.609\n0.001\n0.685\n0.839\n0.800\n0.853\n97.686\n0.000\n\n\n6\n0.645\n0.774\n0.669\n0.797\n63.842\n0.000\n0.850\n0.869\n0.857\n0.875\n24.199\n0.000\n0.720\n0.839\n0.800\n0.849\n75.313\n0.000\n\n\n7\n0.727\n0.794\n0.735\n0.821\n70.529\n0.000\n0.849\n0.866\n0.862\n0.872\n12.677\n0.005\n0.702\n0.854\n0.828\n0.868\n110.599\n0.000\n\n\n8\n0.738\n0.810\n0.758\n0.811\n57.865\n0.000\n0.868\n0.873\n0.867\n0.877\n7.535\n0.057\n0.717\n0.861\n0.833\n0.870\n126.446\n0.000\n\n\n9\n0.778\n0.833\n0.797\n0.836\n38.683\n0.000\n0.871\n0.880\n0.870\n0.879\n8.419\n0.038\n0.793\n0.861\n0.845\n0.870\n69.917\n0.000\n\n\n10\n0.767\n0.830\n0.794\n0.833\n47.298\n0.000\n0.872\n0.877\n0.871\n0.881\n9.849\n0.020\n0.787\n0.872\n0.844\n0.875\n89.605\n0.000\n\n\n11\n0.780\n0.827\n0.799\n0.830\n38.571\n0.000\n0.870\n0.878\n0.866\n0.878\n4.333\n0.228\n0.798\n0.867\n0.844\n0.865\n68.845\n0.000\n\n\n12\n0.787\n0.833\n0.811\n0.833\n28.954\n0.000\n0.876\n0.880\n0.876\n0.882\n4.283\n0.232\n0.808\n0.876\n0.862\n0.875\n71.107\n0.000\n\n\n13\n0.807\n0.831\n0.806\n0.831\n16.346\n0.001\n0.877\n0.884\n0.877\n0.878\n5.176\n0.159\n0.816\n0.871\n0.847\n0.874\n69.197\n0.000\n\n\n14\n0.802\n0.842\n0.828\n0.828\n18.119\n0.000\n0.877\n0.885\n0.872\n0.879\n3.501\n0.321\n0.813\n0.877\n0.863\n0.874\n63.730\n0.000\n\n\n15\n0.801\n0.837\n0.830\n0.833\n18.467\n0.000\n0.873\n0.882\n0.876\n0.882\n3.256\n0.354\n0.835\n0.872\n0.858\n0.880\n54.007\n0.000\n\n\n16\n0.809\n0.847\n0.823\n0.843\n27.844\n0.000\n0.875\n0.885\n0.887\n0.886\n6.688\n0.083\n0.841\n0.876\n0.857\n0.880\n52.211\n0.000\n\n\n17\n0.807\n0.848\n0.830\n0.836\n15.002\n0.002\n0.881\n0.880\n0.876\n0.881\n1.035\n0.793\n0.833\n0.877\n0.865\n0.880\n55.932\n0.000\n\n\n18\n0.807\n0.842\n0.837\n0.843\n16.857\n0.001\n0.882\n0.885\n0.886\n0.882\n2.252\n0.522\n0.838\n0.875\n0.861\n0.880\n50.721\n0.000\n\n\n19\n0.812\n0.842\n0.829\n0.837\n11.724\n0.008\n0.880\n0.889\n0.878\n0.881\n3.603\n0.308\n0.846\n0.875\n0.865\n0.885\n53.275\n0.000\n\n\n20\n0.813\n0.851\n0.830\n0.844\n20.166\n0.000\n0.885\n0.884\n0.883\n0.884\n1.086\n0.780\n0.842\n0.877\n0.868\n0.883\n51.086\n0.000\n\n\n21\n0.814\n0.854\n0.842\n0.837\n17.413\n0.001\n0.875\n0.881\n0.886\n0.883\n4.813\n0.186\n0.844\n0.875\n0.867\n0.885\n56.551\n0.000\n\n\n22\n0.826\n0.850\n0.851\n0.842\n13.438\n0.004\n0.879\n0.887\n0.887\n0.883\n5.229\n0.156\n0.845\n0.875\n0.867\n0.885\n48.592\n0.000\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nshow(models_seated_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 8.4: Post-hoc tests for Table 8.3 to determine the best-performant models.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nSTANDING_UP\nThe results in Table 8.5, 8.6 show that the CNN models are the best-performing with any amount of data and data source. The CNN-LSTM are also the best-performing with the smartphone and fused datasets with any quantity of data, while with the smartwatch dataset it struggles with low quantities of data. The LSTM also performs well with high amounts of data using the smartphone and smartwatch datasets. It also provides better results than the MLP with the fused dataset.\n\n\nCode\nmodels_standing_tests, models_standing_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.STANDING_UP, ActivityMetric.F1), \n    SOURCES, \n    MODELS\n)\nmodels_standing_tests\n\n\n\n\nTable 8.5: Statistical comparison of STANDING_UP performance obtained by the models for each data source.\n\n\n\n\n\n\n\n\n\n\nsp\nsw\nfused\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.060\n0.130\n0.093\n0.139\n7.222\n0.065\n0.434\n0.583\n0.546\n0.517\n75.799\n0.0\n0.141\n0.320\n0.290\n0.275\n42.826\n0.0\n\n\n2\n0.209\n0.362\n0.272\n0.318\n8.956\n0.030\n0.555\n0.675\n0.632\n0.656\n62.556\n0.0\n0.320\n0.563\n0.481\n0.487\n37.790\n0.0\n\n\n3\n0.324\n0.482\n0.369\n0.494\n23.561\n0.000\n0.563\n0.716\n0.667\n0.681\n118.717\n0.0\n0.469\n0.667\n0.550\n0.646\n56.076\n0.0\n\n\n4\n0.485\n0.609\n0.470\n0.600\n20.113\n0.000\n0.613\n0.767\n0.717\n0.730\n128.312\n0.0\n0.594\n0.738\n0.670\n0.722\n43.020\n0.0\n\n\n5\n0.496\n0.657\n0.541\n0.638\n29.682\n0.000\n0.634\n0.781\n0.753\n0.760\n145.508\n0.0\n0.614\n0.749\n0.699\n0.742\n54.173\n0.0\n\n\n6\n0.514\n0.643\n0.528\n0.626\n13.981\n0.003\n0.637\n0.800\n0.771\n0.772\n164.682\n0.0\n0.626\n0.766\n0.701\n0.760\n44.998\n0.0\n\n\n7\n0.590\n0.693\n0.564\n0.699\n28.916\n0.000\n0.658\n0.809\n0.768\n0.786\n169.250\n0.0\n0.667\n0.777\n0.713\n0.777\n65.890\n0.0\n\n\n8\n0.607\n0.713\n0.642\n0.699\n21.332\n0.000\n0.674\n0.804\n0.796\n0.788\n158.415\n0.0\n0.678\n0.793\n0.732\n0.793\n56.438\n0.0\n\n\n9\n0.651\n0.745\n0.704\n0.726\n18.538\n0.000\n0.694\n0.823\n0.811\n0.805\n198.948\n0.0\n0.731\n0.813\n0.777\n0.809\n51.521\n0.0\n\n\n10\n0.643\n0.769\n0.707\n0.743\n30.327\n0.000\n0.698\n0.830\n0.824\n0.818\n182.902\n0.0\n0.732\n0.818\n0.789\n0.822\n49.148\n0.0\n\n\n11\n0.681\n0.775\n0.724\n0.771\n22.470\n0.000\n0.696\n0.823\n0.814\n0.814\n185.085\n0.0\n0.735\n0.827\n0.791\n0.824\n51.660\n0.0\n\n\n12\n0.675\n0.748\n0.721\n0.756\n14.996\n0.002\n0.688\n0.830\n0.825\n0.827\n208.469\n0.0\n0.740\n0.825\n0.786\n0.840\n50.615\n0.0\n\n\n13\n0.696\n0.775\n0.762\n0.771\n18.254\n0.000\n0.693\n0.833\n0.832\n0.819\n202.887\n0.0\n0.766\n0.841\n0.800\n0.847\n63.189\n0.0\n\n\n14\n0.712\n0.788\n0.765\n0.777\n19.215\n0.000\n0.697\n0.832\n0.835\n0.826\n189.750\n0.0\n0.755\n0.838\n0.813\n0.839\n50.068\n0.0\n\n\n15\n0.721\n0.792\n0.765\n0.769\n21.447\n0.000\n0.703\n0.845\n0.842\n0.839\n220.215\n0.0\n0.769\n0.848\n0.821\n0.851\n53.440\n0.0\n\n\n16\n0.726\n0.800\n0.765\n0.791\n22.335\n0.000\n0.715\n0.843\n0.841\n0.836\n203.906\n0.0\n0.779\n0.857\n0.830\n0.861\n68.146\n0.0\n\n\n17\n0.738\n0.791\n0.780\n0.783\n12.886\n0.005\n0.712\n0.842\n0.843\n0.832\n222.401\n0.0\n0.778\n0.857\n0.825\n0.857\n60.781\n0.0\n\n\n18\n0.724\n0.805\n0.779\n0.786\n20.594\n0.000\n0.716\n0.843\n0.854\n0.833\n198.738\n0.0\n0.789\n0.859\n0.825\n0.860\n53.781\n0.0\n\n\n19\n0.753\n0.809\n0.788\n0.798\n17.009\n0.001\n0.712\n0.847\n0.846\n0.831\n199.322\n0.0\n0.794\n0.860\n0.844\n0.870\n61.465\n0.0\n\n\n20\n0.749\n0.805\n0.789\n0.800\n18.931\n0.000\n0.714\n0.850\n0.852\n0.838\n211.687\n0.0\n0.798\n0.869\n0.845\n0.862\n48.278\n0.0\n\n\n21\n0.753\n0.809\n0.800\n0.802\n21.894\n0.000\n0.717\n0.847\n0.846\n0.837\n206.533\n0.0\n0.800\n0.871\n0.847\n0.865\n44.830\n0.0\n\n\n22\n0.745\n0.821\n0.792\n0.807\n26.722\n0.000\n0.714\n0.857\n0.851\n0.844\n220.480\n0.0\n0.800\n0.871\n0.859\n0.872\n51.819\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nshow(models_standing_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 8.6: Post-hoc tests for Table 8.5 to determine the best-performant models.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nWALKING\nThe results shown in Table 8.7, 8.8 indicate that the CNN models provide the best results with any amount of data across the three data sources. The CNN-LSTM obtains good results with low amounts of data with the smartphone dataset, while it also produces the best results with medium and high quantities of data with the fused dataset. The LSTM performs well using the smartwatch dataset, similar to the CNN. The MLP provides the worst results with the smartwatch dataset, although its results are not different from the LSTM using the smartphone and fused datasets.\n\n\nCode\nmodels_walking_tests, models_walking_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.WALKING, ActivityMetric.F1), \n    SOURCES, \n    MODELS\n)\nmodels_walking_tests\n\n\n\n\nTable 8.7: Statistical comparison of WALKING performance obtained by the models for each data source.\n\n\n\n\n\n\n\n\n\n\nsp\nsw\nfused\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.027\n0.471\n0.347\n0.509\n32.545\n0.000\n0.679\n0.765\n0.734\n0.736\n41.215\n0.0\n0.071\n0.620\n0.576\n0.603\n74.004\n0.000\n\n\n2\n0.599\n0.775\n0.728\n0.794\n36.256\n0.000\n0.778\n0.833\n0.801\n0.810\n22.720\n0.0\n0.667\n0.811\n0.759\n0.768\n43.791\n0.000\n\n\n3\n0.764\n0.826\n0.784\n0.820\n37.277\n0.000\n0.795\n0.852\n0.841\n0.843\n28.667\n0.0\n0.780\n0.861\n0.803\n0.836\n60.772\n0.000\n\n\n4\n0.817\n0.859\n0.799\n0.847\n31.366\n0.000\n0.815\n0.865\n0.860\n0.861\n53.271\n0.0\n0.813\n0.875\n0.847\n0.853\n44.495\n0.000\n\n\n5\n0.856\n0.889\n0.838\n0.874\n39.282\n0.000\n0.835\n0.883\n0.870\n0.867\n46.765\n0.0\n0.847\n0.888\n0.853\n0.883\n49.080\n0.000\n\n\n6\n0.845\n0.874\n0.833\n0.874\n25.530\n0.000\n0.848\n0.886\n0.877\n0.870\n39.992\n0.0\n0.856\n0.893\n0.860\n0.885\n40.431\n0.000\n\n\n7\n0.874\n0.899\n0.842\n0.898\n47.897\n0.000\n0.849\n0.887\n0.883\n0.871\n54.890\n0.0\n0.876\n0.901\n0.880\n0.894\n29.142\n0.000\n\n\n8\n0.870\n0.895\n0.848\n0.884\n35.927\n0.000\n0.855\n0.886\n0.889\n0.879\n47.876\n0.0\n0.882\n0.904\n0.877\n0.898\n30.463\n0.000\n\n\n9\n0.893\n0.905\n0.882\n0.887\n21.562\n0.000\n0.859\n0.900\n0.893\n0.893\n73.398\n0.0\n0.887\n0.915\n0.890\n0.914\n44.828\n0.000\n\n\n10\n0.898\n0.910\n0.884\n0.904\n23.344\n0.000\n0.865\n0.901\n0.894\n0.892\n61.901\n0.0\n0.899\n0.915\n0.904\n0.913\n22.228\n0.000\n\n\n11\n0.898\n0.907\n0.879\n0.899\n15.977\n0.001\n0.868\n0.897\n0.894\n0.892\n60.148\n0.0\n0.899\n0.912\n0.902\n0.912\n14.110\n0.003\n\n\n12\n0.905\n0.911\n0.887\n0.896\n18.304\n0.000\n0.866\n0.901\n0.899\n0.893\n78.130\n0.0\n0.904\n0.914\n0.904\n0.919\n26.160\n0.000\n\n\n13\n0.905\n0.917\n0.900\n0.906\n17.876\n0.000\n0.869\n0.901\n0.898\n0.896\n80.415\n0.0\n0.908\n0.924\n0.907\n0.925\n48.442\n0.000\n\n\n14\n0.909\n0.916\n0.903\n0.910\n14.686\n0.002\n0.870\n0.907\n0.906\n0.898\n102.948\n0.0\n0.908\n0.925\n0.917\n0.925\n31.878\n0.000\n\n\n15\n0.909\n0.922\n0.902\n0.908\n18.750\n0.000\n0.874\n0.908\n0.905\n0.900\n85.587\n0.0\n0.915\n0.925\n0.914\n0.927\n33.798\n0.000\n\n\n16\n0.910\n0.925\n0.904\n0.909\n29.269\n0.000\n0.869\n0.909\n0.905\n0.899\n100.920\n0.0\n0.914\n0.925\n0.919\n0.930\n39.304\n0.000\n\n\n17\n0.914\n0.925\n0.903\n0.910\n24.184\n0.000\n0.871\n0.908\n0.909\n0.902\n106.378\n0.0\n0.918\n0.928\n0.916\n0.928\n33.331\n0.000\n\n\n18\n0.913\n0.928\n0.909\n0.914\n23.263\n0.000\n0.873\n0.908\n0.906\n0.902\n100.240\n0.0\n0.916\n0.928\n0.918\n0.933\n36.701\n0.000\n\n\n19\n0.916\n0.927\n0.905\n0.918\n31.125\n0.000\n0.875\n0.908\n0.906\n0.904\n106.258\n0.0\n0.918\n0.929\n0.919\n0.932\n34.601\n0.000\n\n\n20\n0.916\n0.928\n0.912\n0.917\n24.246\n0.000\n0.874\n0.912\n0.910\n0.906\n109.615\n0.0\n0.922\n0.930\n0.925\n0.932\n30.369\n0.000\n\n\n21\n0.917\n0.931\n0.915\n0.918\n35.637\n0.000\n0.880\n0.913\n0.913\n0.905\n97.691\n0.0\n0.923\n0.931\n0.925\n0.933\n26.441\n0.000\n\n\n22\n0.918\n0.932\n0.912\n0.921\n32.243\n0.000\n0.877\n0.913\n0.911\n0.907\n126.709\n0.0\n0.923\n0.933\n0.925\n0.935\n34.526\n0.000\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nshow(models_walking_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 8.8: Post-hoc tests for Table 8.7 to determine the best-performant models.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nTURNING\nTable 8.9, 8.10 show that the CNN, LSTM and CNN-LSTM obtain the best results in the smartwatch dataset. These three models also perform well with low amounts of data using the smartphone and fused datasets. However, no significant differences among models are observed after \\(n \\geq 5\\) and \\(n \\geq 7\\), respectively. In the case of the smartphone dataset, significant differences appear after \\(n \\geq 15\\), with the MLP and the CNN being the best. With the fused dataset, after \\(n \\geq 21\\), the LSTM provides the significantly worse results.\n\n\nCode\nmodels_turning_tests, models_turning_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.TURNING, ActivityMetric.F1), \n    SOURCES, \n    MODELS\n)\nmodels_turning_tests\n\n\n\n\nTable 8.9: Statistical comparison of TURNING performance obtained by the models for each data source.\n\n\n\n\n\n\n\n\n\n\nsp\nsw\nfused\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.151\n0.589\n0.402\n0.568\n100.751\n0.000\n0.412\n0.648\n0.514\n0.574\n232.008\n0.0\n0.344\n0.534\n0.478\n0.497\n82.072\n0.000\n\n\n2\n0.636\n0.779\n0.744\n0.763\n27.331\n0.000\n0.614\n0.692\n0.642\n0.663\n67.321\n0.0\n0.451\n0.761\n0.768\n0.682\n103.413\n0.000\n\n\n3\n0.733\n0.796\n0.784\n0.773\n19.168\n0.000\n0.674\n0.728\n0.715\n0.709\n57.915\n0.0\n0.648\n0.796\n0.783\n0.759\n94.346\n0.000\n\n\n4\n0.783\n0.820\n0.813\n0.818\n11.748\n0.008\n0.670\n0.742\n0.728\n0.727\n67.707\n0.0\n0.738\n0.810\n0.818\n0.785\n35.390\n0.000\n\n\n5\n0.829\n0.826\n0.811\n0.839\n7.205\n0.066\n0.701\n0.753\n0.746\n0.741\n47.987\n0.0\n0.784\n0.827\n0.835\n0.809\n35.670\n0.000\n\n\n6\n0.821\n0.827\n0.812\n0.827\n5.653\n0.130\n0.714\n0.766\n0.761\n0.757\n64.089\n0.0\n0.797\n0.823\n0.827\n0.822\n18.993\n0.000\n\n\n7\n0.840\n0.842\n0.825\n0.844\n5.340\n0.149\n0.716\n0.773\n0.763\n0.758\n64.602\n0.0\n0.828\n0.837\n0.835\n0.830\n8.279\n0.041\n\n\n8\n0.836\n0.849\n0.832\n0.846\n5.351\n0.148\n0.723\n0.772\n0.780\n0.759\n67.717\n0.0\n0.834\n0.839\n0.841\n0.832\n3.417\n0.332\n\n\n9\n0.847\n0.843\n0.838\n0.839\n3.940\n0.268\n0.728\n0.786\n0.773\n0.781\n72.782\n0.0\n0.836\n0.849\n0.845\n0.840\n6.762\n0.080\n\n\n10\n0.854\n0.854\n0.851\n0.844\n1.941\n0.585\n0.734\n0.793\n0.786\n0.786\n76.116\n0.0\n0.846\n0.849\n0.851\n0.847\n0.834\n0.841\n\n\n11\n0.845\n0.852\n0.845\n0.846\n3.307\n0.347\n0.728\n0.784\n0.784\n0.782\n78.787\n0.0\n0.850\n0.849\n0.850\n0.839\n7.096\n0.069\n\n\n12\n0.853\n0.859\n0.848\n0.848\n7.396\n0.060\n0.726\n0.794\n0.786\n0.790\n95.204\n0.0\n0.859\n0.849\n0.855\n0.854\n2.081\n0.556\n\n\n13\n0.856\n0.864\n0.852\n0.860\n7.294\n0.063\n0.736\n0.799\n0.788\n0.785\n95.504\n0.0\n0.859\n0.860\n0.851\n0.859\n1.590\n0.662\n\n\n14\n0.856\n0.860\n0.850\n0.856\n3.968\n0.265\n0.743\n0.797\n0.802\n0.791\n88.731\n0.0\n0.862\n0.862\n0.855\n0.855\n6.602\n0.086\n\n\n15\n0.861\n0.867\n0.854\n0.857\n10.448\n0.015\n0.745\n0.802\n0.799\n0.795\n95.701\n0.0\n0.865\n0.860\n0.860\n0.856\n4.725\n0.193\n\n\n16\n0.862\n0.867\n0.860\n0.856\n8.346\n0.039\n0.738\n0.802\n0.807\n0.794\n101.760\n0.0\n0.863\n0.862\n0.860\n0.857\n0.397\n0.941\n\n\n17\n0.866\n0.871\n0.855\n0.860\n14.598\n0.002\n0.735\n0.801\n0.806\n0.806\n116.932\n0.0\n0.868\n0.862\n0.859\n0.860\n5.646\n0.130\n\n\n18\n0.864\n0.866\n0.858\n0.858\n9.824\n0.020\n0.743\n0.802\n0.805\n0.800\n107.298\n0.0\n0.862\n0.867\n0.860\n0.862\n3.121\n0.373\n\n\n19\n0.866\n0.869\n0.853\n0.860\n14.999\n0.002\n0.742\n0.807\n0.812\n0.805\n106.796\n0.0\n0.864\n0.869\n0.859\n0.863\n3.746\n0.290\n\n\n20\n0.864\n0.870\n0.859\n0.864\n13.366\n0.004\n0.742\n0.812\n0.815\n0.799\n126.077\n0.0\n0.868\n0.868\n0.860\n0.866\n5.491\n0.139\n\n\n21\n0.865\n0.874\n0.860\n0.862\n15.351\n0.002\n0.750\n0.819\n0.812\n0.803\n125.665\n0.0\n0.870\n0.869\n0.859\n0.864\n12.210\n0.007\n\n\n22\n0.864\n0.870\n0.861\n0.865\n8.253\n0.041\n0.743\n0.812\n0.817\n0.807\n134.359\n0.0\n0.868\n0.870\n0.861\n0.866\n10.585\n0.014\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nshow(models_turning_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 8.10: Post-hoc tests for Table 8.9 to determine the best-performant models.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nSITTING_DOWN\nThe results from Table 8.11, 8.12 indicate that the CNN is the best-performing model with any quantity of data across data sources. The CNN-LSTM also performs well with any amount of data using the fused dataset, while also showing a good performance with low and medium amounts of data using the smartphone and smartwatch datasets. The LSTM performs well with high amounts of data using the smartphone and smartwatch datasets, and provides better results than the MLP model using the fused dataset. In the case of the MLP, it provides the worst results in any scenario.\n\n\nCode\nmodels_sitting_tests, models_sitting_posthoc = statistical_comparison(\n    reports,\n    (TargetFilter.SITTING_DOWN, ActivityMetric.F1), \n    SOURCES, \n    MODELS\n)\nmodels_sitting_tests\n\n\n\n\nTable 8.11: Statistical comparison of SITTING_DOWN performance obtained by the models for each data source.\n\n\n\n\n\n\n\n\n\n\nsp\nsw\nfused\n\n\n\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\nmlp\ncnn\nlstm\ncnn-lstm\nH(3)\np-value\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.108\n0.102\n0.094\n0.152\n3.785\n0.286\n0.368\n0.583\n0.499\n0.479\n139.149\n0.0\n0.100\n0.238\n0.252\n0.168\n39.136\n0.0\n\n\n2\n0.207\n0.363\n0.305\n0.265\n11.881\n0.008\n0.458\n0.657\n0.649\n0.624\n190.870\n0.0\n0.346\n0.581\n0.511\n0.478\n33.878\n0.0\n\n\n3\n0.286\n0.473\n0.406\n0.471\n35.016\n0.000\n0.535\n0.725\n0.691\n0.717\n170.778\n0.0\n0.443\n0.671\n0.610\n0.620\n54.230\n0.0\n\n\n4\n0.420\n0.619\n0.502\n0.572\n35.908\n0.000\n0.603\n0.754\n0.718\n0.730\n147.828\n0.0\n0.569\n0.712\n0.680\n0.709\n46.529\n0.0\n\n\n5\n0.507\n0.651\n0.547\n0.613\n32.061\n0.000\n0.632\n0.768\n0.750\n0.748\n140.511\n0.0\n0.612\n0.753\n0.654\n0.754\n91.191\n0.0\n\n\n6\n0.518\n0.650\n0.617\n0.632\n27.322\n0.000\n0.633\n0.786\n0.766\n0.766\n176.413\n0.0\n0.642\n0.779\n0.711\n0.780\n79.841\n0.0\n\n\n7\n0.594\n0.704\n0.597\n0.680\n38.170\n0.000\n0.659\n0.793\n0.778\n0.778\n166.434\n0.0\n0.658\n0.800\n0.738\n0.812\n109.438\n0.0\n\n\n8\n0.612\n0.705\n0.634\n0.693\n33.109\n0.000\n0.667\n0.806\n0.794\n0.786\n168.501\n0.0\n0.679\n0.811\n0.754\n0.807\n94.447\n0.0\n\n\n9\n0.659\n0.738\n0.689\n0.692\n30.200\n0.000\n0.688\n0.822\n0.808\n0.806\n175.911\n0.0\n0.698\n0.827\n0.785\n0.815\n92.280\n0.0\n\n\n10\n0.660\n0.751\n0.736\n0.724\n35.525\n0.000\n0.695\n0.829\n0.815\n0.802\n169.903\n0.0\n0.731\n0.841\n0.792\n0.828\n88.379\n0.0\n\n\n11\n0.669\n0.766\n0.739\n0.727\n29.183\n0.000\n0.695\n0.828\n0.809\n0.811\n178.988\n0.0\n0.728\n0.828\n0.786\n0.828\n95.685\n0.0\n\n\n12\n0.687\n0.765\n0.743\n0.736\n22.593\n0.000\n0.697\n0.824\n0.818\n0.805\n179.573\n0.0\n0.743\n0.843\n0.814\n0.842\n101.743\n0.0\n\n\n13\n0.710\n0.774\n0.753\n0.756\n19.137\n0.000\n0.701\n0.827\n0.817\n0.804\n185.931\n0.0\n0.765\n0.850\n0.809\n0.856\n105.213\n0.0\n\n\n14\n0.717\n0.784\n0.765\n0.753\n21.136\n0.000\n0.706\n0.832\n0.823\n0.811\n202.297\n0.0\n0.762\n0.860\n0.831\n0.849\n106.044\n0.0\n\n\n15\n0.713\n0.787\n0.778\n0.761\n26.514\n0.000\n0.714\n0.834\n0.828\n0.814\n186.076\n0.0\n0.783\n0.857\n0.824\n0.857\n75.894\n0.0\n\n\n16\n0.714\n0.791\n0.783\n0.759\n37.802\n0.000\n0.704\n0.845\n0.831\n0.823\n218.240\n0.0\n0.785\n0.848\n0.838\n0.865\n95.678\n0.0\n\n\n17\n0.734\n0.809\n0.796\n0.774\n28.286\n0.000\n0.720\n0.837\n0.833\n0.822\n182.075\n0.0\n0.786\n0.857\n0.832\n0.860\n86.950\n0.0\n\n\n18\n0.739\n0.800\n0.795\n0.768\n30.999\n0.000\n0.715\n0.846\n0.833\n0.824\n191.156\n0.0\n0.785\n0.869\n0.837\n0.866\n120.677\n0.0\n\n\n19\n0.743\n0.806\n0.786\n0.769\n27.103\n0.000\n0.722\n0.845\n0.844\n0.823\n204.957\n0.0\n0.800\n0.873\n0.839\n0.870\n86.678\n0.0\n\n\n20\n0.750\n0.807\n0.796\n0.783\n32.416\n0.000\n0.714\n0.838\n0.833\n0.822\n201.035\n0.0\n0.802\n0.873\n0.850\n0.876\n92.564\n0.0\n\n\n21\n0.744\n0.816\n0.806\n0.788\n37.110\n0.000\n0.730\n0.840\n0.841\n0.822\n180.729\n0.0\n0.813\n0.872\n0.840\n0.871\n91.379\n0.0\n\n\n22\n0.768\n0.821\n0.800\n0.790\n29.114\n0.000\n0.726\n0.850\n0.842\n0.829\n211.321\n0.0\n0.810\n0.879\n0.846\n0.876\n94.619\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nshow(models_sitting_posthoc, classes=\"display nowrap compact\")\n\n\n\n\nTable 8.12: Post-hoc tests for Table 8.11 to determine the best-performant models.\n\n\n\n\n\n    \n      \n      \n      A\n      B\n      mean(A)\n      std(A)\n      mean(B)\n      std(B)\n      U\n      p-value\n    \n    \n      focus\n      n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  Loading... (need help?)",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical comparison of models performance</span>"
    ]
  },
  {
    "objectID": "03.3_models.html#summary",
    "href": "03.3_models.html#summary",
    "title": "Statistical comparison of models performance",
    "section": "Summary",
    "text": "Summary\nThe results obtained in the executed analyses show that the CNN is always the best-performing model in terms of overall accuracy for all data sources and any amount of data. The LSTM also performs well with the smartwatch dataset, and the CNN-LSTM with the smartwatch and fused datasets. The MLP is the worst performing model across data sources, where the differences between the smartwatch- and fused-trained models are significant.\nRegarding activity-wise performance, the CNN model presents the best results in every activity and data source. The LSTM performs similarly to the CNN model using the smartwatch dataset in all activities and the smartphone dataset in all activities except TURNING. The performance of the CNN-LSTM seems to work well in some activities on the smartphone and smartwatch datasets, although their results are a bit unstable. On the other hand, the CNN-LSTM shines with the fused dataset, obtaining similar results as the CNN. The MLP model presents the worst results in every case except the WALKING and TURNING with the smartphone dataset.\nThese results are graphically summarized in Figure 8.1, 8.2, representing the best-performing model in terms of overall accuracy and activities F1-score. Following, some examples are given to show how to interpret the figure: in the SITTING_DOWN activity and for \\(n=1\\), when using the smartphone dataset no significant differences among model types are observed; with the smartwatch dataset, the CNN model statistically obtains the best performance, and with the fused dataset, the LSTM has the best performance, although not statistically better when compared with another model (whether MLP, CNN or CNN-LSTM, it should be determined by checking Table 8.11).\nThe figure shows a clear dominance by the CNN model, where it still provides the best metrics even when there is no significant difference compared to other models. In addition, when it does not provide the best results, they are still not significantly different from the best models on most occasions. It is noticeable the lack of influence of the model in the SEATED activity with smartwatch dataset, and in the TURNING activity with smartphone and fused datasets.\nIn summary, it can be stated that the CNN model is the best of the considered ones since it performs well in every situation. In addition, the LSTM and the CNN-LSTM would also be a feasible option when using smartwatch and fused data. The usage of the MLP model would be strongly discouraged since here and in related works it obtains the worst results.\n\n\nCode\nsources_results = {\n    TargetFilter.MODEL: models_overlall_tests, \n    TargetFilter.SEATED: models_seated_tests, \n    TargetFilter.STANDING_UP: models_standing_tests, \n    TargetFilter.WALKING: models_walking_tests, \n    TargetFilter.TURNING: models_turning_tests, \n    TargetFilter.SITTING_DOWN: models_sitting_tests\n}\n\nbest_sources = obtain_best_items(sources_results, MODELS, SOURCES)\nsignificance_sources = load_best_significant(SIGNIFICANCE_FILE)\n\nplot_visual_comparison(best_sources, significance_sources, MODELS, SOURCES)\n\n\n\n\n                                                \n\n\nFigure 8.1: Graphical representation of best models for each metric, data source and amount of data combination. Symbology: ▲ (MLP), ■ (CNN), ◆ (LSTM) and ● (CNN-LSTM).\n\n\n\n\n\n\nCode\nplot_visual_ties(best_sources, significance_sources, MODELS, SOURCES)\n\n\n\n\n                                                \n\n\nFigure 8.2: Visual representation of performance ties. The plot indicates the amount of times a specific model tied with each other.",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical comparison of models performance</span>"
    ]
  },
  {
    "objectID": "03.3_models.html#code-reference",
    "href": "03.3_models.html#code-reference",
    "title": "Statistical comparison of models performance",
    "section": "Code reference",
    "text": "Code reference\n\n\n\n\n\n\nTip\n\n\n\nThe documentation of the Python functions employed in this section can be found in Chapter 3 reference:\n\ndata_loading:\n\nload_reports\nload_best_significant\n\nmodel:\n\nActivityMetric\nModel\nModelMetric\nSource\nTargetFilter\nobtain_best_items\n\nstatistical_tests:\n\nstatistical_comparison\n\nvisualization:\n\nplot_visual_comparison\nplot_visual_ties",
    "crumbs": [
      "Multidimensional Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical comparison of models performance</span>"
    ]
  },
  {
    "objectID": "04.1_splitting-approach.html",
    "href": "04.1_splitting-approach.html",
    "title": "Splitting approach analysis",
    "section": "",
    "text": "Results\nCode\nfrom libs.chapter4.analysis.data_loading import load_reports\nfrom libs.chapter4.analysis.statistical_tests import compare_splitting_approaches\n\nreports = load_reports()\nTable 9.1 compares the overall accuracy and F1-scores of TURNING and SITTING_DOWN from one side, and TURN_TO_SIT activities from the other side, obtained for each data source – smartwatch (sw) or smartphone (sp) – and splitting approach – turning and sitting down (ts) or turn_to_sit (tts) – from the trained models. The overall accuracy obtained with the models trained with the ts is statistically better than the ones trained with the tts datasets. Moreover, the F1-score of the TURNING activity is statistically worse in the tts datasets due to the reduced number of training samples for that activity compared with the ts datasets, caused by the fact that the TURN_TO_SIT activity includes the TURNING activity (which is one of the other activities to be individually detected). In addition, the F1-score of the TURN_TO_SIT activity is low compared with the scores of TURNING and SITTING_DOWN in the ts datasets.\nCode\ndef get_accuracy_and_f1_scores(reports):\n    results = {}\n    for dataset_key, dataset_reports in reports.items():\n        source = dataset_key.split('_')[0]\n        if source not in results:\n            results[source] = {}\n\n        dataset_accuracies = []\n        dataset_turning = []\n        dataset_specific = []\n\n        specific_act, specific_score, key = ('SITTING_DOWN', 'f1-sitting-down', 'ts') if 'turning_and_sitting' in dataset_key else ('TURN_TO_SIT', 'f1-turn_to_sit', 'tts')\n            \n        for dataset_report in dataset_reports:\n            dataset_accuracies.append(dataset_report['accuracy'])\n            dataset_turning.append(dataset_report['TURNING']['f1-score'])\n            dataset_specific.append(dataset_report[specific_act]['f1-score'])\n\n        results[source][key] = {\n            'accuracy': dataset_accuracies,\n            'f1-turning': dataset_turning,\n            f'{specific_score}': dataset_specific\n        }\n\n    return results\n\nresults = get_accuracy_and_f1_scores(reports)\ncomparison = compare_splitting_approaches(results, ['accuracy', 'f1-turning', 'f1-sitting-down', 'f1-turn_to_sit'])\ncomparison\n\n\n\n\nTable 9.1: Overall accuracy and F1-scores of TURNING, SITTING_DOWN and TURN_TO_SIT for each data source and splitting approach.\n\n\n\n\n\n\n\n\n\n\nsource\nmetric\nturning_sitting\nturn_to_sit\ntwo-tailed test\n\n\n\n\n0\nsw\naccuracy\n0.848\n0.809\nt(198)=13.459315696940894, p-val=0.0, power=1.0\n\n\n1\nsw\nf1-turning\n0.795\n0.565\nt(172.92193683041882)=44.34041443328367, p-val...\n\n\n2\nsw\nf1-sitting-down\n0.804\n-\n-\n\n\n3\nsw\nf1-turn_to_sit\n-\n0.735\n-\n\n\n4\nsp\naccuracy\n0.857\n0.789\nU=9036.0, p-val=0.0, power=1\n\n\n5\nsp\nf1-turning\n0.846\n0.529\nt(135.5274649940879)=57.06501597390656, p-val=...\n\n\n6\nsp\nf1-sitting-down\n0.753\n-\n-\n\n\n7\nsp\nf1-turn_to_sit\n-\n0.655\n-",
    "crumbs": [
      "HAR in mHealth: TUG test",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Splitting approach analysis</span>"
    ]
  },
  {
    "objectID": "04.1_splitting-approach.html#summary",
    "href": "04.1_splitting-approach.html#summary",
    "title": "Splitting approach analysis",
    "section": "Summary",
    "text": "Summary\nFrom these results, we conclude that more accurate results are obtained when considering TURNING and SITTING_DOWN as separate activities, compared to combining them, since not only the overall accuracy of the prediction model is better, but also the predictability for the TURNING activity. Therefore, the first approach (separate activities) will be used in the implementation and evaluation of the system.",
    "crumbs": [
      "HAR in mHealth: TUG test",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Splitting approach analysis</span>"
    ]
  },
  {
    "objectID": "04.1_splitting-approach.html#code-reference",
    "href": "04.1_splitting-approach.html#code-reference",
    "title": "Splitting approach analysis",
    "section": "Code reference",
    "text": "Code reference\n\n\n\n\n\n\nTip\n\n\n\nThe documentation of the Python functions employed in this section can be found in Chapter 4 reference:\n\ndata_loading:\n\nload_reports\n\nstatistical_tests:\n\ncompare_splitting_approaches\n\n\n\n\n\n\n\n\nAdame, M Reyes, Ahmed Al-Jawad, Michailas Romanovas, Markus A Hobert, Walter Maetzler, Knut Möller, and Yiannos Manoli. 2012. “TUG Test Instrumentation for Parkinson’s Disease Patients Using Inertial Sensors and Dynamic Time Warping.” Biomedical Engineering/Biomedizinische Technik 57 (SI-1-Track-E): 1071–74. https://doi.org/10.1515/bmt-2012-4426.\n\n\nAnsai, Juliana Hotta, Ana Claudia Silva Farche, Paulo Giusti Rossi, Larissa Pires de Andrade, Theresa Helissa Nakagawa, and Anielle Cristhine de Medeiros Takahashi. 2019. “Performance of Different Timed up and Go Subtasks in Frailty Syndrome.” Journal of Geriatric Physical Therapy 42 (4): 287–93. https://doi.org/10.1519/JPT.0000000000000162.\n\n\nBeyea, James, Chris A McGibbon, Andrew Sexton, Jeremy Noble, and Colleen O’Connell. 2017. “Convergent Validity of a Wearable Sensor System for Measuring Sub-Task Performance During the Timed up-and-Go Test.” Sensors 17 (4): 934. https://doi.org/10.3390/S17040934.\n\n\nCoelln, Rainer von et al. 2019. “Quantitative Mobility Metrics from a Wearable Sensor Predict Incident Parkinsonism in Older Adults.” Parkinsonism & Related Disorders 65: 190–96. https://doi.org/10.1016/J.PARKRELDIS.2019.06.012.\n\n\nMadhushri, Priyanka, Armen A Dzhagaryan, Emil Jovanov, and Aleksandar Milenkovic. 2016. “A Smartphone Application Suite for Assessing Mobility.” In 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 3117–20. IEEE. https://doi.org/10.1109/EMBC.2016.7591389.\n\n\nMilosevic, Mladen, Emil Jovanov, and Aleksandar Milenković. 2013. “Quantifying Timed-up-and-Go Test: A Smartphone Implementation.” In 2013 IEEE International Conference on Body Sensor Networks, 1–6. IEEE. https://doi.org/10.1109/BSN.2013.6575478.\n\n\nSalarian, Arash et al. 2010. “iTUG, a Sensitive and Reliable Measure of Mobility.” IEEE Transactions on Neural Systems and Rehabilitation Engineering 18 (3): 303–10. https://doi.org/10.1109/TNSRE.2010.2047606.\n\n\nZakaria, Nor Aini, Yutaka Kuwae, Toshiyo Tamura, Kotaro Minato, and Shigehiko Kanaya. 2015. “Quantitative Analysis of Fall Risk Using TUG Test.” Computer Methods in Biomechanics and Biomedical Engineering 18 (4): 426–37. https://doi.org/10.1080/10255842.2013.805211.",
    "crumbs": [
      "HAR in mHealth: TUG test",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Splitting approach analysis</span>"
    ]
  },
  {
    "objectID": "04.2_system-reliability.html",
    "href": "04.2_system-reliability.html",
    "title": "System’s agreement and reliabilitty",
    "section": "",
    "text": "Subjects information\nTable 10.1 contains information (i.e., age, gender, number of TUG executions) regarding the participants in the evaluation of the system.\nCode\nshow(subjects_info)\nprint(age_info)\nprint(gender_info)\n\n\nAge info: min=21.00, max=73.00, mean=43.77, std=14.08\nGender info: male=16 (53.333333333333336), female=14 (46.666666666666664)\n\n\n\n\nTable 10.1: Information of the participants in the evaluation.\n\n\n\n\n\n    \n      \n      Id\n      Age\n      Gender\n      Valid\n    \n  Loading... (need help?)",
    "crumbs": [
      "HAR in mHealth: TUG test",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>System's agreement and reliabilitty</span>"
    ]
  },
  {
    "objectID": "04.2_system-reliability.html#analysis",
    "href": "04.2_system-reliability.html#analysis",
    "title": "System’s agreement and reliabilitty",
    "section": "Analysis",
    "text": "Analysis\n\nTUG results preparation\nIn some ocassions, although the systems are able to obtain valid results, the way these results are obtained is incorrect. For instance, the system might not be able to correctly detect when the subject has sit down (due to trousers/pocket characteristics), but ending up detecting the end of the test after the subject extracts the smartphone from the pocket and generating apparently valid results. Therefore, these results are not valid and must be marked as invalid. Following, these cases are reported:\n\ns10: all the results from the smartphone (C2) are invalid. The pocket of the subject was small, with half of the smartphone being outside it. When the subject sat down, the smartphone was in a position that the system was not able to recognize as SEATED.\ns11: the \\(4^{th}\\) result from the smartphone (C2) is invalid. The subject wore pants with loose pockets, and in the \\(4^{th}\\) execution was not able to detect the subject as being SEATED.\ns25: the \\(3^{rd}\\) and \\(5^{th}\\) results from the smartphone (C2) are invalid. The pocket of the subject was small, with half of the smartphone being outside it. When the subject sat down, the smartphone was in a position that the system was not able to recognize as SEATED.\n\nNext, these cases are manually modified in order to invalidate them (i.e., setting their duration to -1).\n\n\nCode\ninvalid_c2 = {\n    's10': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    's11': [4],\n    's25': [3, 5]\n}\n\nc2_results = invalidate_executions(c2_results, invalid_c2)\n\n\n\n\nAnalysis of errors\nEach results is classified depending on the information that the systems were able to obtain:\n\nsuccess: the system was able to compute the total duration of the test and the duration of every subphase.\npartial_success the was system was able to compute the total duration of the test, but failed to compute the duration of any of the subphases.\nfailure: the system was not able to compute the total duration of the test.\n\n\n\nCode\nprint('Analyzing C1 (smartwatch) results')\nc1_results['status'] = c1_results.apply(check_status, axis=1)\n\nprint('Analyzing C2 (smartphone) results')\nc2_results['status'] = c2_results.apply(check_status, axis=1)\n\nerrors_df = compute_errors_by_subject({ 'C1': c1_results, 'C2': c2_results }, man_results)\n\n\nAnalyzing C1 (smartwatch) results\nFound partial success: 19\nFound partial success: 23\nFound partial success: 24\nFound partial success: 26\nFound failure: 41\nFound failure: 43\nFound partial success: 80\nFound failure: 81\nFound partial success: 84\nFound failure: 104\nFound partial success: 110\nFound partial success: 155\nFound partial success: 163\nFound partial success: 164\nFound partial success: 192\nFound failure: 201\nFound partial success: 207\nFound partial success: 209\nFound partial success: 243\nFound partial success: 244\nFound partial success: 248\nFound failure: 275\nAnalyzing C2 (smartphone) results\nFound failure: 10\nFound failure: 11\nFound failure: 14\nFound failure: 16\nFound failure: 18\nFound failure: 43\nFound failure: 55\nFound failure: 89\nFound failure: 90\nFound failure: 91\nFound failure: 92\nFound failure: 93\nFound failure: 94\nFound failure: 95\nFound failure: 96\nFound failure: 97\nFound failure: 98\nFound failure: 102\nFound failure: 112\nFound partial success: 120\nFound failure: 130\nFound partial success: 132\nFound partial success: 135\nFound failure: 138\nFound failure: 139\nFound failure: 140\nFound failure: 141\nFound failure: 146\nFound partial success: 154\nFound failure: 191\nFound partial success: 197\nFound failure: 200\nFound failure: 201\nFound failure: 204\nFound partial success: 210\nFound failure: 223\nFound failure: 229\nFound failure: 231\nFound failure: 232\nFound failure: 233\nFound failure: 234\nFound failure: 235\nFound failure: 236\nFound failure: 237\nFound failure: 238\nFound failure: 239\nFound failure: 240\nFound failure: 241\nFound failure: 264\nFound failure: 266\nFound failure: 267\nFound failure: 268\nFound failure: 269\nFound failure: 270\nFound partial success: 282\n\n\nTable 10.2 shows the success, partial success and failure results classification. C1 obtained a higher success and partial success rate than C2 (\\(92.44\\%\\) and \\(5.5\\%\\) vs. \\(81.1\\%\\) and \\(2.41\\%\\)). Regarding failures, C1 obtained a better rate than C2 (\\(2.06\\%\\) vs \\(16.49\\%\\)).\nA deeper analysis of the failures showed that male participants were responsible for \\(5\\) out of the \\(6\\) failures in C1, where the system failed to recognize the standing_up subphase. On the other hand, female participants were responsible for \\(39\\) out of the \\(48\\) failures in C2, mainly due to wearing jeans with small pockets that placed the smartphone in an un-favourable position, which significantly affected the recognition of the sitting_down subphase.\n\n\nCode\nexecution_status = errors_df[['system', 'status']].groupby('system').value_counts().to_frame('counts').reset_index(level=1).pivot(columns='status', values='counts')\nexecution_status\n\n\n\n\nTable 10.2: Classified results obtained by the system in both configurations.\n\n\n\n\n\n\n\n\n\nstatus\nfailure\npartial_success\nsuccess\n\n\nsystem\n\n\n\n\n\n\n\nC1\n6\n16\n269\n\n\nC2\n48\n7\n236\n\n\n\n\n\n\n\n\n\n\nFigure 10.1 shows the distribution of the measurement error (i.e., system_measure \\(-\\) manual_measure) for the total duration (i.e., duration) of the test and each subphase (i.e., standing_up, first_walk, first_turn, second_walk, second_turn and sitting_down) for the executions classified as success. In addition, the mean of the distribution of each measurement is compared to \\(0\\) with a T-test or a W-test. Results indicate that the mean of the measurement errors of standing_up and sitting_down are not significantly different from \\(0\\) in both configurations, neither are the errors of first_turn and duration in C1 and C2, respectively. These results indicate that, on average, the system provides very accurate measurements for the mentioned metrics and configurations compared with the reference measurements.\n\n\nCode\nerror_distribution(errors_df)\n\n\n\n\n                                                \n\n\nFigure 10.1: Measurements error’s distribution for the total duration and each subphase for success executions in C1 and C2. At the bottom are the p-values of the one-sample T-test (green) or W-test (white) comparing the means of each distribution with \\(0\\), where a p-value &gt; \\(0.05\\) indicates no significant differences between the distribution mean and \\(0\\).\n\n\n\n\nTable 10.3 shows the inter-subject RMSE and the results of the statistical tests executed comparing each measure in both systems. C1 reported significantly better results in the duration, first_turn and sitting_down measures. Other not significant differences were observed in standing_up and second_walk in favour of C1, and in first_walk and second_turn in favour of C2. These results denote that the system performs better in C1 than in C2 when taking into account different subjects (i.e., C1 is more adaptable to different subjects).\n\n\nCode\ncompare_rmse_distributions(errors_df)\n\n\n\n\nTable 10.3: Comparison of the inter-subject RMSE of each measurement for both system configurations.\n\n\n\n\n\n\n\n\n\n\nMeasure\nM(C1)\nM(C2)\nTest\n\n\n\n\n0\nduration\n287.000000\n389.000000\nU=280.5, p-val=0.031, power=0.542\n\n\n1\nstanding_up\n286.766667\n336.821429\nt(56.00)=-1.23, p-val=0.224, power=0.227200896...\n\n\n2\nfirst_walk\n296.000000\n268.500000\nU=460.5, p-val=0.534, power=0.338\n\n\n3\nfirst_turn\n278.500000\n356.000000\nU=285.5, p-val=0.037, power=0.434\n\n\n4\nsecond_walk\n291.000000\n338.000000\nU=326.0, p-val=0.146, power=0.283\n\n\n5\nsecond_turn\n233.000000\n217.000000\nU=410.5, p-val=0.889, power=0.197\n\n\n6\nsitting_down\n269.500000\n322.500000\nU=280.0, p-val=0.03, power=0.603\n\n\n\n\n\n\n\n\n\n\n\n\nBland-Altman analysis\nTo determine the agreement between each system configuration and the reference measures, Figure 10.2 and Figure 10.3 show the Bland-Altman agreement analysis for the duration and the rest of the measures, respectively. For the duration measurement, C1 and C2 show an excellent mean of \\(-0.044\\) and \\(-0.051\\) with \\(95\\%\\) agreement limits of \\([-0.7,0.61]\\) and \\([-0.91,0.8]\\). These results indicate a low bias with regards to the reference method and that \\(5\\%\\) of the measurements exceed an error of \\(0.7\\) seconds in C1 and \\(0.9\\) in C2.\nRegarding the measures of the TUG subphases, C1 shows slightly better means (all under \\(0.1\\) seconds) than C2 in all subphases except for the standing_up. The limits of agreement in C1 are around \\(0.6-0.9\\) and in C2 exceed the second. C1 results indicate a low bias with respect to reference measures, while C2 shows biases of around \\(0.2\\) seconds for walking and turning activities, which are underestimated and overestimated, respectively.\n\nCode\nbland_altman_plot(c1_results, man_results, 'C1', ['duration'], with_titles=False).show()\nbland_altman_plot(c2_results, man_results, 'C2', ['duration'], with_titles=False).show()\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) C1 configuration\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) C2 configuration\n\n\n\n\n\n\n\nFigure 10.2: Blant-Altman analysis of the total duration of the test computed by the system in both configurations. The red line represents the mean difference and both dotted lines are the limits of agreement (with \\(95\\)% CI).\n\n\n\n\nCode\nbland_altman_plot(c1_results, man_results, 'C1', PHASES, limit_y_axis_to=[-2, 2]).show()\nbland_altman_plot(c2_results, man_results, 'C2', PHASES, limit_y_axis_to=[-2, 2]).show()\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) C1 configuration\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) C2 configuration\n\n\n\n\n\n\n\nFigure 10.3: Blant-Altman analysis of the TUG subphases of the test computed by the system in both configurations. The red line represents the mean difference and both dotted lines are the limits of agreement (with \\(95\\)% CI).\n\n\n\n\n\nIntraclass Correlation Coefficent (ICC) analysis\nTo measure the reliability of both configurations, Table 10.4 contains the ICC estimates and their \\(95\\%\\) confident intervals based on a single rater, absolute-agreement, 2-way mixed-effects model comparing the measures obtained by both system configurations with the reference ones. ICC results show excellent reliability for the duration in both system configurations. Regarding the subphases, C1 achieves excellent reliability in the second_walk; good reliability in first_walk; moderate reliability in second_turn and sitting_down, and poor reliability for the remaining ones, whereby the reliability of the first_turn is non-significant (i.e., CI contains 0). C2 achieves moderate reliability in first_walk and second_walk while obtaining poor reliability in the rest, with non-significant results for standing_up, first_turn and second_turn.\n\n\nCode\ncompute_icc([c1_results, c2_results], man_results, ['C1', 'C2'], icc_type='ICC2')\n\n\n\n\nTable 10.4: Intraclass correlation from single rater, absolute-agreement, 2-way mixed-effects model between system’s configurations measurements and the reference (manual) measurements.\n\n\n\n\n\n\n\n\n\n\n\nICC\nCI\nF Test\np-value\n\n\nphase\nsystem\n\n\n\n\n\n\n\n\nduration\nC1\n0.979\n[0.96, 0.99]\n94.521\n0.000\n\n\nC2\n0.972\n[0.94, 0.99]\n72.498\n0.000\n\n\nstanding_up\nC1\n0.456\n[0.12, 0.7]\n2.677\n0.005\n\n\nC2\n0.176\n[-0.22, 0.52]\n1.414\n0.187\n\n\nfirst_walk\nC1\n0.822\n[0.63, 0.91]\n11.755\n0.000\n\n\nC2\n0.687\n[0.32, 0.86]\n7.076\n0.000\n\n\nfirst_turn\nC1\n0.264\n[-0.11, 0.57]\n1.694\n0.081\n\n\nC2\n0.026\n[-0.19, 0.3]\n1.085\n0.417\n\n\nsecond_walk\nC1\n0.916\n[0.83, 0.96]\n24.004\n0.000\n\n\nC2\n0.627\n[0.1, 0.84]\n6.922\n0.000\n\n\nsecond_turn\nC1\n0.540\n[0.2, 0.76]\n3.987\n0.000\n\n\nC2\n0.107\n[-0.19, 0.42]\n1.298\n0.251\n\n\nsitting_down\nC1\n0.697\n[0.45, 0.84]\n5.471\n0.000\n\n\nC2\n0.454\n[0.1, 0.7]\n2.634\n0.007",
    "crumbs": [
      "HAR in mHealth: TUG test",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>System's agreement and reliabilitty</span>"
    ]
  },
  {
    "objectID": "04.2_system-reliability.html#summary",
    "href": "04.2_system-reliability.html#summary",
    "title": "System’s agreement and reliabilitty",
    "section": "Summary",
    "text": "Summary\nThese results (errors, Bland-Altman and ICC analyses) show that while the system is reliable in terms of measuring the total duration of the test, measurements of individual subphases must be considered carefully, especially when using the system with configuration C2. This is probably caused by the underestimation of some subphases while overestimating others, leading to a reliable total duration but less reliable subphases’ durations.\nWhen comparing C1 and C2 configurations, the presented results provide evidence that the smartwatch-based system performs better than the smartphone-based system, both for the overall test duration and for the individual subphases detection.\nOverall, the developed system, using off-the-shelf hardware (i.e., smartphone and smartwatch), presented state-of-the-art results in the computation of the TUG test duration, even when compared with systems using specialized sensor units and/or positioning sensing devices in unnatural positions (e.g., strapped to the arms or legs). This paves the way to real-life applicability (e.g., self-administration) of the TUG test and other similar tests. On the other hand, the computed duration of the TUG subphases must be considered carefully, especially in the C2 configuration, which is penalized given the diversity of front pockets and phone orientations.",
    "crumbs": [
      "HAR in mHealth: TUG test",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>System's agreement and reliabilitty</span>"
    ]
  },
  {
    "objectID": "04.2_system-reliability.html#code-reference",
    "href": "04.2_system-reliability.html#code-reference",
    "title": "System’s agreement and reliabilitty",
    "section": "Code reference",
    "text": "Code reference\n\n\n\n\n\n\nTip\n\n\n\nThe documentation of the Python functions employed in this section can be found in Chapter 4 reference:\n\ndata_loading:\n\nload_subjects_info\nload_experiment_results\n\nstatistical_tests:\n\ncompare_rmse_distributions\ncompute_icc\n\ntug_results_processing:\n\ncheck_status\ncompute_errors_by_subject\ninvalidate_executions\n\nvisualization:\n\nerror_distribution\nbland_altman_plot",
    "crumbs": [
      "HAR in mHealth: TUG test",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>System's agreement and reliabilitty</span>"
    ]
  },
  {
    "objectID": "04.3_energy-consumption.html",
    "href": "04.3_energy-consumption.html",
    "title": "Energy consumption",
    "section": "",
    "text": "Analysis\nTable 11.1 shows the estimated energy consumption (% and mA) for each configuration and device (i.e., C1 and C2). The average consumption per TUG execution using the system with the C1 configuration is approximately \\(0.01\\%\\) and \\(0.005\\%\\) of the total battery of the smartwatch and the smartphone respectively, which is around \\(0.058mA\\) and \\(0.254mA\\), yielding a combined consumption of \\(0.312mA\\). In C2 configuration, the system consumes \\(0.006\\%\\) of the smartphone’s battery, which equals \\(0.307mA\\). While both configurations report a similar consumption, C1 is limited by the consumption of the smartwatch device with respect to C2 (i.e., the smartwatch’s battery would run out before the smartphone’s).\nCode\nmean_df = mean_consumption_per_device(battery_df)\nmean_df.round(3)\n\n\n\n\nTable 11.1: System’s energy consumption per TUG execution on its two configurations.\n\n\n\n\n\n\n\n\n\n\n\nconsumption (%)\nconsumption (mA)\n\n\nconfiguration\ndevice\n\n\n\n\n\n\nC1\nsw\n0.010\n0.058\n\n\nsp-paired\n0.005\n0.254\n\n\nC2\nsp\n0.006\n0.307",
    "crumbs": [
      "HAR in mHealth: TUG test",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Energy consumption</span>"
    ]
  },
  {
    "objectID": "04.3_energy-consumption.html#summary",
    "href": "04.3_energy-consumption.html#summary",
    "title": "Energy consumption",
    "section": "Summary",
    "text": "Summary\nThese reported consumptions would allow to hypothetically run (without taking into account the consumption of other services running in the devices) thousands of TUG executions with a single battery load: \\(+10,000\\) in C1 and \\(+16,000\\) in C2. Therefore, we consider that the consumption of the developed system is low, and in both cases the need for performing sufficient TUG tests on a single battery charge is comfortably covered.\n\n\n\n\n\n\nTip\n\n\n\nThe documentation of the Python functions employed in this section can be found in Chapter 4 reference:\n\ndata_loading:\n\nload_battery_results\n\nbattery:\n\nmean_consumption_per_device",
    "crumbs": [
      "HAR in mHealth: TUG test",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Energy consumption</span>"
    ]
  },
  {
    "objectID": "05.1_localized-har.html",
    "href": "05.1_localized-har.html",
    "title": "Localized HAR based on Wi-Fi CSI",
    "section": "",
    "text": "Results\nCode\nimport os\nimport numpy as np\n\nfrom libs.chapter5.analysis.reports import extract_metrics, metrics_summary, metric_increment_summary\nfrom libs.chapter5.analysis.visualization import plot_confusion_matrix\nfrom libs.common.utils import load_json\n\nREPORTS_DIR = os.path.join('data', 'chapter5', 'model-reports', 'preliminar-dataset', 'proposed-method', '{}_report.json')\nPREDICITON_TARGET = 'Activity'\nLABELS = ['SEATED_RX','STANDING_UP_RX','WALKING_TX','TURNING_TX','SITTING_DOWN_TX', 'SEATED_TX', 'STANDING_UP_TX','WALKING_RX','TURNING_RX','SITTING_DOWN_RX']",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Localized HAR based on Wi-Fi CSI</span>"
    ]
  },
  {
    "objectID": "05.1_localized-har.html#results",
    "href": "05.1_localized-har.html#results",
    "title": "Localized HAR based on Wi-Fi CSI",
    "section": "",
    "text": "10-fold cross-validation\nFigure 12.1 shows the confusion matrix of the \\(10\\)-fold cross-validation approach. The classification accuracy reaches \\(100\\%\\) in the SEATED_RX and SEATED_TX, and activities such as WALKING_TX, TURNING_TX reach accuracies over \\(90\\%\\). Misclassifications can be observed between the sitting down and standing up activities, but a clear diagonal (i.e., perfect prediction) can be seen.\n\n\nCode\ncv_reports = load_json(REPORTS_DIR.format('cv'))\nmean_accuracy, _, _, _ = extract_metrics(cv_reports)\n\naggregated_cf = np.zeros((10, 10))\nfor report in cv_reports:\n    aggregated_cf += report['confusion_matrix']\n\nplot_confusion_matrix({ 'accuracy': mean_accuracy, 'confusion_matrix': aggregated_cf }, PREDICITON_TARGET, LABELS)\n\n\n\n\n                                                \n\n\nFigure 12.1: Confusion matrix of cross-validation approach.\n\n\n\n\n\n\nKeeping temporal dependency\nFigure 12.2 (D1T/D1E) shows similar results as the previous approach: perfect accuracy on SEATED_RX and over \\(90\\%\\) on WALKING_TX and SEATED_TX. In addition, the main diagonal can be perfectly observed.\n\n\nCode\nd1_report = load_json(REPORTS_DIR.format('d1'))\n\nplot_confusion_matrix(d1_report, PREDICITON_TARGET, LABELS)\n\n\n\n\n                                                \n\n\nFigure 12.2: Confusion matrix of D1T/D1E approach.\n\n\n\n\n\n\nThe effect of time\n\nEvaluation with D2 (10 minutes after D1)\nIn the D1T/D2 evaluation (Figure 12.3), while the diagonal can still be observed, activities such as SEATED_RX are mostly misclassified as WALKING_TX. However, the SEATED_TX or TURNING_TX are perfectly classified.\n\n\nCode\nd2_report = load_json(REPORTS_DIR.format('d2'))\n\nplot_confusion_matrix(d2_report, PREDICITON_TARGET, LABELS)\n\n\n\n\n                                                \n\n\nFigure 12.3: Confusion matrix of D1T/D2 approach.\n\n\n\n\n\n\nEvaluation with D3 (30 minutes after D1)\nAfter \\(30\\) minutes (Figure 12.4), the diagonal starts to disappear in the activities going towards the Rx, which are misclassified as activities closer to the Tx, such as WALKING_TX and TURNING_TX.\n\n\nCode\nd3_report = load_json(REPORTS_DIR.format('d3'))\n\nplot_confusion_matrix(d3_report, PREDICITON_TARGET, LABELS)\n\n\n\n\n                                                \n\n\nFigure 12.4: Confusion matrix of D1T/D3 approach.\n\n\n\n\n\n\nEvaluation with D4 (90 minutes after D1)\nUn the D1T/D4 (Figure 12.5), activities going towards the Rx are completely misclassified, mainly as WALKING_TX, TURNING_TX and STANDING_UP_RX.\n\n\nCode\nd4_report = load_json(REPORTS_DIR.format('d4'))\n\nplot_confusion_matrix(d4_report, PREDICITON_TARGET, LABELS)\n\n\n\n\n                                                \n\n\nFigure 12.5: Confusion matrix of D1T/D4 approach.",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Localized HAR based on Wi-Fi CSI</span>"
    ]
  },
  {
    "objectID": "05.1_localized-har.html#summary",
    "href": "05.1_localized-har.html#summary",
    "title": "Localized HAR based on Wi-Fi CSI",
    "section": "Summary",
    "text": "Summary\nTable 12.1 contains the accuracy, precision, recall and F1-score metrics obtained in each evaluation approach and Table 12.2 the relative decrement in each metric with regards to the first evaluation approach. The \\(10\\)-fold cross-validation achieves the best metrics, with averages around \\(86-87\\%\\). Then, in the second approach, where the temporal dependency of the data is maintained in D1, the performance metrics slightly decrease around \\(84-85\\%\\), showing a relative drop of \\(\\approx3\\%\\) in accuracy and recall, and \\(\\approx2\\%\\) in precision and F1-score.\n\n\nCode\nmetrics_summary([cv_reports, d1_report, d2_report, d3_report, d4_report], ['CV', 'D1T/D1E', 'D1T/D2', 'D1T/D3', 'D1T/D4'])\n\n\n\n\nTable 12.1: Summary of obtained metrics in the evaluation approaches.\n\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nF1-score\n\n\n\n\nCV\n0.869255\n0.870630\n0.869255\n0.857086\n\n\nD1T/D1E\n0.841584\n0.853984\n0.841584\n0.842004\n\n\nD1T/D2\n0.519231\n0.617909\n0.519231\n0.490104\n\n\nD1T/D3\n0.382353\n0.462571\n0.382353\n0.313558\n\n\nD1T/D4\n0.265306\n0.320546\n0.265306\n0.202500\n\n\n\n\n\n\n\n\n\n\nThe evaluation results of the HAR model with D2, D3 and D4 show a drastic drop in the reported metrics. For instance, the accuracy drops to \\(\\approx52\\%\\) in D2, \\(\\approx38\\%\\) in D3 and \\(\\approx26\\%\\) in D4. These results constitute relative accuracy drops of \\(40.26\\%\\), \\(56.01\\%\\) and \\(69.47\\%\\) with data gathered just \\(10\\), \\(30\\) and \\(90\\) minutes after the training data was collected. Similar drops can be observed in the remaining metrics.\n\n\nCode\ncomparisons = {\n    'CV vs. D1T/D1E': [cv_reports, d1_report],\n    'CV vs. D1T/D2': [cv_reports, d2_report],\n    'CV vs. D1T/D3': [cv_reports, d3_report],\n    'CV vs. D1T/D4': [cv_reports, d4_report],\n}\nmetric_increment_summary(comparisons)\n\n\n\n\nTable 12.2: Decrement (%) of metrics in the evaluation approaches.\n\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nF1-score\n\n\n\n\nCV vs. D1T/D1E\n-3.183271\n-1.911931\n-3.183271\n-1.759685\n\n\nCV vs. D1T/D2\n-40.267145\n-29.027340\n-40.267145\n-42.817424\n\n\nCV vs. D1T/D3\n-56.013715\n-46.869374\n-56.013715\n-63.415865\n\n\nCV vs. D1T/D4\n-69.478904\n-63.182340\n-69.478904\n-76.373408\n\n\n\n\n\n\n\n\n\n\nThese results show a clear degradation in the classification accuracy of the employed CNN model when the evaluation took into account data collected spaced in time regarding the training data. That is, classification accuracy quickly degrades over time. Notwithstanding, temporal instability of data is only one possible explanation for the poor obtained results. Concretely, the following factors could affect the results:\n\nThe selected methods might not be able to properly work with CSI data, i.e., generalize from the training data. While CNN approaches have proven to provide good results working with CSI data, most related works using the ESP32 microcontroller employ other architectures, such as the MLP.\nThe employed hardware for CSI extraction, ESP32-S2 microcontroller, might not be appropriate for such a task. Other devices, such as the Intel 5300 or Atheros NICs might be a better option.\nThe collected dataset might have been affected by some external interference, altering the environment and changing the CSI data.\nThe CSI data is not stable over time and therefore can not be used for real-life applications.\n\nNext, points (1) and (4) are explored in Validation of employed methods and Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers. The remaining factors can not be explored due to resource limitations (2) and the impossibility of determining the existence of external interferences while collecting the dataset (3).",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Localized HAR based on Wi-Fi CSI</span>"
    ]
  },
  {
    "objectID": "05.1_localized-har.html#code-reference",
    "href": "05.1_localized-har.html#code-reference",
    "title": "Localized HAR based on Wi-Fi CSI",
    "section": "Code reference",
    "text": "Code reference\n\n\n\n\n\n\nTip\n\n\n\nThe documentation of the Python functions employed in this section can be found in Chapter 5 reference:\n\nreports:\n\nextract_metrics\nmetrics_summary\nmetric_increment_summary\n\nvisualization:\n\nplot_confusion_matrix",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Localized HAR based on Wi-Fi CSI</span>"
    ]
  },
  {
    "objectID": "05.2_methods-validation.html",
    "href": "05.2_methods-validation.html",
    "title": "Validation of employed methods",
    "section": "",
    "text": "Results\nCode\nimport os\n\nfrom libs.chapter5.analysis.reports import metrics_summary, metric_increment_summary\nfrom libs.common.utils import load_json\n\nREPORTS_DIR = os.path.join('data', 'chapter5', 'model-reports')\nREPORTS_FILE = '{}_report.json'\nCHOI_REPORTS_PATH = os.path.join(REPORTS_DIR, 'preliminar-dataset', 'choi-method')",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Validation of employed methods</span>"
    ]
  },
  {
    "objectID": "05.2_methods-validation.html#results",
    "href": "05.2_methods-validation.html#results",
    "title": "Validation of employed methods",
    "section": "",
    "text": "Validation on public datasets\nTable 13.1 shows the average accuracy, precision, recall, and F1-score of the cross-validation procedure obtained by the selected CNN model in the two described public datasets.\nThe results of the model in the StanWiFi dataset are around \\(96\\%\\) in all metrics. These results are better than the ones presented by the creators of the database (Yousefi et al. 2017), but slightly worse than other proposed solutions in the literature.\nRegarding the Multienvironment datasets (E1 and E2), the model obtains significantly worse results than other works in the literature. These results can be explained due to the higher complexity of the dataset compared with StanWiFi and the collected dataset. In addition, the employed model is an adaptation from the previous section and it is not optimized for this dataset, whereas other works completely focus on this dataset and use more complex methods such as specific feature extraction, adaptative windows, or windowing approaches with an excessive overlapping.\n\n\nCode\nstanwifi_reports = load_json(os.path.join(REPORTS_DIR, 'stanwifi', REPORTS_FILE.format('cv')))\ne1_reports = load_json(os.path.join(REPORTS_DIR, 'multienvironment', REPORTS_FILE.format('e1-cv')))\ne2_reports = load_json(os.path.join(REPORTS_DIR, 'multienvironment', REPORTS_FILE.format('e2-cv')))\nmetrics_summary([stanwifi_reports, e1_reports, e2_reports], ['StanWiFi CV', 'Multienvironment E1', 'Multienvironment E2'])\n\n\n\n\nTable 13.1: Results of applying the proposed method on the StanWiFi and Multienvironment E1 and E2 datasets.\n\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nF1-score\n\n\n\n\nStanWiFi CV\n0.962032\n0.963551\n0.962032\n0.961797\n\n\nMultienvironment E1\n0.830443\n0.871437\n0.830443\n0.832676\n\n\nMultienvironment E2\n0.789978\n0.821395\n0.789978\n0.783325\n\n\n\n\n\n\n\n\n\n\nOverall, these results show that while the employed method does not improve the existing results in the literature, neither does it completely fail in its classification purpose. In addition, the model could yield more satisfactory results after a proper optimization for both datasets.\n\n\nValidation of other method\nTable 13.2 contains the accuracy, precision, recall and F1-score metrics obtained in each evaluation approach and Table 13.3 include the relative decrement in each metric regarding the first evaluation approach.\n\n\nCode\nchoi_cv_report = load_json(os.path.join(CHOI_REPORTS_PATH, REPORTS_FILE.format('cv')))\nchoi_d1_report = load_json(os.path.join(CHOI_REPORTS_PATH, REPORTS_FILE.format('d1')))\nchoi_d2_report = load_json(os.path.join(CHOI_REPORTS_PATH, REPORTS_FILE.format('d2')))\nchoi_d3_report = load_json(os.path.join(CHOI_REPORTS_PATH, REPORTS_FILE.format('d3')))\nchoi_d4_report = load_json(os.path.join(CHOI_REPORTS_PATH, REPORTS_FILE.format('d4')))\n\nmetrics_summary([choi_cv_report, choi_d1_report, choi_d2_report, choi_d3_report, choi_d4_report], ['CV', 'D1T/D1E', 'D1T/D2', 'D1T/D3', 'D1T/D4'])\n\n\n\n\nTable 13.2: Summary of obtained metrics in the evaluation approaches using Choi’s method.\n\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nF1-score\n\n\n\n\nCV\n0.898863\n0.914035\n0.898863\n0.890320\n\n\nD1T/D1E\n0.841584\n0.875248\n0.841584\n0.845221\n\n\nD1T/D2\n0.259615\n0.305082\n0.259615\n0.253803\n\n\nD1T/D3\n0.284314\n0.254167\n0.284314\n0.234527\n\n\nD1T/D4\n0.112245\n0.112294\n0.112245\n0.082696\n\n\n\n\n\n\n\n\n\n\nIn the \\(10\\)-fold cross-validation approach, Choi’s method achieves better results than the one employed in the previous section, around three percentual points in all metrics.\nRegarding the D1T/D1E evaluation, both methods obtain similar outcomes around \\(\\approx 84\\%\\) in all metric except in Choi’s method precision, which is two percentual points better than the other method.\nHowever, Choi’s method fails when taking into account the effect of time. In the D1T/D2, D1T/D3, D1T/D4 evaluations, the obtained results are much worse than the ones presented previously, with accuracy drops of \\(6.37\\%\\), \\(71.11\\%\\), \\(68.36\\%\\) and \\(87.51\\%\\) (\\(3.18\\%\\), \\(40.26\\%\\), \\(56.01\\%\\) and \\(69.47\\%\\) in our method).\n\n\nCode\ncomparisons = {\n    'CV vs. D1T/D1E': [choi_cv_report, choi_d1_report],\n    'CV vs. D1T/D2': [choi_cv_report, choi_d2_report],\n    'CV vs. D1T/D3': [choi_cv_report, choi_d3_report],\n    'CV vs. D1T/D4': [choi_cv_report, choi_d4_report],\n}\ndisplay(metric_increment_summary(comparisons))\n\n\n\n\nTable 13.3: Decrement (%) of metrics in the evaluation approaches using Choi’s method.\n\n\n\n\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nF1-score\n\n\n\n\nCV vs. D1T/D1E\n-6.372340\n-4.243565\n-6.372340\n-5.065488\n\n\nCV vs. D1T/D2\n-71.117350\n-66.622465\n-71.117350\n-71.493068\n\n\nCV vs. D1T/D3\n-68.369617\n-72.192902\n-68.369617\n-73.658177\n\n\nCV vs. D1T/D4\n-87.512565\n-87.714499\n-87.512565\n-90.711641",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Validation of employed methods</span>"
    ]
  },
  {
    "objectID": "05.2_methods-validation.html#summary",
    "href": "05.2_methods-validation.html#summary",
    "title": "Validation of employed methods",
    "section": "Summary",
    "text": "Summary\nBased on the results obtained in the previous sections, we can determine that the employed methods and model – factor (1) – are not the cause of the bad results obtained in Localized HAR based on Wi-Fi CSI since 1) the methods and model obtained acceptable results in other public datasets and 2) a validated method in the literature also obtained very poor results with the collected datasets.",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Validation of employed methods</span>"
    ]
  },
  {
    "objectID": "05.2_methods-validation.html#code-reference",
    "href": "05.2_methods-validation.html#code-reference",
    "title": "Validation of employed methods",
    "section": "Code reference",
    "text": "Code reference\n\n\n\n\n\n\nTip\n\n\n\nThe documentation of the Python functions employed in this section can be found in Chapter 5 reference:\n\nreports:\n\nmetrics_summary\nmetric_increment_summary\n\n\n\n\n\n\n\n\nYousefi, Siamak, Hirokazu Narui, Sankalp Dayal, Stefano Ermon, and Shahrokh Valaee. 2017. “A Survey on Behavior Recognition Using WiFi Channel State Information.” IEEE Communications Magazine 55 (10): 98–104. https://doi.org/10.1109/MCOM.2017.1700082.",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Validation of employed methods</span>"
    ]
  },
  {
    "objectID": "05.3_csi-stability.html",
    "href": "05.3_csi-stability.html",
    "title": "Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers",
    "section": "",
    "text": "Results\nCode\nimport os \nimport numpy as np\n\nfrom libs.chapter5.analysis.visualization import plot_confusion_matrix\nfrom libs.common.utils import load_json\n\nREPORT_PATH = os.path.join('data', 'chapter5', 'model-reports', 'lodo-dataset', 'reports.json')\nPREDICTION_TARGET = 'Activity'\nLABELS = ['03/28', '03/29', '03/30', '03/31', '04/01']\n\nlodo_reports = load_json(REPORT_PATH)\nFigure 14.1 shows the confusion matrix obtained from the LODO procedure. The matrix shows different patterns for the samples collected on different days:\nCode\ndef plot_combined_confusion_matrix(reports):\n    combined_cf = np.zeros((5, 5))\n    for report in reports:\n        report_cf = np.array(report['confusion_matrix'])\n        combined_cf += report_cf\n\n    return plot_confusion_matrix({'accuracy': 0, 'confusion_matrix': combined_cf}, 'Day', LABELS)\n\n\nplot_combined_confusion_matrix(lodo_reports)\n\n\n\n\n                                                \n\n\nFigure 14.1: Confusion matrix of the LODO procedure.",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers</span>"
    ]
  },
  {
    "objectID": "05.3_csi-stability.html#results",
    "href": "05.3_csi-stability.html#results",
    "title": "Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers",
    "section": "",
    "text": "\\(03/28\\): its samples were mainly classified into the next day, although a significant amount of them (\\(31.7\\%\\)) were classified into \\(04/01\\).\n\\(03/29\\): more than \\(90\\%\\) of its samples were classified into the next two days. Few samples were classified into the previous day.\n\\(03/30\\): the \\(33.8\\%\\) and \\(43.8\\%\\) of its samples were classified into its previous and next day. The remaining samples were classified into \\(03/28\\) (\\(12.8\\%\\)) and \\(04/01\\) (\\(9.6\\%\\)).\n\\(03/31\\): more than the \\(75\\%\\) of its samples were classified into the previous day and a \\(14.9\\%\\) into \\(03/29\\). Few samples were classified into the next day.\n\\(04/01\\): as in \\(03/31\\), around \\(75\\%\\) and \\(16\\%\\) of the samples were classified into the two previous days. Few samples were classified into the remaining days.",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers</span>"
    ]
  },
  {
    "objectID": "05.3_csi-stability.html#summary",
    "href": "05.3_csi-stability.html#summary",
    "title": "Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers",
    "section": "Summary",
    "text": "Summary\nWhile a clear pattern cannot be seen across all days, the samples of each day tend to be classified into the adjacent days. The main exception is the day \\(03/28\\), whose \\(30\\%\\) of samples are classified into the most distant day. Notwithstanding, the results indicate that the CSI data from one day is more similar to the adjacent days than to other days. Consequently, these results would indicate that the collected CSI data is not stable over time, confirming factor (4).",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers</span>"
    ]
  },
  {
    "objectID": "05.3_csi-stability.html#code-reference",
    "href": "05.3_csi-stability.html#code-reference",
    "title": "Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers",
    "section": "Code reference",
    "text": "Code reference\n\n\n\n\n\n\nTip\n\n\n\nThe documentation of the Python functions employed in this section can be found in Chapter 5 reference:\n\nplot_confusion_matrix",
    "crumbs": [
      "Wi-Fi CSI based HAR",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Human Activity Recognition with Consumer Devices and Real-Life Perspectives",
    "section": "",
    "text": "Funding",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#funding",
    "href": "index.html#funding",
    "title": "Human Activity Recognition with Consumer Devices and Real-Life Perspectives",
    "section": "",
    "text": "This thesis is funded by the Spanish Ministry of Universities with a predoctoral grant (FPU19/05352) and a research stay grant (EST23/00320).\n\n\n\n\n\n\n\n\nFinancial support for derived activities of this dissertation was received from the SyMptOMS-ET project (PID2020-120250RB-I00), funded by MICIU/AEI/10.13039/501100011033.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Human Activity Recognition with Consumer Devices and Real-Life Perspectives",
    "section": "License",
    "text": "License\n\n\nHuman Activity Recognition with Consumer Devices and Real-Life Perspectives Copyright © 2024 Miguel Matey Sanz. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Motivation\nHAR is a research area aiming to determine human behaviour, actions, and activities from physical signals extracted from sensing devices (Gupta et al. 2022). Historically, HAR has had two main approaches regarding sensing devices: vision- and inertial-based sensors (Minh Dang et al. 2020). More recently, a breakthrough approach was proposed: monitoring the state of the channels of Wi-Fi networks (i.e., Wi-Fi CSI), which can be affected by the presence and movement of individuals, among other factors (Ma, Zhou, and Wang 2019). In this thesis, we focus on inertial- and Wi-Fi CSI-based HAR.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#motivation",
    "href": "01_intro.html#motivation",
    "title": "Introduction",
    "section": "",
    "text": "Inertial-based HAR\nThe mainstream availability of smartphones and smartwatches equipped with IMUs offers an inexpensive, ubiquitous, and reliable way to develop HAR systems that can be employed in real-life scenarios. Due to the potential of these systems, some challenges and opportunities were identified:\n\nModern Android smartphones and Wear OS smartwatches suffer from energy restrictions that can affect the data collection. The guidelines proposed by González-Pérez et al. (2022) overcome these restrictions, showing only a \\(1\\%\\) of missing data in their experiments. Therefore, providing data collection libraries implementing these guidelines might be useful for the HAR and other communities that need reliable data collection tools.\nOnly five public datasets with smartphone and smartwatch data for HAR exist, where four of them used the devices in their common placement (carried in a pocket and the wrist, respectively). From those, RealWorld (Sztyler and Stuckenschmidt 2016), ExtraSensory (Vaizman, Ellis, and Lanckriet 2017) and DOMINO (Arrotta et al. 2023) include data from diverse people in terms of age and gender, traits that might help build generalizable HAR systems. Therefore, the HAR research community would benefit from more public and comprehensive datasets that adequately satisfy these aspects.\nThere are no works exploring how the accuracy of ML and DL evolves regarding the amount of training data in smartphone- and smartwatch-based HAR. In addition, existing research presents contradictory results in terms of data sources and model comparisons. Therefore, analysing how ML and DL models evolve regarding the quantity of training data while also comparing which data source and model architecture perform best could provide valuable insights for the research community.\nMost of the existing real-life smartphone- and smartwatch-HAR-based systems focus on mHealth applications. However, the evaluation of those systems is still lagging behind in validating them for real-life (with real users) usage. Therefore, developing a mHealth system to solve a real-life problem with its corresponding validation following Goldsack et al. (2020) V3 framework – as far as possible – would be a great research output.\n\n\n\nWi-FI CSI-based HAR\nRegarding the new trend in Wi-Fi CSI-based HAR, we observed that few works employed ESP32 microcontrollers and that one of them pointed out a possible instability over time in the collected CSI data that could undermine the performance of HAR systems. Therefore, more research on this topic would be useful to shed light on the mentioned issue and determine its viability in real-life scenarios.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#research-objectives",
    "href": "01_intro.html#research-objectives",
    "title": "Introduction",
    "section": "Research objectives",
    "text": "Research objectives\nFrom the identified challenges and opportunities, the following research objectives were defined and will be addressed in successive chapters:\n\nRO1: Provide software tools for reliable data collection in Android smartphones and Wear OS smartwatches. Create software libraries enabling the development of smartphone and smartwatch applications for data collection. Make these libraries available as open-source software solutions to the research community, serving as enablers for future HAR research.\nRO2: Collect HAR dataset of smartphone and smartwatch data from heterogeneous subjects. Generate a public dataset composed of IMU data from a smartphone and smartwatch carefully selecting participants to preserve age diversity and gender balance.\nRO3: Analyse and compare the effect of the amount of training data, the data source and the model architecture on the performance of ML- and DL-based HAR. Determine how the accuracy of several model architectures evolves regarding the amount of data used for training them and the nature of the data employed. Provide insights that might guide researchers when choosing the amount of data they need to collect, the most suitable data source or the most appropriate model for their HAR systems.\nRO4: Develop and evaluate an mHealth system using HAR with smartphones and smartwatches. Create an application for off-the-shelve smartphones and smartwatches capable of automating the computation of results of a well-known mobility test and validating it for real-life usage.\nRO5: Analyse the feasibility of Wi-Fi CSI data for real-life HAR systems. Design experiments to evaluate the use of CSI data collected with ESP32 microcontrollers taking into account the stability of the data over time.\n\nAs depicted in Figure 2.1, these research objectives constitute the path of the thesis, which focuses on inertial-based HAR with the development of data collection tools, the collection of a dataset, a study of the factors affecting the performance of ML and DL models, and the development and evaluation of an mHealth application focusing on inertial-based HAR, to finish with a feasibility study of the usage of the Wi-Fi CSI data for HAR.\n\n\n\n\n\n\nFigure 2.1: Thesis’ research path.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#thesis-structure",
    "href": "01_intro.html#thesis-structure",
    "title": "Introduction",
    "section": "Thesis structure",
    "text": "Thesis structure\nThe rest of the dissertation is structured as follows:\n\n\n\n\n\n\n\n\n\n\nMaterials & Methods\n\n\nDescription of the materials and methods employed during the development of the thesis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultidimensional Analysis of ML and DL on HAR\n\n\nAnalysis on how the amount of training data affects the classification performance on several models and data sources.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHAR in mHealth: TUG Test Using Smartphones and Smartwatches\n\n\nImplementation and evaluation of an mHealth system for instrumenting a mobility test.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking into the Future: Wi-Fi CSI based HAR\n\n\nAnalyses the feasibility of Wi-Fi CSI-based HAR systems in real-world applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions\n\n\nSummarises the thesis, discussing the accomplishment of the research objectives, the contributions and the limitations of this dissertation.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nArrotta, Luca, Gabriele Civitarese, Riccardo Presotto, and Claudio Bettini. 2023. “DOMINO: A Dataset for Context-Aware Human Activity Recognition Using Mobile Devices.” In 2023 24th IEEE International Conference on Mobile Data Management (MDM), 346–51. IEEE. https://doi.org/10.1109/MDM58254.2023.00063.\n\n\nGoldsack, Jennifer C, Andrea Coravos, Jessie P Bakker, Brinnae Bent, Ariel V Dowling, Cheryl Fitzer-Attas, Alan Godfrey, et al. 2020. “Verification, Analytical Validation, and Clinical Validation (V3): The Foundation of Determining Fit-for-Purpose for Biometric Monitoring Technologies (BioMeTs).” Npj Digital Medicine 3 (1): 55. https://doi.org/10.1038/S41746-020-0260-4.\n\n\nGonzález-Pérez, Alberto, Miguel Matey-Sanz, Carlos Granell, and Sven Casteleyn. 2022. “Using Mobile Devices as Scientific Measurement Instruments: Reliable Android Task Scheduling.” Pervasive and Mobile Computing 81: 101550. https://doi.org/10.1016/j.pmcj.2022.101550.\n\n\nGupta, Neha, Suneet K Gupta, Rajesh K Pathak, Vanita Jain, Parisa Rashidi, and Jasjit S Suri. 2022. “Human Activity Recognition in Artificial Intelligence Framework: A Narrative Review.” Artificial Intelligence Review 55 (6): 4755–4808. https://doi.org/10.1007/s10462-021-10116-x.\n\n\nMa, Yongsen, Gang Zhou, and Shuangquan Wang. 2019. “WiFi Sensing with Channel State Information: A Survey.” ACM Comput. Surv. 52 (3): 1–36. https://doi.org/10.1145/3310194.\n\n\nMinh Dang, L., Kyungbok Min, Hanxiang Wang, Md. Jalil Piran, Cheol Hee Lee, and Hyeonjoon Moon. 2020. “Sensor-Based and Vision-Based Human Activity Recognition: A Comprehensive Survey.” Pattern Recognition 108: 107561. https://doi.org/https://doi.org/10.1016/j.patcog.2020.107561.\n\n\nSztyler, Timo, and Heiner Stuckenschmidt. 2016. “On-Body Localization of Wearable Devices: An Investigation of Position-Aware Activity Recognition.” In 2016 IEEE International Conference on Pervasive Computing and Communications (PerCom), 1–9. https://doi.org/10.1109/PERCOM.2016.7456521.\n\n\nVaizman, Yonatan, Katherine Ellis, and Gert Lanckriet. 2017. “Recognizing Detailed Human Context in the Wild from Smartphones and Smartwatches.” IEEE Pervasive Computing 16 (4): 62–74. https://doi.org/10.1109/MPRV.2017.3971131.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02.1_collection-tools.html",
    "href": "02.1_collection-tools.html",
    "title": "Data collection libraries",
    "section": "",
    "text": "Background Sensors\nThe Background Sensors library is an Android library developed in Java that implements a reliable passive collection of IMU sensors (i.e., accelerometer, gyroscope and magnetometer), and, therefore, it can be used to develop native Android applications that require to collect samples from such sensors. The library has been developed to support devices running from Andriod 5.0 (API level 21) to Android 13 (API level 33).\nFigure 3.1 includes the simplified architecture of the Background Sensors library.",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data collection libraries</span>"
    ]
  },
  {
    "objectID": "02.1_collection-tools.html#background-sensors",
    "href": "02.1_collection-tools.html#background-sensors",
    "title": "Data collection libraries",
    "section": "",
    "text": "Availability\n\n\n\nThe version of the library at the moment of writing this thesis is v1.3.0. The library is available in:\n\nMaven Central Repository\nGitHub\nZenodo (Matey-Sanz, Casteleyn, and Granell 2024a)\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: Conceptual architecture of the Background Sensors library.\n\n\n\n\n\n\n\n\n\nLibrary documentation\n\n\n\nThe full documentation of the library and its components can be found in GitHub.\n\n\n\nSample usage\n\n\n\n\n\n\nTip\n\n\n\nTap on the numbers at the end of the lines to obtain insights about the code.\n\n\npublic class ExampleActivity extends Activity {\n\n    private SensorManager sensorManager;\n    private ServiceManager serviceManager;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n\n1        sensorManager = new SensorManager(this);\n        List&lt;Sensor&gt; sensors = sensorManager.availableSensors(BaseSensor.values());\n\n2        serviceManager = new ServiceManager(this, BaseSensorRecordingService.class);\n    }\n\n    public void start() {\n        Sensor sensor = BaseSensor.ACCELEROMETER;\n\n3        CollectionConfiguration config = new CollectionConfiguration(\n            sensor,\n            10000, // sensorDelay (microseconds)\n            100    // batchSize\n        );\n\n4        RecordCallback callback = new RecordCallback {\n            public void onRecordsCollected(List&lt;Record&gt; records) {\n                // Accelerometer samples received here\n            }\n        };\n\n5        serviceManager.startCollection(config, callback);\n    }\n\n    public void stop() {\n6        serviceManager.stopCollection(BaseSensor.ACCELEROMETER);\n    }\n}\n\n1\n\nCreate an instance of SensorManager and call availableSensors(...) to determine which sensors are available in the device.\n\n2\n\nCreate a ServiceManager instance with a SensorRecordingService.\n\n3\n\nCreate a CollectionConfiguration indicating the sensor, the sensorDelay(microseconds) and the batchSize.\n\n4\n\nImplement the RecordCallback. The samples will be received here.\n\n5\n\nStart the data collection calling ServiceManager#startCollection(...).\n\n6\n\nStop the data collection calling ServiceManager#stopCollection(...).",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data collection libraries</span>"
    ]
  },
  {
    "objectID": "02.1_collection-tools.html#wearos-sensors-nativescript-wearos-sensors",
    "href": "02.1_collection-tools.html#wearos-sensors-nativescript-wearos-sensors",
    "title": "Data collection libraries",
    "section": "WearOS Sensors & NativeScript WearOS Sensors",
    "text": "WearOS Sensors & NativeScript WearOS Sensors\nThe WearOS Sensors library is a Wear OS library written in Java that can be used to develop native Wear OS applications and, like Background Sensors, implements a reliable passive collection of IMU sensors (i.e., accelerometer, gyroscope and magnetometer). In addition, it supports collecting data from the heart rate sensor and the location services (i.e., GNSS).\nOn the other hand, NativeScript WearOS Sensors is a library written in TypeScript that can be used to build smartphone applications complementary to WearOS Sensors developed applications. Unlike the other libraries, this one cannot be used to develop native applications but applications built with the NativeScript development framework, which uses web technologies (e.g., JavaScript, TypeScript) to build Android and iOS applications.\nThe following features are available when using applications developed with these libraries:\n\nWearOS Sensors:\n\nStart and stop from the smartwatch the data collection of the available sensors in the device.\nStore collected data in the smartwatch.\n\nWearOS Sensors + Nativescript WearOS Sensors:\n\nStart and stop from the smartwatch the data collection of the available sensors in the device.\nStart and stop from the paired smartphone the data collection of the available sensors in the smartwatch.\nStore the collected data in the smartwatch or in the smartphone.\n\n\nThe WearOS Sensors and NativeScript WearOS Senosrs libraries have been developed to support devices running from Wear OS 1.0 ( level 23) to Wear OS 4 ( level 33) and from Android 6 ( level 23) to Android 13 ( level 33), respectively.\n\n\n\n\n\n\nAvailability\n\n\n\nThe version of WearOS Sensors and NativeScript WearOS Sensors at the moment of writing this thesis are v1.2.1 and v1.3.0. The libraries are available in:\n\nWearOS Sensors:\n\nMaven Central Repository\nGitHub\nZenodo (Matey-Sanz, Casteleyn, and Granell 2024c)\n\nNativeScript WearOS Sensors:\n\nNode Package Manager\nGitHub\nZenodo (Matey-Sanz, Casteleyn, and Granell 2024b)\n\n\n\n\nFigure 3.2 includes the simplified architecture of the WearOS Sensors and NativeScript WearOS Sensors libraries.\n\n\n\n\n\n\nFigure 3.2: Conceptual architecture of the Wear OS Sensors and NativeScript WearOS Sensors libraries.\n\n\n\n\n\n\n\n\n\nLibrary documentation\n\n\n\nThe full documentation of the libraries and their components can be found in their respective repositories: WearOS Sensors and NativeScript WearOS Sensors.\n\n\n\nSample usage\n\nManaging collection process and storing data in smartwatch.\nSince WearOS Sensors is built on top of Background Sensors the same code as the one presented previously can be employed for this purpose.\n\n\nManaging collection process in smartwatch, storing data in smartphone.\npublic class ExampleActivity extends Activity {\n    private CommandClient commandClient;\n\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n\n1        commandClient = new CommandClient(this);\n    }\n\n    public void start() {\n        Sensor sensor = WearSensor.ACCELEROMETER;\n\n2        CollectionConfiguration config = new CollectionConfiguration(\n            sensor,\n            10000, // sensorDelay (microseconds)\n            100 // batchSize\n        );\n\n3        commandClient.sendStartCommand(config);\n    }\n\n    public void stop() {\n4        commandClient.sendStopCommand(WearSensor.ACCELEROMETER);\n    }\n}\n\n1\n\nCreate a CommandClient instance.\n\n2\n\nCreate a CollectionConfiguration including the sensor, sensorDelay (microseconds) and batchSize.\n\n3\n\nStart the data collection using the CommandClient#sendStartCommand(...).\n\n4\n\nStop the data collection using the CommandClient#sendStopCommand(...).\n\n\nfunction registerListener() {\n5    const collectorManager = getCollectorManager();\n\n6    collectorManager.addSensorListener((sensorRecord: SensorRecord&lt;any&gt;) =&gt; {\n        // Accelerometer samples received here.\n    });\n}\n\n5\n\nObtain a CollectorManager instance using the getCollectorManager() function.\n\n6\n\nRegister a listener with the CollectorManager#addSensorListener(...).\n\n\n\n\nManaging collection process and storing data in smartphone.\nlet collectorManager;\nfunction registerListener() {\n1    const collectorManager = getCollectorManager();\n\n2    collectorManager.addSensorListener((sensorRecord: SensorRecord&lt;any&gt;) =&gt; {\n        // Accelerometer samples received here.\n    });\n}\n\nlet smartwatch;\nasync function getConnectedSmartwatch() {\n3    const nodesDiscovered = await getNodeDiscoverer().getConnectedNodes();\n    smartwatch = nodesDiscovered[0];\n}\n\nfunction start() {\n4    const config: CollectionConfiguration = {sensorInterval: 10000, batchSize: 100};\n    \n5    collectorManager.startCollecting(smartwatch, SensorType.ACCELEROMETER, config);\n}\n\nfunction stop() {\n6    collectorManager.stopCollecting(smartwatch, SensorType.ACCELEROMETER);\n}\n\n1\n\nObtain a CollectorManager instance using the getCollectorManager() function.\n\n2\n\nRegister a listener with the CollectorManager#addSensorListener(...).\n\n3\n\nUse NodeDiscoverer#getConnectedNodes() to get a reference to the connected smartwatches.\n\n4\n\nCreate a CollectionConfiguration including the sensorInterval (microseconds) and batchSize.\n\n5\n\nStart the data collection on the selected smartwatch calling CollectorManager#startCollecting(...).\n\n6\n\nStop the data collection on the selected smartwatch calling CollectorManager#stopCollecting(...).",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data collection libraries</span>"
    ]
  },
  {
    "objectID": "02.1_collection-tools.html#integration-in-the-awarns-framework",
    "href": "02.1_collection-tools.html#integration-in-the-awarns-framework",
    "title": "Data collection libraries",
    "section": "Integration in the AwarNS Framework",
    "text": "Integration in the AwarNS Framework\nThe AwarNS Framework, implemented by a colleague researcher, is an Android-based development framework aimed to facilitate the development of context-aware smartphone applications which require systematic data acquisition, on-device analyses and perform actions based on these analyses (González-Pérez et al. 2023). The framework is built on top of the NativeScript Task Dispatcher, which allows the definition and execution of reactive workflows using its building blocks: Tasks (code units), Events (execution drivers) and TaskGraphs (allow the definition of reactive workflows, i.e., Tasks triggered by Events) (González-Pérez et al. 2022).\nThis dissertation contributes to the AwarNS Framework by wrapping the previously described data collection tools in it. This integration would allow the combination of already existing features in the framework (e.g., geolocation) with the features of the developed libraries (e.g., IMU data for HAR) to implement context-aware applications (e.g., recognition of activities in specific contexts).\nIn particular, two new packages are developed and added into the framework: the Phone Sensors and the Wear OS packages. These packages wrap up the Background Sensors and NativeScript WearOS Sensors libraries to integrate their functionality into the AwarNS Framework.\n\n\n\n\n\n\nNote\n\n\n\nFor a more detailed description of the AwarNS Framework, check its GitHub repository and the research paper (González-Pérez et al. 2023)\n\n\n\n\n\nUML diagram of the integration of Background Sensors and NativeScript WearOS Sensors in AwarNS.\n\n\n\n\n\n\n\n\nLibrary documentation\n\n\n\nThe full documentation of the libraries and their components can be found in the AwarNS Framework repository: Phone Sensors and WearOS Sensors.\n\n\n\n\n\n\nBoonstra, Tjeerd W, Jennifer Nicholas, Quincy JJ Wong, Frances Shaw, Samuel Townsend, and Helen Christensen. 2018. “Using Mobile Phone Sensor Technology for Mental Health Research: Integrated Analysis to Identify Hidden Challenges and Potential Solutions.” J. Med. Internet Res. 20 (7): e10131. https://doi.org/10.2196/10131.\n\n\nGonzález-Pérez, Alberto, Miguel Matey-Sanz, Carlos Granell, and Sven Casteleyn. 2022. “Using Mobile Devices as Scientific Measurement Instruments: Reliable Android Task Scheduling.” Pervasive and Mobile Computing 81: 101550. https://doi.org/10.1016/j.pmcj.2022.101550.\n\n\nGonzález-Pérez, Alberto, Miguel Matey-Sanz, Carlos Granell, Laura Díaz-Sanahuja, Juana Bretón-López, and Sven Casteleyn. 2023. “AwarNS: A Framework for Developing Context-Aware Reactive Mobile Applications for Health and Mental Health.” Journal of Biomedical Informatics, 104359. https://doi.org/10.1016/j.jbi.2023.104359.\n\n\nMatey-Sanz, Miguel, Sven Casteleyn, and Carlos Granell. 2024a. “Background Sensors.” Zenodo. https://doi.org/10.5281/zenodo.10635734.\n\n\n———. 2024b. “NativeScript WearOS Sensors.” Zenodo. https://doi.org/10.5281/zenodo.10640461.\n\n\n———. 2024c. “WearOS Sensors.” Zenodo. https://doi.org/10.5281/zenodo.10640429.",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data collection libraries</span>"
    ]
  },
  {
    "objectID": "02.3_methods.html",
    "href": "02.3_methods.html",
    "title": "Common methods",
    "section": "",
    "text": "ML and DL\nIn the context of HAR and thoroughout this thesis, labelled data is employed to train ML or DL models and to determine –classify– the activity a user is performing given some input data. Therefore, this dissertation faces a supervised learning classification problem.\nNext, the models employed, the evaluation metrics used to evaluate them and the tools used to develop them are described.",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common methods</span>"
    ]
  },
  {
    "objectID": "02.3_methods.html#sec-ml_methods",
    "href": "02.3_methods.html#sec-ml_methods",
    "title": "Common methods",
    "section": "",
    "text": "Models employed\n\nMultilayer Perceptron (MLP)\nThe Perceptron was invented by McCulloch and Pitts (1943), inspired by the neurons present in the brain, which receive, process and transmit information. Rosenblatt (1958) implemented the first Perceptron with learning capabilities, where a neuron would receive a series of inputs in the input layer. These inputs would be combined through a weighted sum and passed through an activation function, generating an output in the output layer. However, the Perceptron presented an issue: it could only solve problems with linear solutions.\nA solution for this problem was proposed with the MLP, which consisted of stacking several perceptions. However, the learning ability in these MLP was not possible until the development of the Backpropagation algorithm. The MLP is usually fed with features extracted from raw data after a feature engineering process into an input layer, multiple hidden layers and an output layer. Since feature engineering is required, MLP are considered ML techniques.\n\n\n\n\n\n\nFigure 5.1: Architecture of Multilayer Perceptron.\n\n\n\nFigure 5.1 shows the architecture of an MLP. Each layer contains a set of neurons, and each neuron is connected to the neurons in the next layer with a weight (parameters of the network). The outputs of the first hidden layer can be formulated as: \\[\\begin{equation}\n    h_i^1 = \\sigma^1(b+\\sum^{H_1}_{j=1}w_{ij}^1x_i)\\,,\n\\end{equation}\\] where \\(x_i\\) is the \\(i^{th}\\) input, \\(w_{ij}^1\\) is the weight of the connection between \\(x_i\\) and the \\(j^{th}\\) neuron in the layer, \\(b\\) is a constant bias not affected by the previous layer, \\(H_1\\) is the amount of neurons in the layer and \\(\\sigma^1\\) is the activation function. Given \\(k\\) layers, the equation can be generalized as follows: \\[\\begin{equation}\n    h_i^k = \\sigma^k(b+\\sum^{H_k}_{j=1}w_{ij}^kh_i^{k-1})\\,.\n\\end{equation}\\]\nThe results of the output layer can also be expressed as follows: \\[\\begin{equation}\n    y_i = \\sigma^{out}(b+\\sum^{H_{out}}_{j=1}w_{ij}^{out}h_i^{k})\\,.\n\\end{equation}\\]\n\n\nConvolutional Neural Network (CNN)\nFukushima (1980) invented the Neocognitron for vision-based pattern recognition, inspired by the work of (Hubel and Wiesel 1959), who showed that individual cells on the visual cortex respond to small regions of the visual field and that neighbouring cells have similar and overlapping receptive fields. Then, Atlas, Homma, and Marks (1987) proposed replacing the multiplications inside a neuron with a convolution, a computation between a small area (receptive field) and a filter containing the trainable weights of the network.\nThese networks are the CNN and contain convolution layers, which are useful for feature recognition in data with spatial and temporal domains. These networks have diverse applications, such as vision applications or time series prediction. They can be directly fed with raw data (i.e., no need for feature extraction), and therefore, are considered as DL techniques.\nThe convolutional layers apply a set of convolutional kernels (learnt during training) to an input to obtain an output. These kernels \\(K\\in R^{i\\times j}\\) are applied to an input matrix \\(A\\in R^{x\\times y}\\) sliding them over its width and height computing the convolution operation over each data point, obtaining as a result a new matrix \\(B\\in R^{m\\times n \\times d}\\), where \\(m=x-i+1\\), \\(n=y-j+1\\) and \\(d\\) is the number convolutional kernels applied. For 1 and 2-dimensional convolutions, the result for \\(B_{(m,n)}\\) is defined as an element-wise multiplication, or: \\[\\begin{equation}\n    B_{(m,n)} = \\sum_i\\sum_j A_{(m+i,n+j)}K_{(i,j)}\\,.\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 5.2: Architecture of Convolutional Neural Network.\n\n\n\nFigure 5.2 shows the architecture of a CNN network. Since the convolutional layers act as feature extractors, these networks add after the convolutional layers and a flattening layer to reduce the convolved output to one dimension, a classifier or a regressor (e.g., MLP).\n\n\nLong-Short Term Memory (LSTM)\nWhen the data has a sequential nature (e.g., time-series), RNN can be useful since they can remember information from the previous status and use it for the current status. This is possible because the output of specific neurons can be employed as subsequent input of the same neurons. However, the learning, driven by the backpropagation algorithm, suffers from several issues such as the vanishing and exploding gradient.\nTo solve these issues, Hochreiter and Schmidhuber (1997) invented the LSTM architecture, consisting of LSTM cells. An LSTM cell receives as input the state (\\(C_{t-1}\\), or long-term memory) and hidden state (\\(H_{t-1}\\), or short-term memory) of the previous cell at instant \\(t-1\\), and an input vector (\\(X_{t}\\)) at the current instant (\\(t\\)). Its outputs are the state (\\(C_t\\)) and the hidden state (\\(H_t\\)). Since the input vectors are raw sequences, they are considered DL techniques. While LSTM networks are usually used for forecasting, they can also be combined with classifiers (i.e., MLP) to solve such tasks.\nThe architecture of the LSTM cell is depicted in Figure 5.3. Internally, the cell is composed of three gates (i.e., neural networks):\n\nForget gate (\\(F_t\\), Equation 5.1): determines based on \\(X_t\\) and \\(H_{t-1}\\) (short-term memory) what information must be removed (i.e., forgotten) from the state at the previous instant (\\(C_{t-1}\\), long-term memory).\nInput gate (\\(I_t\\), Equation 5.2): determines based on \\(X_t\\) and \\(H_{t-1}\\) what information must be included (i.e., remembered) in the cell state (\\({C_t}\\)). The updated \\(C_{t}\\) Equation 5.4 will be used for the cell at \\(t+1\\).\nOutput gate (\\(O_t\\), Equation 5.3): determines based on \\(H_{t-1}\\) and \\(C_{t}\\) what information must be kept in \\(H_{t}\\) Equation 5.5 and used for the LSTM cell at \\(t+1\\).\n\n\n\n\n\n\n\nFigure 5.3: Architecture of the Long Short-Term Memory network.\n\n\n\n\\[\n    F_t=\\sigma(W^X_FX_t + W^{h_{t-1}}_Fh_{t-1} + b_F)\\,.\n\\tag{5.1}\\]\n\\[\n    I_t=\\sigma(W^X_IX_t + W^{h_{t-1}}_Ih_{t-1} + b_I) \\cdot \\tanh(W^X_CX_t + W^{h_{t-1}}_Ch_{t-1} + b_C)\\,.\n\\tag{5.2}\\]\n\\[\n    O_t=\\sigma(W^X_OX_t + W^{h_{t-1}}_Oh_{t-1} + b_O)\\,.\n\\tag{5.3}\\]\n\\[\n    C_t=C_{t-1} \\cdot F_t + I_t\\,.\n\\tag{5.4}\\]\n\\[\n    H_t=\\tanh(C_t) \\cdot O_t\\,.\n\\tag{5.5}\\]\n\n\nCNN Long-Short Term Memory (CNN-LSTM)\nWhile the LSTM architectures are useful for modelling temporal dependencies, they do not leverage the spatial nature of the data. To address this issue several solutions were proposed, such as the ConvLSTM network which extends the LSTM by adding convolutional operations in the input and state-to-state transitions (Shi et al. 2015). Another approach was to join the CNN and the LSTM networks, in the so-called CNN-LSTM network to take advantage of the spatial and temporal modelling capabilities of both networks (Sainath et al. 2015). Like the CNN and LSTM, both networks are considered DL techniques.\nIn this dissertation, we employ the CNN-LSTM since it has shown similar and better results than the ConvLSTM in HAR and other fields while being less resource-consuming.\nThe architecture of the CNN-LSTM network is depicted in Figure 5.4. The input of the network consists of convolutional layers for spatiotemporal modelling. Then, the outputs of those convolutional layers go through several LSTM layers for modelling temporal dependencies. Finally, the outputs of the LSTM layers are fed into a MLP to perform classification.\n\n\n\n\n\n\nFigure 5.4: Architecture of the CNN Long Short-Term Memory network.\n\n\n\n\n\n\nEvaluation metrics\nThe performance of the ML and DL models can be measured in several ways. Along this thesis, we employ the accuracy, precision, recall and F1-score metrics\n\nAccuracy\nMeasures the ratio between the correct and all the generated predictions. It allows to obtain an overall insight into the performance of the model.\n\\[\n    Accuracy=\\frac{Correct~predictions}{All~predictions}\\,.\n\\]\n\n\nPrecision\nThe ratio between the correct predictions of a certain class \\(A\\) and all the generated predictions of class \\(A\\), whether they are correct or not. It answers the following question: how good is the model identifying samples of class \\(A\\)? Precision is defined by the following equation: \\[\n    Precision_{A}=\\frac{Correct~predictions~of~A}{All~predictions~of~A}=\\frac{TP_{A}}{TP_{A} + FP_{A}}\\,,\n\\] where \\(TP_{A}\\) (true postives) is the number of correct preditions of class \\(A\\) and \\(FP_{A}\\) (false positives) is the number of samples wrongly predicted as class \\(A\\).\n\n\nRecall\nMeasures the ratio between the correct predictions of class \\(A\\) and all samples that should be predicted as \\(A\\). It answers the following question: how good is the model to identify samples of class \\(A\\)? Recall is defined as: \\[\n    Recall_{A}=\\frac{Correct~predictions~of~A}{All~real~instances~of~A}=\\frac{TP_{A}}{TP_{A} + FN_{A}}\\,,\n\\] where \\(FN_{A}\\) (false negatives) is the number of samples of class \\(A\\) classified to other classes.\n\n\nF1-score\nMeasures the predictive performance of a model on specific classes taking into account the precision and the recall. It is defined as: \\[\n    F1-score_{A}=2\\frac{Precision_{A}*Recall_{A}}{Precision_{A}+Recall_{A}}\\,.\n\\]\n\n\n\nTools\nThe ML and DL models employed in the following chapters have been built in Python 3.9 using the Keras library under the TensorFlow v2.10.0 backend.\nIn addition, this thesis contributes to the AwarNS Framework by developing a package to run ML and DL models in a smartphone device: the ML Kit package. This package allows to run TensorFlow Lite MLP and CNN models in Android smarthpones.\n\n\n\n\n\n\nAvailability\n\n\n\nThe full documentation of the library and its components can be found in the AwarNS Framework ML Kit repository. The library is available in:",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common methods</span>"
    ]
  },
  {
    "objectID": "02.3_methods.html#sec-stats_methods",
    "href": "02.3_methods.html#sec-stats_methods",
    "title": "Common methods",
    "section": "Statistical tools",
    "text": "Statistical tools\n\nSignificance tests\nThe significance tests, also known as statistical hypothesis tests, are a procedure to determine if the data sampled from a population supports a certain hypothesis (e.g., the samples with the trait \\(X\\) are “better” than the samples with the trait \\(Y\\)) that can be extrapolated to the whole population. These significance tests establish two complementary hypotheses:\n\n\\(H_0\\) (null hypothesis): considered true unless the data evidences otherwise.\n\\(H_1\\) (alternative hypothesis): must be proven by the data.\n\nThe procedure to determine the acceptance or rejection of any hypothesis involves the computation of a test statistic and its significance, i.e., p-value. The p-value indicates the probability of wrongly rejecting the null hypothesis. Therefore, with a \\(p-value &lt; \\alpha\\), \\(H_0\\) can be rejected, thus considering \\(H_1\\) with an acceptable error probability.\nNext, the significance tests employed in the following sections are described.\n\nShapiro-Wilk test\nThe Shapiro-Wilk test is used to contrast the normality of a distribution, i.e., the data is sampled from a normally distributed population (Shapiro and Wilk 1965). This test precedes other significance tests since the normality of the distribution must be taken into account to choose the right significance test.\n\n\nT-test\nThe T-test is any statistical hypothesis test which test statistic adheres to the Student’s T-distribution (Student 1908). The T-tests are parametric1 tests, thus they can only be used with normal distributions. A variation of the Student’s T-Test is Welch’s T-test (Welch 1947), employed when the distributions are not homoscedastic (i.e., unequal variances). Several T-tests are employed:\n1 Parametric tests operate with the mean of distributions.\nOne-sample T-test\nIt determines if the mean of a group is significantly different from a specific value. For example, it can be used to determine the errors in some measures obtained with a device are different from \\(0\\). Its hypothesis can be defined as: \\[\n    \\begin{cases}\n        H_0: \\mu_A = x\\,, \\\\\n        H_1: \\mu_A \\neq x\\,,\n    \\end{cases}\n\\] where \\(\\mu_A\\) represents the mean of the population \\(A\\) and \\(x\\) the specific value.\n\n\nTwo-sample T-test\nIt is used to determine if the difference between two groups is statistically significant. It can be used to know if two groups (\\(A\\) and \\(B\\)) are different and in consequence, determine which group is better. Its hypothesis can be defined as: \\[\n    \\begin{cases}\n        H_0: \\mu_A = \\mu_B\\,, \\\\\n        H_1: \\mu_A \\neq \\mu_B\\,.\n    \\end{cases}\n\\]\n\n\n\nWilcoxon signed-rank test (W-test)\nThe W-test is the non-parametric2 counterpart of the one-sample T-test (Wilcoxon 1945). It is applied to compare a non-normally distributed group with a specific value. Its hypothesis can be defined as: \\[\n    \\begin{cases}\n        H_0: \\eta_A = x\\,, \\\\\n        H_1: \\eta_A \\neq x\\,,\n    \\end{cases}\n\\] where \\(\\eta_A\\) represents the median of the population \\(A\\).\n2 Non-parametric tests usually operate with the median of distributions.\n\nMann-Whitney U-test (MWU)\nThe MWU is the non-parametric counterpart of the two-sample T-test (Mann and Whitney 1947). It should be applied to compare two non-normally distributed groups (\\(A\\) and \\(B\\)). Its hypothesis can be defined as: \\[\n    \\begin{cases}\n        H_0: \\eta_A = \\eta_B\\,, \\\\\n        H_1: \\eta_A \\neq \\eta_B\\,.\n    \\end{cases}\n\\]\nWhen multiple MWU tests are executed within groups (i.e., pairwise) and compared, the resulting p-values are corrected using the Benjamini/Hochberg False Discovery Rate correction.\n\n\nKruskal-Wallis H-test (KWH)\nThe KWH is a non-parametric significance test used to compare three or more non-normally distributed groups (Kruskal and Wallis 1952). Its hypothesis can be defined as: \\[\n    \\begin{cases}\n        H_0: \\eta \\mathit{~of~groups~are~equal}\\,, \\\\\n        H_1: \\eta \\mathit{~of~groups~are~different}\\,.\n    \\end{cases}\n\\]\nSince the KWH compares three or more groups, the rejection of \\(H_0\\) is not enough to determine the differences among groups. Therefore, post-hoc tests (e.g., MWU) must be applied to make conclusions.\n\n\n\nBland-Altman agreement (BA)\nThe BA analysis is a graphical tool helpful to determine the agreement between measurements from different systems, usually employed to compare a new measurement technique with a gold standard (Bland and Altman 1986).\nIt allows the identification of systematic differences between the measurements by computing the mean differences between them. A mean value different from \\(0\\) indicates the presence of a fixed bias in the measurement method. In addition, the \\(95\\%\\) limits of agreement are also computed to identify outliers.\n\n\nIntraclass Correlation Coefficient (ICC)\nThe ICC is used to assess the reliability of the measurements obtained from a specific method. ICC outputs a value between \\(0\\) and \\(1\\) and its \\(95\\%\\) confidence interval, where values less than \\(0.5\\), in \\(0.5-0.75\\), in \\(0.75-0.9\\), and greater than \\(0.9\\), respectively indicate poor, moderate, good, and excellent reliability (Koo and Li 2016).\nDepending on the measurement methods (i.e., raters), the way the measurements will be considered (e.g., single or aggregated) and the feature to consider (e.g., absolute agreement or consistency), there are up to ten different ICC definitions (McGraw and Wong 1996).\n\n\n\n\nAtlas, Les, Toshiteru Homma, and Robert Marks. 1987. “An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification.” In Neural Information Processing Systems.\n\n\nBland, J Martin, and DouglasG Altman. 1986. “Statistical Methods for Assessing Agreement Between Two Methods of Clinical Measurement.” The Lancet 327 (8476): 307–10. https://doi.org/10.1016/S0140-6736(86)90837-8.\n\n\nFukushima, Kunihiko. 1980. “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.” Biological Cybernetics 36 (4): 193–202. https://doi.org/10.1007/BF00344251.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nHubel, D. H., and T. N. Wiesel. 1959. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex.” The Journal of Physiology 148 (3): 574–91. https://doi.org/10.1113/jphysiol.1959.sp006308.\n\n\nKoo, Terry K, and Mae Y Li. 2016. “A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research.” Journal of Chiropractic Medicine 15 (2): 155–63. https://doi.org/10.1016/J.JCM.2016.02.012.\n\n\nKruskal, William H., and W. Allen Wallis. 1952. “Use of Ranks in One-Criterion Variance Analysis.” Journal of the American Statistical Association 47 (260): 583–621. https://doi.org/10.1080/01621459.1952.10483441.\n\n\nMann, Henry B, and Donald R Whitney. 1947. “On a Test of Whether One of Two Random Variables Is Stochastically Larger Than the Other.” The Annals of Mathematical Statistics, 50–60.\n\n\nMcCulloch, Warren S, and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” The Bulletin of Mathematical Biophysics 5: 115–33. https://doi.org/10.1007/BF02478259.\n\n\nMcGraw, Kenneth O, and Seok P Wong. 1996. “Forming Inferences about Some Intraclass Correlation Coefficients.” Psychological Methods 1 (1): 30. https://doi.org/10.1037/1082-989X.1.1.30.\n\n\nRosenblatt, Frank. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” Psychological Review 65 (6): 386. https://doi.org/10.1037/h0042519.\n\n\nSainath, Tara N., Oriol Vinyals, Andrew Senior, and Haşim Sak. 2015. “Convolutional, Long Short-Term Memory, Fully Connected Deep Neural Networks.” In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4580–84. https://doi.org/10.1109/ICASSP.2015.7178838.\n\n\nShapiro, S. S., and M. B. Wilk. 1965. “An analysis of variance test for normality (complete samples)†.” Biometrika 52 (3-4): 591–611. https://doi.org/10.1093/biomet/52.3-4.591.\n\n\nShi, Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun WOO. 2015. “Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting.” In Advances in Neural Information Processing Systems, edited by C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett. Vol. 28. Curran Associates, Inc.\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika, 1–25. https://doi.org/10.2307/2331554.\n\n\nWelch, B. L. 1947. “The Generalization of ‘Student’s’ Problem When Several Different Population Variances Are Involved.” Biometrika 34 (1-2): 28–35. https://doi.org/10.1093/biomet/34.1-2.28.\n\n\nWilcoxon, Frank. 1945. “Individual Comparisons by Ranking Methods.” Biometrics Bulletin 1 (6): 80–83. https://doi.org/10.2307/3001968.",
    "crumbs": [
      "Materials & Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common methods</span>"
    ]
  },
  {
    "objectID": "06_conclusions.html",
    "href": "06_conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Accomplishment of objectives\nIn Introduction, the objectives of this research were defined. Next, a summary is provided for each goal indicating how it was fulfilled.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "06_conclusions.html#accomplishment-of-objectives",
    "href": "06_conclusions.html#accomplishment-of-objectives",
    "title": "Conclusions",
    "section": "",
    "text": "RO1: Develop software tools for reliable data collection in Android smartphones and Wear OS smartwatches\nData collection libraries described the Background Sensors, WearOS Sensors and NativeScript WearOS Sensors libraries, which grant access to some specific sensors in Android smartphones and smartwatches. The libraries were implemented following the guidelines presented by González-Pérez et al. (2022), which proved to achieve a reliable task execution (i.e., data collection) with less than \\(1\\%\\) of missing data.\nThese libraries were developed and tested to fully support smartphones and smartwatches running up to Android 13 and Wear OS 4, respectively. Although these OS versions are not the latest, significant technical changes have not been introduced in most recent versions – Android 14 and 15 and Wear OS 51 – that would affect their operation.\n1 At the time of writing this dissertation, Android 15 is in Beta phase and Wear OS 5 is in Developer Previews (i.e., features and technical requirements might still change in the release version).The libraries were made available as open-source software. In addition, the libraries were integrated into the AwarNS Framework, a modular framework to ease the development of context-aware applications (González-Pérez et al. 2023).\n\n\nRO2: Collect HAR dataset of smartphone and smartwatch data from heterogeneous subjects\nSmartphone and smartwatch HAR dataset described a dataset containing accelerometer and gyroscope samples from a smartphone and a smartwatch associated with five human activities. The dataset was collected focusing on the heterogeneity and diversity of the participants, resulting in a dataset with these characteristics: the widest age range of its subjects and a balance of \\(56\\%\\)/\\(44\\%\\) male/female participants. However, when compared with related datasets, the collected dataset was limited in terms of the number of activities and participants.\n\n\nRO3: Analyse and compare the effect of the amount of training data, the data source and the model architecture on the performance of ML- and DL-based HAR\nMultidimensional analysis of ML and DL on HAR presented an analysis exploring how the classification performance on four ML and DL models and three data sources were affected by the amount of data used for training them. Therefore, a three-way statistical analysis considering the amount of data, type of model and data source was executed.\nResults unveiled different patterns in how the performance of classification models evolves depending on the amount of training data and the data source employed. Regarding data sources, the smartwatch data provided the best results when the amount of available data was low, while the fusion of smartphone and smartwatch data generally obtained the best results as the amount of data increased. When focusing on the models, the CNN showed the best results in almost any circumstance, making it the clear choice for HAR systems.\n\n\nRO4: Develop and evaluate a mHealth system using HAR with smartphones and smartwatches\nHAR in mHealth: TUG test using smartphones and smartwatches a mHealth system to automate the execution of the TUG test was implemented and analytically validated. The system, composed of two applications, employed the inertial data from a consumer smartphone or smartwatch to automatically compute the test’s results.\nBoth systems were validated on \\(30\\) test subjects and compared, showing an average error of \\(44\\) ms and \\(51\\) ms when computing the total duration of the test with the smartwatch and smartphone inertial data, respectively. In general, results indicated that the system performed better with the smartwatch data than with the smartphone data.\nCompared with other proposals in the literature, the developed system improved the Bland-Altman results of other state-of-the-art solutions, although it presented lower Intraclass Correlation Coefficient results than other approaches. Notwithstanding, the evaluation outcomes showed that HAR with consumer smartphones and smartwatches can successfully be used for real-life mHealth applications, which typically require a high degree of reliability.\n\n\nRO5: Analyse the feasibility of Wi-Fi CSI data for real-life HAR systems\nLooking into the future: Wi-Fi CSI based HAR explored the usage of the Wi-Fi CSI applied to HAR, while also taking advantage of its reported versatility adding a location dimension to the target activities.\nA first experiment showed good results when evaluating the classification accuracy of a CNN in a limited timeframe, which however downgraded when evaluating it using data from a different time span than the one used for training it. Subsequent experiments indicated a clear similarity between the CSI data of adjacent days while being different from other days. These results suggested the instability of the CSI data, thus hampering its usage for real-life applications.\nHowever, there is still work to be done since the observed instability might have been produced by hardware or software limitations in the employed devices or by uncontrollable interferences in the environment.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "06_conclusions.html#contributions",
    "href": "06_conclusions.html#contributions",
    "title": "Conclusions",
    "section": "Contributions",
    "text": "Contributions\nSeveral research outcomes were produced during the development of this thesis. These outcomes are listed below and classified as journal or conference contributions, datasets, reproducibility packages, software.\n\nJournals\n\n“Dataset of inertial measurements of smartphones and smartwatches for human activity recognition”. In: Data in Brief (Matey-Sanz, Casteleyn, and Granell 2023a).\n“Implementing and evaluating the Timed Up and Go test automation using smartphones and smartwatches”. In: IEEE Journal of Biomedical and Health Informatics (Matey-Sanz, González-Pérez, et al. 2024).\n\n\n\nConferences\n\n“Instrumented Timed Up and Go Test Using Inertial Sensors from Consumer Wearable Devices”. In: International Conference on Artificial Intelligence in Medicine (Matey-Sanz et al. 2022).\n“Analysis and Impact of Training Set Size in Cross-Subject Human Activity Recognition”. In: 26th Iberoamerican Congress on Pattern Recognition (Matey-Sanz, Torres-Sospedra, et al. 2024).\n“Temporal Stability on Human Activity Recognition based on Wi-Fi CSI”. In: 13th International Conference on Indoor Positioning and Indoor Navigation (Matey-Sanz, Torres-Sospedra, and Moreira 2023).\n\n\n\nDatasets\n\n“Smartphone and smartwatch inertial measurements from heterogeneous subjects for human activity recognition”. In: Zenodo (Matey-Sanz, Casteleyn, and Granell 2023b).\n\n\n\nReproducibility packages\n\nCode and data resources for “Instrumented Timed Up and Go Test Using Inertial Sensors from Consumer Wearable Devices”. In Zenodo (Matey-Sanz 2022).\nReproducible package for “Analysis and Impact of Training Set Size in Cross-Subject Human Activity Recognition”. In Zenodo (Matey-Sanz 2023b).\nReproducible package for “Temporal Stability on Human Activity Recognition based on Wi-Fi CSI”. In Zenodo (Matey-Sanz 2023a).\n\n\n\nSoftwate\n\nBackground Sensors [v1.3.0] (Matey-Sanz, Casteleyn, and Granell 2024a)\nWearOS Sensors [v1.2.1] (Matey-Sanz, Casteleyn, and Granell 2024c)\nNativeScript WearOS Sensors [v1.3.0] (Matey-Sanz, Casteleyn, and Granell 2024b)\nTug Test Smartphone Application [v2.0.0] (Matey-Sanz and González-Pérez 2022a)\nTug Test Smartwatch Application [v2.0.0] (Matey-Sanz and González-Pérez 2022b)\n\n\n\n\n\nArrotta, Luca, Gabriele Civitarese, Riccardo Presotto, and Claudio Bettini. 2023. “DOMINO: A Dataset for Context-Aware Human Activity Recognition Using Mobile Devices.” In 2023 24th IEEE International Conference on Mobile Data Management (MDM), 346–51. IEEE. https://doi.org/10.1109/MDM58254.2023.00063.\n\n\nGonzález-Pérez, Alberto, Miguel Matey-Sanz, Carlos Granell, and Sven Casteleyn. 2022. “Using Mobile Devices as Scientific Measurement Instruments: Reliable Android Task Scheduling.” Pervasive and Mobile Computing 81: 101550. https://doi.org/10.1016/j.pmcj.2022.101550.\n\n\nGonzález-Pérez, Alberto, Miguel Matey-Sanz, Carlos Granell, Laura Díaz-Sanahuja, Juana Bretón-López, and Sven Casteleyn. 2023. “AwarNS: A Framework for Developing Context-Aware Reactive Mobile Applications for Health and Mental Health.” Journal of Biomedical Informatics, 104359. https://doi.org/10.1016/j.jbi.2023.104359.\n\n\nMatey-Sanz, Miguel. 2022. “Code and data resources for \"Instrumented Timed Up and Go test using inertial sensors from consumer wearable devices\".” Zenodo. https://doi.org/10.5281/zenodo.6405874.\n\n\n———. 2023a. “Reproducible package for \"Temporal Stability on Human Activity Recognition based on Wi-Fi CSI\".” Zenodo. https://doi.org/10.5281/zenodo.7991716.\n\n\n———. 2023b. “Reproducible Package for \"Analysis and Impact of Training Set Size in Cross-Subject Human Activity Recognition\".” Zenodo. https://doi.org/10.5281/zenodo.8163542.\n\n\nMatey-Sanz, Miguel, Sven Casteleyn, and Carlos Granell. 2023a. “Dataset of Inertial Measurements of Smartphones and Smartwatches for Human Activity Recognition.” Data in Brief 51: 109809. https://doi.org/10.1016/j.dib.2023.109809.\n\n\n———. 2023b. “Smartphone and smartwatch inertial measurements from heterogeneous subjects for human activity recognition.” Zenodo. https://doi.org/10.5281/zenodo.8398688.\n\n\n———. 2024a. “Background Sensors.” Zenodo. https://doi.org/10.5281/zenodo.10635734.\n\n\n———. 2024b. “NativeScript WearOS Sensors.” Zenodo. https://doi.org/10.5281/zenodo.10640461.\n\n\n———. 2024c. “WearOS Sensors.” Zenodo. https://doi.org/10.5281/zenodo.10640429.\n\n\nMatey-Sanz, Miguel, and Alberto González-Pérez. 2022a. “TUG Test Smartphone Application.” Zenodo. https://doi.org/10.5281/zenodo.7456835.\n\n\n———. 2022b. “TUG Test Smartwatch Application.” Zenodo. https://doi.org/10.5281/zenodo.7457098.\n\n\nMatey-Sanz, Miguel, Alberto González-Pérez, Sven Casteleyn, and Carlos Granell. 2022. “Instrumented Timed up and Go Test Using Inertial Sensors from Consumer Wearable Devices.” In International Conference on Artificial Intelligence in Medicine, 144–54. Springer. https://doi.org/10.1007/978-3-031-09342-5\\_14.\n\n\n———. 2024. “Implementing and Evaluating the Timed up and Go Test Automation Using Smartphones and Smartwatches.” IEEE Journal of Biomedical and Health Informatics 28 (11): 6594–6605. https://doi.org/10.1109/JBHI.2024.3456169.\n\n\nMatey-Sanz, Miguel, Joaquín Torres-Sospedra, Alberto González-Pérez, Sven Casteleyn, and Carlos Granell. 2024. “Analysis and Impact of Training Set Size in Cross-Subject Human Activity Recognition.” In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, 391–405. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-49018-7_28.\n\n\nMatey-Sanz, Miguel, Joaquín Torres-Sospedra, and Adriano Moreira. 2023. “Temporal Stability on Human Activity Recognition Based on Wi-Fi CSI.” In 2023 13th International Conference on Indoor Positioning and Indoor Navigation (IPIN), 1–6. https://doi.org/10.1109/IPIN57070.2023.10332214.\n\n\nStisen, Allan, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. 2015. “Smart Devices Are Different: Assessing and Mitigating Mobile Sensing Heterogeneities for Activity Recognition.” In Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems, 127–40. https://doi.org/10.1145/2809695.2809718.\n\n\nSztyler, Timo, and Heiner Stuckenschmidt. 2016. “On-Body Localization of Wearable Devices: An Investigation of Position-Aware Activity Recognition.” In 2016 IEEE International Conference on Pervasive Computing and Communications (PerCom), 1–9. https://doi.org/10.1109/PERCOM.2016.7456521.\n\n\nVaizman, Yonatan, Katherine Ellis, and Gert Lanckriet. 2017. “Recognizing Detailed Human Context in the Wild from Smartphones and Smartwatches.” IEEE Pervasive Computing 16 (4): 62–74. https://doi.org/10.1109/MPRV.2017.3971131.\n\n\nWeiss, Gary M, Kenichi Yoneda, and Thaier Hayajneh. 2019. “Smartphone and Smartwatch-Based Biometrics Using Activities of Daily Living.” IEEE Access 7: 133190–202. https://doi.org/10.1109/ACCESS.2019.2940729.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Conclusions</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adame, M Reyes, Ahmed Al-Jawad, Michailas Romanovas, Markus A Hobert,\nWalter Maetzler, Knut Möller, and Yiannos Manoli. 2012. “TUG Test\nInstrumentation for Parkinson’s Disease Patients Using Inertial Sensors\nand Dynamic Time Warping.” Biomedical\nEngineering/Biomedizinische Technik 57 (SI-1-Track-E): 1071–74. https://doi.org/10.1515/bmt-2012-4426.\n\n\nAlsaify, Baha A, Mahmoud M Almazari, Rami Alazrai, and Mohammad I Daoud.\n2020. “A Dataset for Wi-Fi-Based Human Activity Recognition in\nLine-of-Sight and Non-Line-of-Sight Indoor Environments.”\nData in Brief 33: 106534. https://doi.org/10.1016/j.dib.2020.106534.\n\n\nAnsai, Juliana Hotta, Ana Claudia Silva Farche, Paulo Giusti Rossi,\nLarissa Pires de Andrade, Theresa Helissa Nakagawa, and Anielle\nCristhine de Medeiros Takahashi. 2019. “Performance of Different\nTimed up and Go Subtasks in Frailty Syndrome.” Journal of\nGeriatric Physical Therapy 42 (4): 287–93. https://doi.org/10.1519/JPT.0000000000000162.\n\n\nArrotta, Luca, Gabriele Civitarese, Riccardo Presotto, and Claudio\nBettini. 2023. “DOMINO: A Dataset for Context-Aware Human Activity\nRecognition Using Mobile Devices.” In 2023 24th IEEE\nInternational Conference on Mobile Data Management (MDM), 346–51.\nIEEE. https://doi.org/10.1109/MDM58254.2023.00063.\n\n\nAtlas, Les, Toshiteru Homma, and Robert Marks. 1987. “An Artificial Neural Network for Spatio-Temporal Bipolar\nPatterns: Application to Phoneme Classification.” In\nNeural Information Processing Systems.\n\n\nBeyea, James, Chris A McGibbon, Andrew Sexton, Jeremy Noble, and Colleen\nO’Connell. 2017. “Convergent Validity of a Wearable Sensor System\nfor Measuring Sub-Task Performance During the Timed up-and-Go\nTest.” Sensors 17 (4): 934. https://doi.org/10.3390/S17040934.\n\n\nBland, J Martin, and DouglasG Altman. 1986. “Statistical Methods\nfor Assessing Agreement Between Two Methods of Clinical\nMeasurement.” The Lancet 327 (8476): 307–10. https://doi.org/10.1016/S0140-6736(86)90837-8.\n\n\nBoonstra, Tjeerd W, Jennifer Nicholas, Quincy JJ Wong, Frances Shaw,\nSamuel Townsend, and Helen Christensen. 2018. “Using Mobile Phone\nSensor Technology for Mental Health Research: Integrated Analysis to\nIdentify Hidden Challenges and Potential Solutions.” J. Med.\nInternet Res. 20 (7): e10131. https://doi.org/10.2196/10131.\n\n\nChoi, Hyuckjin, Manato Fujimoto, Tomokazu Matsui, Shinya Misaki, and\nKeiichi Yasumoto. 2022. “Wi-CaL: WiFi Sensing and Machine Learning\nBased Device-Free Crowd Counting and Localization.” IEEE\nAccess 10: 24395–410. https://doi.org/10.1109/ACCESS.2022.3155812.\n\n\nCoelln, Rainer von et al. 2019. “Quantitative Mobility Metrics\nfrom a Wearable Sensor Predict Incident Parkinsonism in Older\nAdults.” Parkinsonism & Related Disorders 65:\n190–96. https://doi.org/10.1016/J.PARKRELDIS.2019.06.012.\n\n\nCoskun, Doruk, Ozlem Durmaz Incel, and Atay Ozgovde. 2015. “Phone\nPosition/Placement Detection Using Accelerometer: Impact on Activity\nRecognition.” In 2015 IEEE Tenth International Conference on\nIntelligent Sensors, Sensor Networks and Information Processing\n(ISSNIP), 1–6. https://doi.org/10.1109/ISSNIP.2015.7106915.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. 1996.\n“A Density-Based Algorithm for Discovering Clusters in Large\nSpatial Databases with Noise.” In Kdd, 96:226–31. 34.\n\n\nFigo, Davide, Pedro C Diniz, Diogo R Ferreira, and Joao MP Cardoso.\n2010. “Preprocessing Techniques for Context Recognition from\nAccelerometer Data.” Personal and Ubiquitous Computing\n14: 645–62. https://doi.org/10.1007/s00779-010-0293-9.\n\n\nFukushima, Kunihiko. 1980. “Neocognitron: A Self-Organizing Neural\nNetwork Model for a Mechanism of Pattern Recognition Unaffected by Shift\nin Position.” Biological Cybernetics 36 (4): 193–202. https://doi.org/10.1007/BF00344251.\n\n\nGholamiangonabadi, Davoud, Nikita Kiselov, and Katarina Grolinger. 2020.\n“Deep Neural Networks for Human Activity Recognition with Wearable\nSensors: Leave-One-Subject-Out Cross-Validation for Model\nSelection.” IEEE Access 8: 133982–94. https://doi.org/10.1109/ACCESS.2020.3010715.\n\n\nGoldsack, Jennifer C, Andrea Coravos, Jessie P Bakker, Brinnae Bent,\nAriel V Dowling, Cheryl Fitzer-Attas, Alan Godfrey, et al. 2020.\n“Verification, Analytical Validation, and Clinical Validation\n(V3): The Foundation of Determining Fit-for-Purpose for Biometric\nMonitoring Technologies (BioMeTs).” Npj Digital Medicine\n3 (1): 55. https://doi.org/10.1038/S41746-020-0260-4.\n\n\nGonzález-Pérez, Alberto, Miguel Matey-Sanz, Carlos Granell, and Sven\nCasteleyn. 2022. “Using Mobile Devices as Scientific Measurement\nInstruments: Reliable Android Task Scheduling.” Pervasive and\nMobile Computing 81: 101550. https://doi.org/10.1016/j.pmcj.2022.101550.\n\n\nGonzález-Pérez, Alberto, Miguel Matey-Sanz, Carlos Granell, Laura\nDíaz-Sanahuja, Juana Bretón-López, and Sven Casteleyn. 2023.\n“AwarNS: A Framework for Developing Context-Aware Reactive Mobile\nApplications for Health and Mental Health.” Journal of\nBiomedical Informatics, 104359. https://doi.org/10.1016/j.jbi.2023.104359.\n\n\nGupta, Neha, Suneet K Gupta, Rajesh K Pathak, Vanita Jain, Parisa\nRashidi, and Jasjit S Suri. 2022. “Human Activity Recognition in\nArtificial Intelligence Framework: A Narrative Review.”\nArtificial Intelligence Review 55 (6): 4755–4808. https://doi.org/10.1007/s10462-021-10116-x.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term\nMemory.” Neural Computation 9 (8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nHubel, D. H., and T. N. Wiesel. 1959. “Receptive Fields of Single\nNeurones in the Cat’s Striate Cortex.” The Journal of\nPhysiology 148 (3): 574–91. https://doi.org/10.1113/jphysiol.1959.sp006308.\n\n\nJaén-Vargas, Milagros et al. 2022. “Effects of Sliding Window\nVariation in the Performance of Acceleration-Based Human Activity\nRecognition Using Deep Learning Models.” PeerJ Computer\nScience 8: e1052. https://doi.org/10.7717/peerj-cs.1052.\n\n\nKoo, Terry K, and Mae Y Li. 2016. “A Guideline of Selecting and\nReporting Intraclass Correlation Coefficients for Reliability\nResearch.” Journal of Chiropractic Medicine 15 (2):\n155–63. https://doi.org/10.1016/J.JCM.2016.02.012.\n\n\nKruskal, William H., and W. Allen Wallis. 1952. “Use of Ranks in\nOne-Criterion Variance Analysis.” Journal of the American\nStatistical Association 47 (260): 583–621. https://doi.org/10.1080/01621459.1952.10483441.\n\n\nMa, Yongsen, Gang Zhou, and Shuangquan Wang. 2019. “WiFi Sensing\nwith Channel State Information: A Survey.” ACM Comput.\nSurv. 52 (3): 1–36. https://doi.org/10.1145/3310194.\n\n\nMadhushri, Priyanka, Armen A Dzhagaryan, Emil Jovanov, and Aleksandar\nMilenkovic. 2016. “A Smartphone Application Suite for Assessing\nMobility.” In 38th Annual International Conference of the\nIEEE Engineering in Medicine and Biology Society (EMBC), 3117–20.\nIEEE. https://doi.org/10.1109/EMBC.2016.7591389.\n\n\nMann, Henry B, and Donald R Whitney. 1947. “On a Test of Whether\nOne of Two Random Variables Is Stochastically Larger Than the\nOther.” The Annals of Mathematical Statistics, 50–60.\n\n\nMatey-Sanz, Miguel. 2022. “Code and data\nresources for \"Instrumented Timed Up and Go test using inertial sensors\nfrom consumer wearable devices\".” Zenodo. https://doi.org/10.5281/zenodo.6405874.\n\n\n———. 2023a. “Reproducible package for\n\"Temporal Stability on Human Activity Recognition based on Wi-Fi\nCSI\".” Zenodo. https://doi.org/10.5281/zenodo.7991716.\n\n\n———. 2023b. “Reproducible Package for\n\"Analysis and Impact of Training Set Size in Cross-Subject Human\nActivity Recognition\".” Zenodo. https://doi.org/10.5281/zenodo.8163542.\n\n\nMatey-Sanz, Miguel, Sven Casteleyn, and Carlos Granell. 2023a.\n“Dataset of Inertial Measurements of Smartphones and Smartwatches\nfor Human Activity Recognition.” Data in Brief 51:\n109809. https://doi.org/10.1016/j.dib.2023.109809.\n\n\n———. 2023b. “Smartphone and smartwatch\ninertial measurements from heterogeneous subjects for human activity\nrecognition.” Zenodo. https://doi.org/10.5281/zenodo.8398688.\n\n\n———. 2024a. “Background Sensors.” Zenodo. https://doi.org/10.5281/zenodo.10635734.\n\n\n———. 2024b. “NativeScript WearOS Sensors.” Zenodo. https://doi.org/10.5281/zenodo.10640461.\n\n\n———. 2024c. “WearOS Sensors.” Zenodo. https://doi.org/10.5281/zenodo.10640429.\n\n\nMatey-Sanz, Miguel, and Alberto González-Pérez. 2022a. “TUG Test\nSmartphone Application.” Zenodo. https://doi.org/10.5281/zenodo.7456835.\n\n\n———. 2022b. “TUG Test Smartwatch Application.” Zenodo. https://doi.org/10.5281/zenodo.7457098.\n\n\nMatey-Sanz, Miguel, Alberto González-Pérez, Sven Casteleyn, and Carlos\nGranell. 2022. “Instrumented Timed up and Go Test Using Inertial\nSensors from Consumer Wearable Devices.” In International\nConference on Artificial Intelligence in Medicine, 144–54.\nSpringer. https://doi.org/10.1007/978-3-031-09342-5\\_14.\n\n\n———. 2024. “Implementing and Evaluating the Timed up and Go Test\nAutomation Using Smartphones and Smartwatches.” IEEE Journal\nof Biomedical and Health Informatics 28 (11): 6594–6605. https://doi.org/10.1109/JBHI.2024.3456169.\n\n\nMatey-Sanz, Miguel, Joaquín Torres-Sospedra, Alberto González-Pérez,\nSven Casteleyn, and Carlos Granell. 2024. “Analysis and Impact of\nTraining Set Size in Cross-Subject Human Activity Recognition.”\nIn Progress in Pattern Recognition, Image Analysis, Computer Vision,\nand Applications, 391–405. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-49018-7_28.\n\n\nMatey-Sanz, Miguel, Joaquín Torres-Sospedra, and Adriano Moreira. 2023.\n“Temporal Stability on Human Activity Recognition Based on Wi-Fi\nCSI.” In 2023 13th International Conference on Indoor\nPositioning and Indoor Navigation (IPIN), 1–6. https://doi.org/10.1109/IPIN57070.2023.10332214.\n\n\nMcCulloch, Warren S, and Walter Pitts. 1943. “A Logical Calculus\nof the Ideas Immanent in Nervous Activity.” The Bulletin of\nMathematical Biophysics 5: 115–33. https://doi.org/10.1007/BF02478259.\n\n\nMcGraw, Kenneth O, and Seok P Wong. 1996. “Forming Inferences\nabout Some Intraclass Correlation Coefficients.”\nPsychological Methods 1 (1): 30. https://doi.org/10.1037/1082-989X.1.1.30.\n\n\nMilosevic, Mladen, Emil Jovanov, and Aleksandar Milenković. 2013.\n“Quantifying Timed-up-and-Go Test: A Smartphone\nImplementation.” In 2013 IEEE International Conference on\nBody Sensor Networks, 1–6. IEEE. https://doi.org/10.1109/BSN.2013.6575478.\n\n\nMinh Dang, L., Kyungbok Min, Hanxiang Wang, Md. Jalil Piran, Cheol Hee\nLee, and Hyeonjoon Moon. 2020. “Sensor-Based and Vision-Based\nHuman Activity Recognition: A Comprehensive Survey.” Pattern\nRecognition 108: 107561. https://doi.org/https://doi.org/10.1016/j.patcog.2020.107561.\n\n\nPodsiadlo, Diane, and Sandra Richardson. 1991. “The Timed\n‘up & Go’: A Test of Basic Functional Mobility for\nFrail Elderly Persons.” Journal of the American Geriatrics\nSociety 39 (2): 142–48. https://doi.org/10.1111/j.1532-5415.1991.tb01616.x.\n\n\nRosenblatt, Frank. 1958. “The Perceptron: A Probabilistic Model\nfor Information Storage and Organization in the Brain.”\nPsychological Review 65 (6): 386. https://doi.org/10.1037/h0042519.\n\n\nSainath, Tara N., Oriol Vinyals, Andrew Senior, and Haşim Sak. 2015.\n“Convolutional, Long Short-Term Memory, Fully Connected Deep\nNeural Networks.” In 2015 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 4580–84. https://doi.org/10.1109/ICASSP.2015.7178838.\n\n\nSalarian, Arash et al. 2010. “iTUG, a Sensitive and Reliable\nMeasure of Mobility.” IEEE Transactions on Neural Systems and\nRehabilitation Engineering 18 (3): 303–10. https://doi.org/10.1109/TNSRE.2010.2047606.\n\n\nSansano, Emilio et al. 2020. “A Study of Deep Neural Networks for\nHuman Activity Recognition.” Comput. Intell. 36 (3):\n1113–39. https://doi.org/10.1111/coin.12318.\n\n\nShapiro, S. S., and M. B. Wilk. 1965. “An\nanalysis of variance test for normality (complete\nsamples)†.” Biometrika 52 (3-4): 591–611. https://doi.org/10.1093/biomet/52.3-4.591.\n\n\nShi, Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and\nWang-chun WOO. 2015. “Convolutional LSTM Network: A Machine\nLearning Approach for Precipitation Nowcasting.” In Advances\nin Neural Information Processing Systems, edited by C. Cortes, N.\nLawrence, D. Lee, M. Sugiyama, and R. Garnett. Vol. 28. Curran\nAssociates, Inc.\n\n\nStisen, Allan, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\nMikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen.\n2015. “Smart Devices Are Different: Assessing and Mitigating\nMobile Sensing Heterogeneities for Activity Recognition.” In\nProceedings of the 13th ACM Conference on Embedded Networked Sensor\nSystems, 127–40. https://doi.org/10.1145/2809695.2809718.\n\n\nStudent. 1908. “The Probable Error of a Mean.”\nBiometrika, 1–25. https://doi.org/10.2307/2331554.\n\n\nSztyler, Timo, and Heiner Stuckenschmidt. 2016. “On-Body\nLocalization of Wearable Devices: An Investigation of Position-Aware\nActivity Recognition.” In 2016 IEEE International Conference\non Pervasive Computing and Communications (PerCom), 1–9. https://doi.org/10.1109/PERCOM.2016.7456521.\n\n\nVaizman, Yonatan, Katherine Ellis, and Gert Lanckriet. 2017.\n“Recognizing Detailed Human Context in the Wild from Smartphones\nand Smartwatches.” IEEE Pervasive Computing 16 (4):\n62–74. https://doi.org/10.1109/MPRV.2017.3971131.\n\n\nWeiss, Gary M, Kenichi Yoneda, and Thaier Hayajneh. 2019.\n“Smartphone and Smartwatch-Based Biometrics Using Activities of\nDaily Living.” IEEE Access 7: 133190–202. https://doi.org/10.1109/ACCESS.2019.2940729.\n\n\nWelch, B. L. 1947. “The Generalization of ‘Student’s’\nProblem When Several Different Population Variances Are\nInvolved.” Biometrika 34 (1-2): 28–35. https://doi.org/10.1093/biomet/34.1-2.28.\n\n\nWilcoxon, Frank. 1945. “Individual Comparisons by Ranking\nMethods.” Biometrics Bulletin 1 (6): 80–83. https://doi.org/10.2307/3001968.\n\n\nYousefi, Siamak, Hirokazu Narui, Sankalp Dayal, Stefano Ermon, and\nShahrokh Valaee. 2017. “A Survey on Behavior Recognition Using\nWiFi Channel State Information.” IEEE Communications\nMagazine 55 (10): 98–104. https://doi.org/10.1109/MCOM.2017.1700082.\n\n\nZakaria, Nor Aini, Yutaka Kuwae, Toshiyo Tamura, Kotaro Minato, and\nShigehiko Kanaya. 2015. “Quantitative Analysis of Fall Risk Using\nTUG Test.” Computer Methods in Biomechanics and Biomedical\nEngineering 18 (4): 426–37. https://doi.org/10.1080/10255842.2013.805211.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "reference/chapter2.data_loading.html",
    "href": "reference/chapter2.data_loading.html",
    "title": "Appendix A — chapter2.data_loading",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>chapter2.data_loading</span>"
    ]
  },
  {
    "objectID": "reference/chapter2.data_loading.html#functions",
    "href": "reference/chapter2.data_loading.html#functions",
    "title": "Appendix A — chapter2.data_loading",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nload_data\nLoads the accelerometer and gyroscope data for each execution.\n\n\nload_executions_info\nLoads the ‘executions_info.csv’ file containing information about the executions (id, phone orientation, turns direction)\n\n\nload_subjects_info\nLoads the ‘subjects_info.csv’ file containing information about the subjects (age, gender, executions)\n\n\n\n\nload_data\nchapter2.data_loading.load_data(path=os.path.join('data', 'chapter2'))\nLoads the accelerometer and gyroscope data for each execution.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nRoot directory of the data.\nos.path.join('data', 'chapter2')\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict\nDict containing pandas dataframes with the accelerometer and gyroscope data for each execution.\n\n\n\n\n\n\nload_executions_info\nchapter2.data_loading.load_executions_info(path=os.path.join('data', 'chapter2', 'executions_info.csv'))\nLoads the ‘executions_info.csv’ file containing information about the executions (id, phone orientation, turns direction)\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nPath of the file.\nos.path.join('data', 'chapter2', 'executions_info.csv')\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame with the contents of the file\n\n\n\n\n\n\nload_subjects_info\nchapter2.data_loading.load_subjects_info(path=os.path.join('data', 'chapter2', 'subjects_info.csv'))\nLoads the ‘subjects_info.csv’ file containing information about the subjects (age, gender, executions)\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nPath of the file.\nos.path.join('data', 'chapter2', 'subjects_info.csv')\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame with the contents of the file",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>chapter2.data_loading</span>"
    ]
  },
  {
    "objectID": "reference/chapter2.exploration.html",
    "href": "reference/chapter2.exploration.html",
    "title": "Appendix B — chapter2.exploration",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>chapter2.exploration</span>"
    ]
  },
  {
    "objectID": "reference/chapter2.exploration.html#functions",
    "href": "reference/chapter2.exploration.html#functions",
    "title": "Appendix B — chapter2.exploration",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncount_samples\nCounts the number of collected samples for each activity and device.\n\n\nexecutions_by_gender\nCounts the number of executions grouped by gender.\n\n\nsubjects_age_range\nComputes age range statisitcs from the subjects of the data collection.\n\n\nsubjects_age_range_by_gender\nComputes age range statisitcs grouped by gender from the subjects of the data collection.\n\n\n\n\ncount_samples\nchapter2.exploration.count_samples(data_collection)\nCounts the number of collected samples for each activity and device.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata_collection\ndict\nCollected data. Use utils.data_loading.load_data() to load the collected data.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame with the count of collected samples.\n\n\n\n\n\n\nexecutions_by_gender\nchapter2.exploration.executions_by_gender(subjects_info)\nCounts the number of executions grouped by gender.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubjects_info\npandas.DataFrame\nDataFrame with the information of the subjects. See: utils.data_loading.load_subjects_info().\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame with executions count grouped by gender.\n\n\n\n\n\n\nsubjects_age_range\nchapter2.exploration.subjects_age_range(subjects_info)\nComputes age range statisitcs from the subjects of the data collection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubjects_info\npandas.DataFrame\nDataFrame with the information of the subjects. See: utils.data_loading.load_subjects_info().\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame with age range statistics.\n\n\n\n\n\n\nsubjects_age_range_by_gender\nchapter2.exploration.subjects_age_range_by_gender(subjects_info)\nComputes age range statisitcs grouped by gender from the subjects of the data collection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubjects_info\npandas.DataFrame\nDataFrame with the information of the subjects. See: utils.data_loading.load_subjects_info().\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame with age range statistics grouped by gender.",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>chapter2.exploration</span>"
    ]
  },
  {
    "objectID": "reference/chapter2.visualization.html",
    "href": "reference/chapter2.visualization.html",
    "title": "Appendix C — chapter2.visualization",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>chapter2.visualization</span>"
    ]
  },
  {
    "objectID": "reference/chapter2.visualization.html#functions",
    "href": "reference/chapter2.visualization.html#functions",
    "title": "Appendix C — chapter2.visualization",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nplot_execution\nGenerates an interactive plot with the accelerometer and gyroscope data of the specified execution.\n\n\nplot_orientation_stats\nGenerates an interactive plot counting the different phone orientations in the executions.\n\n\nplot_turn_direction_combined_stats\nGenerates an interactive plot counting the turning direction of the first_turn and second_turn combined.\n\n\nplot_turn_direction_stats\nGenerates an interactive plot counting the turning direction (right or left) of the first_turn and second_turn\n\n\n\n\nplot_execution\nchapter2.visualization.plot_execution(data_collection, execution)\nGenerates an interactive plot with the accelerometer and gyroscope data of the specified execution.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata_collection\ndict\nDict containing the collected dataset. See: utils.data_loading.load_data()\nrequired\n\n\nexecution\nstr\nexecution data to plot. Format: ‘sXX_YY_{sp|sw}’\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.graph_objs.Figure\nInteractive plot\n\n\n\n\n\n\nplot_orientation_stats\nchapter2.visualization.plot_orientation_stats(executions_info)\nGenerates an interactive plot counting the different phone orientations in the executions.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecutions_info\npandas.DataFrame\nDataFrame with the information of the executions. See: data_loading.load_executions_info()\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.graph_objs.Figure\nInteractive plot\n\n\n\n\n\n\nplot_turn_direction_combined_stats\nchapter2.visualization.plot_turn_direction_combined_stats(executions_info)\nGenerates an interactive plot counting the turning direction of the first_turn and second_turn combined.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecutions_info\npandas.DataFrame\nDataFrame with the information of the executions. See: data_loading.load_executions_info()\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.graph_objs.Figure\nInteractive plot\n\n\n\n\n\n\nplot_turn_direction_stats\nchapter2.visualization.plot_turn_direction_stats(executions_info)\nGenerates an interactive plot counting the turning direction (right or left) of the first_turn and second_turn\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecutions_info\npandas.DataFrame\nDataFrame with the information of the executions. See: data_loading.load_executions_info()\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.graph_objs.Figure\nInteractive plot",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>chapter2.visualization</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.pipeline.01_data-processing.html",
    "href": "reference/chapter3.pipeline.01_data-processing.html",
    "title": "Appendix D — chapter3.pipeline.01_data-processing",
    "section": "",
    "text": "chapter3.pipeline.01_data-processing\nData preprocessing script.\nProcesses the raw data by: applying min-max scaling, fusing smartphone and smartwatch data, arange samples in windows and perform feature extraction. The script stores the raw windows, windows with features extracted and groundtruth for smartphone, smartwatch and fused data.\nExample:\n$ python 01_data-processing.py --input_data_path &lt;PATH_OF_RAW_DATA&gt; --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>chapter3.pipeline.01_data-processing</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.pipeline.02_hyperparameter-optimization.html",
    "href": "reference/chapter3.pipeline.02_hyperparameter-optimization.html",
    "title": "Appendix E — chapter3.pipeline.02_hyperparameter-optimization",
    "section": "",
    "text": "chapter3.pipeline.02_hyperparameter-optimization\nHyperparameters Grid Search script.\nPerforms an hyperparameter Grid Search on the specified model. The selected hyperparameters for the search can be found in tuning_configuration.py.\nExample:\n$ python 02_hyperparameter-optimization.py \n    --data_dir &lt;PATH_OF_DATA&gt; \n    --model &lt;MLP,CNN,LSTM,CNN-LSTM&gt;\n    --phase &lt;initial,extra-layers&gt;\n    --batch_size &lt;BATCH_SIZE&gt;\n    --epochs &lt;EPOCHS&gt;\n    --executions &lt;EXECUTIONS&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>chapter3.pipeline.02_hyperparameter-optimization</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.pipeline.03_incremental-loso.html",
    "href": "reference/chapter3.pipeline.03_incremental-loso.html",
    "title": "Appendix F — chapter3.pipeline.03_incremental-loso",
    "section": "",
    "text": "chapter3.pipeline.03_incremental-loso\nIncremental Leaving-One-Subject-Out script.\nPerforms the ILOSO evaluation.\nExample:\n$ python 03_incremental-loso.py \n    --data_dir &lt;PATH_OF_DATA&gt; \n    --reports_dir &lt;PATH_TO_STORE_RECORDS&gt;\n    --model &lt;MLP,CNN,LSTM,CNN-LSTM&gt;\n    --subject &lt;EVALUATION_SUBJECT&gt;\n    --batch_size &lt;BATCH_SIZE&gt;\n    --epochs &lt;EPOCHS&gt;\n    --splits &lt;SPLITS&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>chapter3.pipeline.03_incremental-loso</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.analysis.data_loading.html",
    "href": "reference/chapter3.analysis.data_loading.html",
    "title": "Appendix G — chapter3.analysis.data_loading",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>chapter3.analysis.data_loading</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.analysis.data_loading.html#functions",
    "href": "reference/chapter3.analysis.data_loading.html#functions",
    "title": "Appendix G — chapter3.analysis.data_loading",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nload_best_significant\nLoads a CSV file containing the number of best significant data sources/models for each combination of number of training\n\n\nload_reports\nLoads the ML and DL reports generated from the ILOSO evaluation.\n\n\n\n\nload_best_significant\nchapter3.analysis.data_loading.load_best_significant(path)\nLoads a CSV file containing the number of best significant data sources/models for each combination of number of training subjects and models/data sources.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nPath to the CSV file.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing specified CSV.\n\n\n\n\n\n\nload_reports\nchapter3.analysis.data_loading.load_reports(path=os.path.join('data', 'chapter3', 'model-reports'))\nLoads the ML and DL reports generated from the ILOSO evaluation.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nRoot directory of the data.\nos.path.join('data', 'chapter3', 'model-reports')\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing the generated reports.",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>chapter3.analysis.data_loading</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.analysis.model.html",
    "href": "reference/chapter3.analysis.model.html",
    "title": "Appendix H — chapter3.analysis.model",
    "section": "",
    "text": "Classes",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>chapter3.analysis.model</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.analysis.model.html#classes",
    "href": "reference/chapter3.analysis.model.html#classes",
    "title": "Appendix H — chapter3.analysis.model",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nActivityMetric\nEnumeration for each metric of interest from the model reports for individual activities. Values: PRECISION, RECALL, F1, SUPPORT.\n\n\nFilter\nClass to represent a filter that can be applied to a dataframe of reports\n\n\nModel\nEnumeration to represent the ML and DL models. Values: MLP, CNN, LSTM, CNN_LSTM.\n\n\nModelMetric\nEnumeration for each metric of interest from the model reports. Values: ACCURACY, TRAINING_TIME.\n\n\nSource\nEnumeration to represent the data sources. Values: SP, SW, FUSED.\n\n\nTargetFilter\nEnumeration for each attribute of interest from the model reports. Values: MODEL, SEATED, STANDING_UP, WALKING, TURNING, SITTING_DOWN.\n\n\n\n\nActivityMetric\nchapter3.analysis.model.ActivityMetric()\nEnumeration for each metric of interest from the model reports for individual activities. Values: PRECISION, RECALL, F1, SUPPORT.\n\n\nFilter\nchapter3.analysis.model.Filter(self, model, source, target, metric)\nClass to represent a filter that can be applied to a dataframe of reports\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nmodel\nstr\nModel to look for. One of libs.chapter3.model.Model\n\n\nsource\nstr\nSource to look for. One of libs.chapter3.model.Source\n\n\ntarget\nstr\nTarget to look for. One of libs.chapter3.model.TargetFilter.\n\n\nmetric\nstr\nMetric to look for. One of libs.chapter3.model.ModelMetric or libs.chapter3.model.ActivityMetric\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\napply\nFunction to apply the filter to a specified dataframe.\n\n\n\n\napply\nchapter3.analysis.model.Filter.apply(df)\nFunction to apply the filter to a specified dataframe.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.DataFrame\nModel reports.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nFiltered model reports.\n\n\n\n\n\n\n\n\nModel\nchapter3.analysis.model.Model()\nEnumeration to represent the ML and DL models. Values: MLP, CNN, LSTM, CNN_LSTM.\n\n\nModelMetric\nchapter3.analysis.model.ModelMetric()\nEnumeration for each metric of interest from the model reports. Values: ACCURACY, TRAINING_TIME.\n\n\nSource\nchapter3.analysis.model.Source()\nEnumeration to represent the data sources. Values: SP, SW, FUSED.\n\n\nTargetFilter\nchapter3.analysis.model.TargetFilter()\nEnumeration for each attribute of interest from the model reports. Values: MODEL, SEATED, STANDING_UP, WALKING, TURNING, SITTING_DOWN.",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>chapter3.analysis.model</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.analysis.model.html#functions",
    "href": "reference/chapter3.analysis.model.html#functions",
    "title": "Appendix H — chapter3.analysis.model",
    "section": "Functions",
    "text": "Functions\n\n\n\nName\nDescription\n\n\n\n\nobtain_best_items\nDetermines the best performant item (model or data source) from a DataFrame of statistical tests comparing groups.\n\n\n\n\nobtain_best_items\nchapter3.analysis.model.obtain_best_items(test_results, focus_on, groups)\nDetermines the best performant item (model or data source) from a DataFrame of statistical tests comparing groups.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntest_results\npandas.DataFrame\nDataFrame with statistical tests. Generated with libs.chapter3.statistical_tests.statistical_comparison.\nrequired\n\n\nfocus_on\nlist[str]\nItems being compared. List items are one of libs.chapter3.model.Source or libs.chapter3.model.Model.\nrequired\n\n\ngroups\nlist[str]\nEach group where focus_on items are compared. List items are one of libs.chapter3.model.Source or libs.chapter3.model.Model.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\ndict\nDict containing the best performant item.",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>chapter3.analysis.model</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.analysis.statistical_tests.html",
    "href": "reference/chapter3.analysis.statistical_tests.html",
    "title": "Appendix I — chapter3.analysis.statistical_tests",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>chapter3.analysis.statistical_tests</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.analysis.statistical_tests.html#functions",
    "href": "reference/chapter3.analysis.statistical_tests.html#functions",
    "title": "Appendix I — chapter3.analysis.statistical_tests",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nis_parametric_data\nDetermines if the results in the reports follow a parametric or a non-parametric distribution.\n\n\npairwise_n_comparision\nComputes pairwise tests for each value of n.\n\n\nstatistical_comparison\n\n\n\n\n\nis_parametric_data\nchapter3.analysis.statistical_tests.is_parametric_data(reports, models, sources)\nDetermines if the results in the reports follow a parametric or a non-parametric distribution.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreports\npandas.DataFrame\nModel reports.\nrequired\n\n\nmodels\nlist[libs.chapter3.model.Models]\nList with the models.\nrequired\n\n\nsources\nlist[libs.chapter3.model.Source]\nList with the data sources.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame indicating if the results from a model+source are parametric (True) or not (False).\n\n\n\n\n\n\npairwise_n_comparision\nchapter3.analysis.statistical_tests.pairwise_n_comparision(data, filters, alternative='two-sided', stars=False, parametric=False)\nComputes pairwise tests for each value of n.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npandas.DataFrame\nModel reports.\nrequired\n\n\nfilters\nstr\nFilter to apply to the model reports. See: libs.chapter3.model.Filter\nrequired\n\n\nalternative\nstr\nHypothesis to test. One of: ‘two-sided’, ‘less’ or ‘greater’.\n'two-sided'\n\n\nstars\nboolean\nReplace p-values under 0.05 by stars. ‘’ when 0.01&lt;p-value&lt;0.05; ’’ when 0.001&lt;p-value&lt;0.01; ’’ when p-value&lt;0.001;\nFalse\n\n\nparametric\nboolean\nCompute parametric or non-parametric tests.\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing the pairwise tests.\n\n\n\n\n\n\nstatistical_comparison\nchapter3.analysis.statistical_tests.statistical_comparison(reports, metric_filter, focus_on, groups, alternative='two-sided')\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreports\npandas.DataFrame\nModel reports.\nrequired\n\n\nmetric_filter\ntuple[str, str]\nMetric filter to apply to the model reports.\nrequired\n\n\nfocus_on\nlist[str]\nItems being compared. List items are one of libs.chapter3.model.Source or libs.chapter3.model.Model.\nrequired\n\n\ngroups\nlist[str]\nEach group where focus_on items are compared. List items are one of libs.chapter3.model.Source or libs.chapter3.model.Model.\nrequired\n\n\nalternative\nstr\nHypothesis to test. One of: ‘two-sided’, ‘less’ or ‘greater’.\n'two-sided'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing groups comparisons.",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>chapter3.analysis.statistical_tests</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.analysis.visualization.html",
    "href": "reference/chapter3.analysis.visualization.html",
    "title": "Appendix J — chapter3.analysis.visualization",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>chapter3.analysis.visualization</span>"
    ]
  },
  {
    "objectID": "reference/chapter3.analysis.visualization.html#functions",
    "href": "reference/chapter3.analysis.visualization.html#functions",
    "title": "Appendix J — chapter3.analysis.visualization",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nplot_comparison\nGenerates a figure containing a plot for each data source and model showing the accuracy/F1-score evolution with regards to the training data (i.e., n).\n\n\nplot_evolution\nGenerates a figure containing a plot for each data source showing the accuracy/F1-score evolution with regards to the training data (i.e., n).\n\n\nplot_pairwise_comparision\nGenerates a figure to represent the pairwise tests generated by libs.chapter3.statistical_tests.pairwise_n_comparison\n\n\nplot_visual_comparison\nGenerates a figure to visually summarize statistical group comparisons. For each group, plots a symbol representing the best performant item (focus_on).\n\n\nplot_visual_ties\nGenerates a figure to indicate the ties in the group comparions. For each item, indicates how many times it has tied with other item.\n\n\n\n\nplot_comparison\nchapter3.analysis.visualization.plot_comparison(reports, models, sources, filters, sources_print)\nGenerates a figure containing a plot for each data source and model showing the accuracy/F1-score evolution with regards to the training data (i.e., n).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreports\npandas.pandas.DataFrame\nModel reports.\nrequired\n\n\nmodels\nlist[libs.chapter3.model.Models]\nList with the models to include in the figure.\nrequired\n\n\nsources\nlist[libs.chapter3.model.Source]\nList with the data sources to include in the figure.\nrequired\n\n\nfilters\nlibs.chapter3.model.Filter\nFilter to apply to the model reports.\nrequired\n\n\nsources_print\ndict\nMapping between a Source and a string representation.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.Figure\nInteractive Plotly figure.\n\n\n\n\n\n\nplot_evolution\nchapter3.analysis.visualization.plot_evolution(reports, sources, filters, fig_titles, filters_secondary=None)\nGenerates a figure containing a plot for each data source showing the accuracy/F1-score evolution with regards to the training data (i.e., n).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreports\npandas.DataFrame\nModel reports.\nrequired\n\n\nsources\nlist[libs.chapter3.model.Source]\nList with the data sources to include in the figure.\nrequired\n\n\nfilters\nlibs.chapter3.model.Filter\nFilter to apply to the model reports.\nrequired\n\n\nfig_titles\nlist[str]\nTitle to use for the plot of each data source.\nrequired\n\n\nfilters_secondary\nlibs.chapter3.model.Filter\nFilter to apply to the model reports, plotting the result in the secondary axis.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.plotly.Figure\nInteractive Plotly figure.\n\n\n\n\n\n\nplot_pairwise_comparision\nchapter3.analysis.visualization.plot_pairwise_comparision(reports, sources, filters, sources_print, alternative='two-sided', stars=False, parametric=False)\nGenerates a figure to represent the pairwise tests generated by libs.chapter3.statistical_tests.pairwise_n_comparison\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreports\npandas.DataFrame\nModel reports.\nrequired\n\n\nsources\nlist[libs.chapter3.model.Source]\nList with the data sources to include in the figure.\nrequired\n\n\nfilters\nlibs.chapter3.model.Filter\nFilter to apply to the model reports.\nrequired\n\n\nsources_print\ndict\nMapping between a Source and a string representation.\nrequired\n\n\nalternative\nstr\nHypothesis to test. One of: ‘two-sided’, ‘less’ or ‘greater’.\n'two-sided'\n\n\nstars\nboolean\nReplace p-values under 0.05 by stars. ‘’ when 0.01&lt;p-value&lt;0.05; ’’ when 0.001&lt;p-value&lt;0.01; ’’ when p-value&lt;0.001;\nFalse\n\n\nparametric\nboolean\nCompute parametric or non-parametric tests.\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.Figure\nInteractive Plotly figure.\n\n\n\n\n\n\nplot_visual_comparison\nchapter3.analysis.visualization.plot_visual_comparison(best_items, significance_results, focus_on, groups)\nGenerates a figure to visually summarize statistical group comparisons. For each group, plots a symbol representing the best performant item (focus_on). If the item is the statistically best performant item (i.e., doesn’t ties with other item), the symbol is filled. Otherwise, the symbol contains a number indicating the quantity of items the best performant item ties with.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbest_items\ndict\nBest item from a performance comparison. See: libs.chapter3.model.obtain_best_items.\nrequired\n\n\nsignificance_results\npd.DataFrame\nDataFrame containing the number of best significant data sources/models for each combination of number of training\nrequired\n\n\n\nsubjects and models/data sources. focus_on (list[str]): Items being compared. List items are one of libs.chapter3.model.Source or libs.chapter3.model.Model. groups (list[str]): Each group where focus_on items are compared. List items are one of libs.chapter3.model.Source or libs.chapter3.model.Model.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.Figure\nInteractive Plotly figure.\n\n\n\n\n\n\nplot_visual_ties\nchapter3.analysis.visualization.plot_visual_ties(best_items, significance_results, focus_on, groups)\nGenerates a figure to indicate the ties in the group comparions. For each item, indicates how many times it has tied with other item. Complementary figure to libs.chapter3.visualization.plot_visual_comparison.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbest_items\ndict\nBest item from a performance comparison. See: libs.chapter3.model.obtain_best_items.\nrequired\n\n\nsignificance_results\npd.DataFrame\nDataFrame containing the number of best significant data sources/models for each combination of number of training\nrequired\n\n\n\nsubjects and models/data sources. focus_on (list[str]): Items being compared. List items are one of libs.chapter3.model.Source or libs.chapter3.model.Model. groups (list[str]): Each group where focus_on items are compared. List items are one of libs.chapter3.model.Source or libs.chapter3.model.Model.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.Figure\nInteractive Plotly figure.",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>chapter3.analysis.visualization</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.pipeline.01_relabel.html",
    "href": "reference/chapter4.pipeline.01_relabel.html",
    "title": "Appendix K — chapter4.pipeline.01_relabel",
    "section": "",
    "text": "chapter4.pipeline.01_relabel\nData relabelling script.\nRelabels the windowed data by replacing the TURNING and SITTING_DOWN labels by the TURN_TO_SIT label. Note that only the TURNING activities inmediately before the SITTING_DOWN activity are replaced by TURN_TO_SIT.\nExample:\n$ python 01_relabel.py --input_data_path &lt;PATH_OF_WINDOWED_DATA&gt; --output_data_path &lt;PATH_TO_STORE_RELABELLED_DATA&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>chapter4.pipeline.01_relabel</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.pipeline.02_splitting-evaluation.html",
    "href": "reference/chapter4.pipeline.02_splitting-evaluation.html",
    "title": "Appendix L — chapter4.pipeline.02_splitting-evaluation",
    "section": "",
    "text": "chapter4.pipeline.02_splitting-evaluation\nSplitting approach evaluation script\nThis script trains 100 models for each data source (smartphone, smartwatch) and splitting approach. For the training process, a 80/20 train/test split is employed with a batch size of 20 windows during 50 epochs.\nExample:\n$ python 02_splitting-evaluation.py \n    --ts_data_path &lt;PATH_OF_TURNING_SITTING_DATA&gt; \n    --tts_data_path &lt;PATH_OF_TURN_TO_SIT_DATA&gt;\n    --reports_output_path &lt;PATH_TO_STORE_REPORTS&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>chapter4.pipeline.02_splitting-evaluation</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.analysis.data_loading.html",
    "href": "reference/chapter4.analysis.data_loading.html",
    "title": "Appendix M — chapter4.analysis.data_loading",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>chapter4.analysis.data_loading</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.analysis.data_loading.html#functions",
    "href": "reference/chapter4.analysis.data_loading.html#functions",
    "title": "Appendix M — chapter4.analysis.data_loading",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nload_battery_results\nLoads the battery consumption report manually generated from Google’s BatteryHistorian tool.\n\n\nload_experiment_results\nLoads the results obtained in the experiment by each participant.\n\n\nload_reports\nLoads the DL reports generated from the splitting approach evaluation.\n\n\nload_subjects_info\nLoads a CSV file containing the information regarding the participants in the evaluation of the system.\n\n\n\n\nload_battery_results\nchapter4.analysis.data_loading.load_battery_results(path=os.path.join('data', 'chapter4', 'battery-consumption', 'report.csv'))\nLoads the battery consumption report manually generated from Google’s BatteryHistorian tool.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nPath to the CSV file.\nos.path.join('data', 'chapter4', 'battery-consumption', 'report.csv')\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame with battery consumption results.\n\n\n\n\n\n\nload_experiment_results\nchapter4.analysis.data_loading.load_experiment_results(path=os.path.join('data', 'chapter4', 'system-results'))\nLoads the results obtained in the experiment by each participant.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nDirectory containing the results of the experiment.\nos.path.join('data', 'chapter4', 'system-results')\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame with the loaded experiment results.\n\n\n\n\n\n\nload_reports\nchapter4.analysis.data_loading.load_reports(reports_path=os.path.join('data', 'chapter4', 'splitting-approach', 'reports.json'))\nLoads the DL reports generated from the splitting approach evaluation.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreports_path\nstr\nRoot directory of the data.\nos.path.join('data', 'chapter4', 'splitting-approach', 'reports.json')\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\ndict\nDictionary containing the generated reports.\n\n\n\n\n\n\nload_subjects_info\nchapter4.analysis.data_loading.load_subjects_info(path=os.path.join('data', 'chapter4', 'system-results', 'subjects.csv'))\nLoads a CSV file containing the information regarding the participants in the evaluation of the system.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nPath to the CSV file.\nos.path.join('data', 'chapter4', 'system-results', 'subjects.csv')\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame with the information of the participants.\n\n\nstr\nFormatted string with statistics regarding participants’ age (e.g., range, mean, std).\n\n\nstr\nFormatted string with statistics regarding participants’ gender (e.g., male/female ratio).",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>chapter4.analysis.data_loading</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.analysis.statistical_tests.html",
    "href": "reference/chapter4.analysis.statistical_tests.html",
    "title": "Appendix N — chapter4.analysis.statistical_tests",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>chapter4.analysis.statistical_tests</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.analysis.statistical_tests.html#functions",
    "href": "reference/chapter4.analysis.statistical_tests.html#functions",
    "title": "Appendix N — chapter4.analysis.statistical_tests",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncompare_distribution_with_zero\nStatistically compares the mean of a distribution with 0 using a T-test (normal data) or a W-test (non-normal data).\n\n\ncompare_rmse_distributions\nStatistically compares the RMSE of two distributions using a two-sample T-test (normal data) or a MWU test (non-normal data).\n\n\ncompare_splitting_approaches\nStatistically compares the two splitting approaches on smartphone and smartwatch data. More concretely,\n\n\ncompute_icc\nComputes the Intraclass Correlation Coefficient (ICC) between the TUG results obtained by the system and\n\n\n\n\ncompare_distribution_with_zero\nchapter4.analysis.statistical_tests.compare_distribution_with_zero(distribution)\nStatistically compares the mean of a distribution with 0 using a T-test (normal data) or a W-test (non-normal data).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndistribution\nlist[float]\narray of numbers constituting the distribution.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nP-value of the test. A value less than 0.05 indicates a significant difference between the distribution mean and zero.\n\n\nbool\nIndicates if the distribution is normal or not and, therefore, which statistical test was used.\n\n\n\n\n\n\ncompare_rmse_distributions\nchapter4.analysis.statistical_tests.compare_rmse_distributions(errors_df)\nStatistically compares the RMSE of two distributions using a two-sample T-test (normal data) or a MWU test (non-normal data). The compared distributions are the inter-subject RMSE of the TUG duration and each subphase.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nerrors_df\npandas.DataFrame\nDataFrame containing the error in ms of the system measures and the reference method for all subjects.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing the tests results of comparing each measure (i.e., TUG duration and subphases).\n\n\n\n\n\n\ncompare_splitting_approaches\nchapter4.analysis.statistical_tests.compare_splitting_approaches(reports, metrics)\nStatistically compares the two splitting approaches on smartphone and smartwatch data. More concretely, determines if there is a significant difference in the accuracy of the models or the F1-score of the TURNING, SITTING_DOWN and TURN_TO_SIT activities.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreports\ndict\nModel reports.\nrequired\n\n\nmetrics\nlist[str]\nPerformance metric to compare.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing the statistical test results.\n\n\n\n\n\n\ncompute_icc\nchapter4.analysis.statistical_tests.compute_icc(system_results, manual_results, labels, icc_type='ICC2')\nComputes the Intraclass Correlation Coefficient (ICC) between the TUG results obtained by the system and the reference methods for each TUG measure.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_results\nlist[pandas.DataFrame]\nContains the DataFrames with the measures generated by both system’s configurations.\nrequired\n\n\nmanual_results\npandas.DataFrame\nDataFrame containing the measures generated by the reference method.\nrequired\n\n\nlabels\nlist[str]\nText labels associated with the DataFrame in system_results.\nrequired\n\n\nicc_type\nstr\nType of ICC to compute. One of: ICC1, ICC2, ICC3, ICC1k, ICC2k, ICC3k.\n'ICC2'\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.DataFrame\nDataFrame containing the ICC results for each TUG measure and system (C1 and C2).",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>chapter4.analysis.statistical_tests</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.analysis.tug_results_processing.html",
    "href": "reference/chapter4.analysis.tug_results_processing.html",
    "title": "Appendix O — chapter4.analysis.tug_results_processing",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>chapter4.analysis.tug_results_processing</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.analysis.tug_results_processing.html#functions",
    "href": "reference/chapter4.analysis.tug_results_processing.html#functions",
    "title": "Appendix O — chapter4.analysis.tug_results_processing",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncheck_status\nChecks the status of the system’s generated results of a TUG execution. An execution is classified\n\n\ncompute_errors_by_subject\nComputes the error in milliseconds between the measures obtained by the systems and the reference measures.\n\n\ncompute_rmse_by_subject\nComputes the intra-subject RMSE between the systems’ and reference methods.\n\n\nextract_manual_results\nExtracts the TUG results generated by the reference system.\n\n\nextract_system_results\nExtracts the TUG results generated by the system.\n\n\ninvalidate_executions\nInvalidates TUG executions not considered as failures by the system but that were wrongly executed due to external factors.\n\n\n\n\ncheck_status\nchapter4.analysis.tug_results_processing.check_status(row)\nChecks the status of the system’s generated results of a TUG execution. An execution is classified as success when all measures where obtained, partial_success when the duration of the test was obtained but the measure of one or more subphases is missing, failure when the duration of the test was not obtained.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrow\npandas.Series\nResults of a TUG execution.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe status of the execution. One of success, partial_success or failure.\n\n\n\n\n\n\ncompute_errors_by_subject\nchapter4.analysis.tug_results_processing.compute_errors_by_subject(systems_results, man_results)\nComputes the error in milliseconds between the measures obtained by the systems and the reference measures.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystems_results\ndict\nDict containing the results (values) generated by both system’s configurations: C1 and C2 (keys).\nrequired\n\n\nman_results\npandas.DataFrame\nDataFrame containing the reference measures.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing the errors between systems’ and reference measures.\n\n\n\n\n\n\ncompute_rmse_by_subject\nchapter4.analysis.tug_results_processing.compute_rmse_by_subject(errors_df)\nComputes the intra-subject RMSE between the systems’ and reference methods.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nerrors_df\npandas.DataFrame\nDataFrame containing the error in ms of the system measures and the reference method for all subjects.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing the intra-subject RMSE for each subject, system and measure.\n\n\n\n\n\n\nextract_manual_results\nchapter4.analysis.tug_results_processing.extract_manual_results(subject, results_file)\nExtracts the TUG results generated by the reference system.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubject\nstr\nID of the subject whose results are extracted.\nrequired\n\n\nresults_file\nstr\nPath of the file containing the results.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing the extracted results.\n\n\n\n\n\n\nextract_system_results\nchapter4.analysis.tug_results_processing.extract_system_results(subject, results_file)\nExtracts the TUG results generated by the system.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubject\nstr\nID of the subject whose results are extracted.\nrequired\n\n\nresults_file\nstr\nPath of the file containing the results.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing the extracted results.\n\n\n\n\n\n\ninvalidate_executions\nchapter4.analysis.tug_results_processing.invalidate_executions(df, executions)\nInvalidates TUG executions not considered as failures by the system but that were wrongly executed due to external factors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.DataFrame\nDataFrame containing the TUG test system’s results.\nrequired\n\n\nexecutions\ndict\nDict indicating which executions (values) of which subject (key) have to be invalidated.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame with the specified samples invalidated.",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>chapter4.analysis.tug_results_processing</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.analysis.visualization.html",
    "href": "reference/chapter4.analysis.visualization.html",
    "title": "Appendix P — chapter4.analysis.visualization",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>chapter4.analysis.visualization</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.analysis.visualization.html#functions",
    "href": "reference/chapter4.analysis.visualization.html#functions",
    "title": "Appendix P — chapter4.analysis.visualization",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbland_altman_plot\nGenerates a Bland-Altman plot to visualize the agreement of the measures generated by the system and the reference method.\n\n\nerror_distribution\nGenerates a figure containing a box plot for each TUG measure and system configuration to show their distributions.\n\n\n\n\nbland_altman_plot\nchapter4.analysis.visualization.bland_altman_plot(system_results, man_results, system_desc, attrs, with_titles=True, limit_y_axis_to=None)\nGenerates a Bland-Altman plot to visualize the agreement of the measures generated by the system and the reference method.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_results\npandas.DataFrame\nDataFrame containing the system’s generated measures.\nrequired\n\n\nman_results\npandas.DataFrame\nDataFrame containing the reference measures.\nrequired\n\n\nsystem_desc\nstr\nLabel identifier for the system measures (e.g., C1, C2, etc.).\nrequired\n\n\nattrs\nlist[str]\nList containing the TUG measures to include in the plot as subplots.\nrequired\n\n\nwith_titles\nbool\nIf True, include a title (i.e., measure’s name) for each subplot.\nTrue\n\n\nlimit_y_axis_to\nlist[float] | None\nIf not None, the y axis of the plots will be limited to the specified interval.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.Figure\nInteractive Plotly figure.\n\n\n\n\n\n\nerror_distribution\nchapter4.analysis.visualization.error_distribution(errors_df)\nGenerates a figure containing a box plot for each TUG measure and system configuration to show their distributions. For each distribution, indicates if it is statistically different from 0 (i.e., no error) or not showing a p-value of a statistical test below the box plot.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nerrors_df\npandas.DataFrame\nDataFrame containing the errors between the systema and the reference measures.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.Figure\nInteractive Plotly figure.",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>chapter4.analysis.visualization</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.analysis.battery.html",
    "href": "reference/chapter4.analysis.battery.html",
    "title": "Appendix Q — chapter4.analysis.battery",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>Q</span>  <span class='chapter-title'>chapter4.analysis.battery</span>"
    ]
  },
  {
    "objectID": "reference/chapter4.analysis.battery.html#functions",
    "href": "reference/chapter4.analysis.battery.html#functions",
    "title": "Appendix Q — chapter4.analysis.battery",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmean_consumption_per_device\nComputes the mean battery consumption per device and system. The battery consumption is reported in mA and the corresponding\n\n\n\n\nmean_consumption_per_device\nchapter4.analysis.battery.mean_consumption_per_device(battery_df)\nComputes the mean battery consumption per device and system. The battery consumption is reported in mA and the corresponding ratio in terms of the battery’s total capacity.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbattery_df\npandas.DataFrame\n\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>Q</span>  <span class='chapter-title'>chapter4.analysis.battery</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.pipeline.01_1_preliminar-dataset-processing.html",
    "href": "reference/chapter5.pipeline.01_1_preliminar-dataset-processing.html",
    "title": "Appendix R — chapter5.pipeline.01_1_preliminar-dataset-processing",
    "section": "",
    "text": "chapter5.pipeline.01_1_preliminar-dataset-processing\nData preprocessing script for preliminar dataset.\nProcesses the raw data by: arange samples in windows and process them using 1) DBSCAN for outlier detection and 2-level DWT for threshold based filtering or 2) Choi et al. method.\nExample:\n$ python 01_1_preliminar-dataset-processing.py \n    --input_data_path &lt;PATH_OF_RAW_DATA&gt; \n    --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;\n    --method &lt;PROCESSING_METHOD&gt;\n    --window_size &lt;WINDOW_SIZE&gt;\n    --window_overlap &lt;WINDOW_OVERLAP&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>R</span>  <span class='chapter-title'>chapter5.pipeline.01_1_preliminar-dataset-processing</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.pipeline.01_2_stanwifi-processing.html",
    "href": "reference/chapter5.pipeline.01_2_stanwifi-processing.html",
    "title": "Appendix S — chapter5.pipeline.01_2_stanwifi-processing",
    "section": "",
    "text": "chapter5.pipeline.01_2_stanwifi-processing\nData preprocessing script for StanWiFi dataset.\nProcesses the raw data by processing the windows generated by the author’s scripts using DBSCAN for outlier detection and 2-level DWT for threshold based filtering.\nExample:\n$ python 01_2_stanwifi-processing.py \n    --input_data_path &lt;PATH_OF_RAW_DATA&gt; \n    --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>S</span>  <span class='chapter-title'>chapter5.pipeline.01_2_stanwifi-processing</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.pipeline.01_3_multienvironment-processing.html",
    "href": "reference/chapter5.pipeline.01_3_multienvironment-processing.html",
    "title": "Appendix T — chapter5.pipeline.01_3_multienvironment-processing",
    "section": "",
    "text": "chapter5.pipeline.01_3_multienvironment-processing\nData preprocessing script for Multi-environment dataset.\nProcesses the raw data by aranging samples in windows and processing them using DBSCAN for outlier detection and 2-level DWT for threshold based filtering\nExample:\n$ python 01_3_multienvironment-processing.py \n    --input_data_path &lt;PATH_OF_RAW_DATA&gt; \n    --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>T</span>  <span class='chapter-title'>chapter5.pipeline.01_3_multienvironment-processing</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.pipeline.01_4_lodo-dataset-processing.html",
    "href": "reference/chapter5.pipeline.01_4_lodo-dataset-processing.html",
    "title": "Appendix U — chapter5.pipeline.01_4_lodo-dataset-processing",
    "section": "",
    "text": "chapter5.pipeline.01_4_lodo-dataset-processing\nData preprocessing script for LODO dataset.\nProcesses the raw data by aranging samples in windows and process them using DBSCAN for outlier detection and 2-level DWT for threshold based filtering.\nExample:\n$ python 01_4_lodo-dataset-processing.py \n    --input_data_path &lt;PATH_OF_RAW_DATA&gt; \n    --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;\n    --window_size &lt;WINDOW_SIZE&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>U</span>  <span class='chapter-title'>chapter5.pipeline.01_4_lodo-dataset-processing</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.pipeline.02_hyperparameter-optimization.html",
    "href": "reference/chapter5.pipeline.02_hyperparameter-optimization.html",
    "title": "Appendix V — chapter5.pipeline.02_hyperparameter-optimization",
    "section": "",
    "text": "chapter5.pipeline.02_hyperparameter-optimization\nHyperparameters Grid Search script.\nPerforms an hyperparameter Grid Search on the specified model. The selected hyperparameters for the search can be found in tuning_configuration.py.\nExample:\n$ python 02_hyperparameter-optimization.py \n    --data_dir &lt;PATH_OF_DATA&gt; \n    --model &lt;MLP,CNN&gt;\n    --phase &lt;initial,extra-layers&gt;\n    --batch_size &lt;BATCH_SIZE&gt;\n    --epochs &lt;EPOCHS&gt;\n    --executions &lt;EXECUTIONS&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>V</span>  <span class='chapter-title'>chapter5.pipeline.02_hyperparameter-optimization</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.pipeline.03_1_multiple-evaluations.html",
    "href": "reference/chapter5.pipeline.03_1_multiple-evaluations.html",
    "title": "Appendix W — chapter5.pipeline.03_1_multiple-evaluations",
    "section": "",
    "text": "chapter5.pipeline.03_1_multiple-evaluations\nMultiple evaluation script\nPerforms a cross-validation and an evaluation with different subsets collected at different time frames.\nExample:\n$ python 03_1_multiple_evaluations.py \n    --data_dir &lt;PATH_OF_DATA&gt; \n    --reports_dir &lt;PATH_TO_STORE_REPORTS&gt;\n    --model &lt;MLP,CNN&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>W</span>  <span class='chapter-title'>chapter5.pipeline.03_1_multiple-evaluations</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.pipeline.03_2_cross-validation.html",
    "href": "reference/chapter5.pipeline.03_2_cross-validation.html",
    "title": "Appendix X — chapter5.pipeline.03_2_cross-validation",
    "section": "",
    "text": "chapter5.pipeline.03_2_cross-validation\nCross-validation script\nPerforms a cross-validation on the selected dataset.\nExample:\n$ python 03_2_cross-validation.py \n    --data_dir &lt;PATH_OF_DATA&gt; \n    --reports_dir &lt;PATH_TO_STORE_REPORTS&gt;\n    --dataset &lt;stanwifi,multienvironment&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>X</span>  <span class='chapter-title'>chapter5.pipeline.03_2_cross-validation</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.pipeline.03_3_lodo.html",
    "href": "reference/chapter5.pipeline.03_3_lodo.html",
    "title": "Appendix Y — chapter5.pipeline.03_3_lodo",
    "section": "",
    "text": "chapter5.pipeline.03_3_lodo\nLeaving-One-Day-Out validation script\nPerforms a Leaving-One-Day-Out evaluation on the LODO dataset.\nExample:\n$ python 03_3_lodo.py \n    --data_dir &lt;PATH_OF_DATA&gt; \n    --reports_dir &lt;PATH_TO_STORE_REPORTS&gt;",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>Y</span>  <span class='chapter-title'>chapter5.pipeline.03_3_lodo</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.analysis.reports.html",
    "href": "reference/chapter5.analysis.reports.html",
    "title": "Appendix Z — chapter5.analysis.reports",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>Z</span>  <span class='chapter-title'>chapter5.analysis.reports</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.analysis.reports.html#functions",
    "href": "reference/chapter5.analysis.reports.html#functions",
    "title": "Appendix Z — chapter5.analysis.reports",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nextract_metrics\nExtracts metrics of interest from a classification report\n\n\nmetric_increment_summary\nCreates a summary with metrics increments/decrements in the provided reports to compare\n\n\nmetrics_summary\nCreates a summary of the classification reports generated by the evaluation of several models.\n\n\n\n\nextract_metrics\nchapter5.analysis.reports.extract_metrics(report)\nExtracts metrics of interest from a classification report\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreport\ndict | list[dict]\nClassification report or a list of reports.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nfloat\nAccuracy obtained by the model.\n\n\nfloat\nPrecision obtained by the model.\n\n\nfloat\nRecall obtained by the model.\n\n\nfloat\nF1-score obtained by the model.\n\n\n\n\n\n\nmetric_increment_summary\nchapter5.analysis.reports.metric_increment_summary(comparisons)\nCreates a summary with metrics increments/decrements in the provided reports to compare different approaches\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncomparisons\ndict\nDict where each entry indicates which reports to compare.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame containing the comparisons.\n\n\n\n\n\n\nmetrics_summary\nchapter5.analysis.reports.metrics_summary(reports, labels)\nCreates a summary of the classification reports generated by the evaluation of several models.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreports\nlist[dict]\nList of classification reports.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\npandas.DataFrame\nDataFrame summarizing the results.",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>Z</span>  <span class='chapter-title'>chapter5.analysis.reports</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.analysis.visualization.html",
    "href": "reference/chapter5.analysis.visualization.html",
    "title": "[Appendix — chapter5.analysis.visualization",
    "section": "",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>[</span>  <span class='chapter-title'>chapter5.analysis.visualization</span>"
    ]
  },
  {
    "objectID": "reference/chapter5.analysis.visualization.html#functions",
    "href": "reference/chapter5.analysis.visualization.html#functions",
    "title": "[Appendix — chapter5.analysis.visualization",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nplot_confusion_matrix\nPlots the confusion matrix resulting from the evaluation of a machine learning model.\n\n\n\n\nplot_confusion_matrix\nchapter5.analysis.visualization.plot_confusion_matrix(report, prediction_target, labels)\nPlots the confusion matrix resulting from the evaluation of a machine learning model.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreport\ndict\nDict containing a classification report from a model evaluation.\nrequired\n\n\nprediction_target\nstr\nString describing what is being classified.\nrequired\n\n\nlabels\nlist[str]\nList with the associated classes labels.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nplotly.Figure\nInteractive Plotly figure.",
    "crumbs": [
      "Appendices",
      "Reference",
      "<span class='chapter-number'>[</span>  <span class='chapter-title'>chapter5.analysis.visualization</span>"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Tools",
    "section": "",
    "text": "Software tools\nThe software tools employed in this thesis are the following:",
    "crumbs": [
      "Appendices",
      "Tools"
    ]
  },
  {
    "objectID": "tools.html#software-tools",
    "href": "tools.html#software-tools",
    "title": "Tools",
    "section": "",
    "text": "Android SDK 33: development of Background Sensors, WearOS Sensors (see Data collection libraries) and TUG Test Smartwatch APP (see HAR in mHealth: TUG test using smartphones and smartwatches).\nNode v16.13, JDK 11 and NativeScript CLI v8.2.3: development of NativeScrit WearOS Sensors, AwarNS Phone Sensors and Wear OS (see Data collection libraries), TUG Test Smartphone APP (see HAR in mHealth: TUG test using smartphones and smartwatches).\nEspressif IoT Development Framework v5.1: development of CSI data collection tools employed in Looking into the future: Wi-Fi CSI based HAR.\nPython 3.9.15: tasks related to ML, DL and data analysis. The following Python modules have been employed for such tasks:\n\n\n\n\nrequirements.txt\n\nalive_progress==3.0.1       # Animated progress bar employed for long-lasting processes\nh5py==3.6.0                 # Numpy dependency\nhampel==1.0.2               # Implementation of Hampel filter\nitables==2.0.0              # Interactive displaying of DataFrames\nkeras==2.10.0               # API for building ML and DL models\nkeras-tuner==1.4.6          # API for tuning (e.g., Grid Search) ML and DL models\nnumpy==1.23.2               # Manipulation of multidimensional arrays \npandas==1.5.2               # Manipulation of tabular data (i.e. DataFrames)\npingouin==0.5.3             # Statistical tests\nplotly==5.14.0              # Interactive plotting library\npython-dateutil==2.8.2      # Utils for manipulation of dates\nPyWavelets==1.4.1           # Implementation of Wavelet transforms\nscikit-learn==1.2.0         # ML and DL tools\nscipy==1.10.0               # Algorithms for scienific computing (e.g., statistics, signal filters, etc.)\ntensorflow==2.10.0          # Backend for running ML and DL models",
    "crumbs": [
      "Appendices",
      "Tools"
    ]
  },
  {
    "objectID": "tools.html#hardware-tools",
    "href": "tools.html#hardware-tools",
    "title": "Tools",
    "section": "Hardware tools",
    "text": "Hardware tools\nFollowing, the harware devices used during the development of this thesis are listed.\n\nSmart devices:\n\nTicWatch Pro 3 GPS (WH12018): IMU data collection for Smartphone and smartwatch HAR dataset and system evaluation in HAR in mHealth: TUG test using smartphones and smartwatches.\nXiaomi Poco X3 Pro (M2102J20SG): IMU data collection for Smartphone and smartwatch HAR dataset and system evaluation in HAR in mHealth: TUG test using smartphones and smartwatches.\nXiaomi Poco F2 Pro (M2004J11G): video recordings for Smartphone and smartwatch HAR dataset and Localized HAR based on Wi-Fi CSI, and system evaluation in HAR in mHealth: TUG test using smartphones and smartwatches.\n\nMicrocontrollers:\n\nESP32-S2 WROOM: CSI data collection for Localized HAR based on Wi-Fi CSI.\nESP32-S3 WROOM: CSI data collection for Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers.\n\nComputers:\n\nWindows 10 PC with i7-8700 CPU, NVIDIA GeForce GTX 750 GPU and 16 GB RAM: ML and DL models.\nMacBook Air M1 16GB RAM: development and analysis tasks.\n\nOther:\n\nTP-Link Archer C80 Router: CSI data collection for Localized HAR based on Wi-Fi CSI and Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers.",
    "crumbs": [
      "Appendices",
      "Tools"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Chapter 2\nDefines the functions employed in the Chapter 2: Materials & Methods.",
    "crumbs": [
      "Appendices",
      "Reference"
    ]
  },
  {
    "objectID": "reference/index.html#chapter-2",
    "href": "reference/index.html#chapter-2",
    "title": "Function reference",
    "section": "",
    "text": "chapter2.data_loading\nProvides functions to load the collected data and associated metadata files.\n\n\nchapter2.exploration\nProvides functions to compute statistics regarding subjects and collected data.\n\n\nchapter2.visualization\nProvides a function to plot the collected data.",
    "crumbs": [
      "Appendices",
      "Reference"
    ]
  },
  {
    "objectID": "reference/index.html#chapter-3",
    "href": "reference/index.html#chapter-3",
    "title": "Function reference",
    "section": "Chapter 3",
    "text": "Chapter 3\nDefines the functions employed in the Chapter 3: Multidimensional analysis of ML and DL on HAR.\n\nPipeline\nFunctions employed to carry out the experiments.\n\n\n\nchapter3.pipeline.01_data-processing\nData preprocessing script.\n\n\nchapter3.pipeline.02_hyperparameter-optimization\nHyperparameters Grid Search script.\n\n\nchapter3.pipeline.03_incremental-loso\nIncremental Leaving-One-Subject-Out script.\n\n\n\n\n\nAnalysis\nFunctions employed to analyse the results of the experiment.\n\n\n\nchapter3.analysis.data_loading\nProvides functions to load the obtained results.\n\n\nchapter3.analysis.model\nDefines a model to manage the reports generated by the machine and deep learning models.\n\n\nchapter3.analysis.statistical_tests\nProvides functions to compute statistical tests to determine the significance of the obtained results.\n\n\nchapter3.analysis.visualization\nProvides functions to visualize the obtained results.",
    "crumbs": [
      "Appendices",
      "Reference"
    ]
  },
  {
    "objectID": "reference/index.html#chapter-4",
    "href": "reference/index.html#chapter-4",
    "title": "Function reference",
    "section": "Chapter 4",
    "text": "Chapter 4\nDefines the functions employed in the Chapter 4 - HAR in mHealth: TUG test using smartphones and smartwatches.\n\nPipeline\nFunctions employed to carry out the experiments.\n\n\n\nchapter4.pipeline.01_relabel\nData relabelling script.\n\n\nchapter4.pipeline.02_splitting-evaluation\nSplitting approach evaluation script\n\n\n\n\n\nAnalysis\nFunctions employed to analyse the results of the experiments.\n\n\n\nchapter4.analysis.data_loading\nProvides functions to load the obtained results.\n\n\nchapter4.analysis.statistical_tests\nProvides functions to compute statistical tests to determine the significance of the obtained results.\n\n\nchapter4.analysis.tug_results_processing\nProvides functions to process the raw results of the TUG test obtained by the developed system and the reference method.\n\n\nchapter4.analysis.visualization\nProvides functions to visualize the obtained results.\n\n\nchapter4.analysis.battery\nProvides functions to process the battery consumption results",
    "crumbs": [
      "Appendices",
      "Reference"
    ]
  },
  {
    "objectID": "reference/index.html#chapter-5",
    "href": "reference/index.html#chapter-5",
    "title": "Function reference",
    "section": "Chapter 5",
    "text": "Chapter 5\nDefines the functions employed in the Chapter 5 - Looking into the future: Wi-Fi CSI based HAR.\n\nPipeline\nFunctions employed to carry out the experiments.\n\n\n\nchapter5.pipeline.01_1_preliminar-dataset-processing\nData preprocessing script for preliminar dataset.\n\n\nchapter5.pipeline.01_2_stanwifi-processing\nData preprocessing script for StanWiFi dataset.\n\n\nchapter5.pipeline.01_3_multienvironment-processing\nData preprocessing script for Multi-environment dataset.\n\n\nchapter5.pipeline.01_4_lodo-dataset-processing\nData preprocessing script for LODO dataset.\n\n\nchapter5.pipeline.02_hyperparameter-optimization\nHyperparameters Grid Search script.\n\n\nchapter5.pipeline.03_1_multiple-evaluations\nMultiple evaluation script\n\n\nchapter5.pipeline.03_2_cross-validation\nCross-validation script\n\n\nchapter5.pipeline.03_3_lodo\nLeaving-One-Day-Out validation script\n\n\n\n\n\nAnalysis\nFunctions employed to analyse the results of the experiments.\n\n\n\nchapter5.analysis.reports\nProvides functions to process the reports generated in the evaluation of classification models.\n\n\nchapter5.analysis.visualization\nProvides functions to visualize the obtained results.",
    "crumbs": [
      "Appendices",
      "Reference"
    ]
  },
  {
    "objectID": "02_materials-methods.html",
    "href": "02_materials-methods.html",
    "title": "Materials & Methods",
    "section": "",
    "text": "This section describes the materials and methods developed (e.g., data collection tools), generated (e.g., dataset) and used (e.g., ML and DL models, statistical tools, etc.) throughout this thesis. More concretely, this section encompasses the follwing subsections:\n\n\n\n\n\n\n\n\nData collection libraries\n\n\nDescribes the reliable data collection libraries for Android smartphones and Wear OS smartwatches.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmartphone and smartwatch HAR dataset\n\n\nDescribes the smartphone and smartwatch inertial sensors collected database from heterogeneous subjects for HAR applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommon methods\n\n\nDescribes the ML and DL architectures employed for HAR classification and the statistical tools used to determine the significance of the presented results.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe contents on this section correspond with the Chapter 2 of the dissertation document.",
    "crumbs": [
      "Materials & Methods"
    ]
  },
  {
    "objectID": "03_analysis-har.html",
    "href": "03_analysis-har.html",
    "title": "Multidimensional Analysis of ML and DL on HAR",
    "section": "",
    "text": "Methodology\nThe collected dataset in Smartphone and smartwatch HAR dataset and the ML and DL techniques described in ML and DL have been used to carry out the specified analysis. The following sections describe specific procedures for data preparation, the process to optimize the hyperparameters of the selected models, and the procedure to evaluate the impact of the amount of training data.",
    "crumbs": [
      "Multidimensional Analysis"
    ]
  },
  {
    "objectID": "03_analysis-har.html#methodology",
    "href": "03_analysis-har.html#methodology",
    "title": "Multidimensional Analysis of ML and DL on HAR",
    "section": "",
    "text": "Data preparation\n\nMin-Max scaling\nA Min-Max scaling was applied to the smartphone and smartwatch data to rescale the data into the \\([-1, 1]\\) range. This rescaling is defined as: \\[\n    v' = \\frac{v - min}{max - min}*(new\\_max-new\\_min) + new\\_min\n\\]\n\n\nData fusion\nA fused dataset was generated to evaluate the impact of the training set size when using smartphone and smartwatch data together. Due to variable sampling rates (\\(102\\) Hz and \\(104\\) Hz for smartphone and smartwatch), the following procedure (depicted in Figure 1) was employed to fusion both data sources:\n\nSince the data collection is started and finalized in each device independently, exceeding samples at the beginning and end of the execution are removed (red dots in Figure 1, step 1).\nSamples are grouped in batches of \\(1\\) second.\nIn each batch, the \\(i^{th}\\) smartphone sample is matched with the \\(i^{th}\\) smartwatch sample. A sample is discarded if it cannot be matched with another sample (red dots in Figure 1, step 3).\n\n\n\n\n\n\n\nFigure 1: Data fusion procedure\n\n\n\n\n\nData windowing\nThe smartphone, smartwatch and fused dataset were split using the sliding window technique. A window size of \\(50\\) samples was used, corresponding to approximately \\(0.5\\) seconds with a \\(100\\) Hz sampling rate, and an overlap of \\(50\\) %. These values have been chosen since they are proven to obtain successful results in HAR (Sansano et al. 2020; Jaén-Vargas et al. 2022). Table 1 contains the resulting windowns in each datasets, ready to be used by the CNN, LSTM and CNN-LSTM models.\n\n\n\nTable 1: Number of resulting data windows in each dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nSEATED\nSTANDING_UP\nWALKING\nTURNING\nSITTING_DOWN\nTotal\n\n\n\n\nSmartphone\n1033\n1081\n4606\n2087\n1235\n10042\n\n\nSmartwatch\n998\n1105\n4691\n2123\n1253\n10170\n\n\nFused\n871\n1083\n4458\n1887\n1066\n9365\n\n\n\n\n\n\n\n\nFeature extraction\nA feature extraction process is executed to use the collected data in MLP models. The extracted features can be classified in mathematical/statistical or angular features:\n\nMathematical/Statistical features are extracted by applying mathematical/statistical functions to the collected data. Although simple, these features are beneficial for discriminating between static and dynamic activities, postures, etc. (Figo et al. 2010). The extracted features are the mean, median, maximum, minimum, sd, range and rms. Each feature is extracted from each sensor (i.e., accelerometer and gyroscope) and axis (i.e., x, y and z).\nAngular features are useful to determine orientation changes of the devices and in combination with the mathematical/statistical features improve the classification accuracy of the models (Coskun, Incel, and Ozgovde 2015). The angular features extracted are the Pitch, the Roll and the Rotational angle for each axis.\n\nFor the smartphone and smartwatch datasets, a total of \\(47\\) (i.e., \\(7*6+1+1+3\\)) features were extracted from each window in each dataset. In the case of the fused dataset, \\(94\\) (i.e., \\(47*2\\)) features were extracted from each window.\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute this process is 01_data-processing.py.\n\n\nCode\n\"\"\"Data preprocessing script.\n\nProcesses the raw data by: applying min-max scaling, fusing smartphone and smartwatch data, arange samples\nin windows and perform feature extraction. The script stores the raw windows, windows with features extracted\nand groundtruth for smartphone, smartwatch and fused data.\n\n**Example**:\n\n    $ python 01_data-processing.py --input_data_path &lt;PATH_OF_RAW_DATA&gt; --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;\n\"\"\"\n\n\nimport argparse\nimport os\nimport numpy as np\nimport sys\n\nsys.path.append(\"../../..\")\n\nfrom alive_progress import alive_bar\nfrom libs.chapter2.data_loading import load_data\nfrom libs.chapter3.pipeline.feature_extraction import apply_feature_extraction\nfrom libs.chapter3.pipeline.data_fusion import fuse_data\nfrom libs.chapter3.pipeline.raw_data_processing import scale, windows, compute_best_class, count_data\n\n\nWINDOW_SIZE = 50\nSTEP_SIZE = WINDOW_SIZE // 2\n\n\ndef clean_raw_data(raw_data):\n    clean_data = {}\n\n    with alive_bar(len(raw_data), title=f'Data cleanning', force_tty=True, monitor='[{percent:.0%}]') as progress_bar:\n        for desc, data in raw_data.items():\n            _, source = desc.rsplit('_', 1)\n            clean_data[desc] = scale(data, source)\n            progress_bar()\n            \n    return clean_data\n\n\ndef fuse_sources(clean_data):\n    fused_clean_data = {}\n    for execution, data in clean_data.items():\n        *exec_id, device = execution.split('_')\n        exec_id = '_'.join(exec_id)\n\n        if not exec_id in fused_clean_data:\n            fused_clean_data[exec_id] = {}\n        fused_clean_data[exec_id][device] = data\n\n    for execution, data in fused_clean_data.items():\n        fused = fuse_data(data['sp'], data['sw'])\n        clean_data[f'{execution}_fused'] = fused\n    \n    return clean_data\n\n\ndef get_windowed_data(clean_data, window_size, step_size): \n    windowed_data = {}\n    gt = {}\n    \n    with alive_bar(len(clean_data), title=f'Data windowing', force_tty=True, monitor='[{percent:.0%}]') as progress_bar:\n        for desc, data in clean_data.items():\n            desc_components = desc.split('_')\n            subject_sensor_desc = f'{desc_components[0]}_{desc_components[2]}'\n\n            windowed_df = windows(data, window_size, step_size)\n            desc_instances = []\n            desc_gt = []\n\n            columns = ['x_acc', 'y_acc', 'z_acc', 'x_gyro', 'y_gyro', 'z_gyro'] if desc_components[2] != 'fused' else ['x_acc_sp', 'y_acc_sp', 'z_acc_sp', 'x_gyro_sp', 'y_gyro_sp', 'z_gyro_sp',\n                         'x_acc_sw', 'y_acc_sw', 'z_acc_sw', 'x_gyro_sw', 'y_gyro_sw', 'z_gyro_sw']\n\n            for i in range(0, data.shape[0], step_size):\n                window = windowed_df.loc[\"{0}:{1}\".format(i, i+window_size)]\n                values = window[columns].transpose()\n                groundtruth = compute_best_class(window)\n                if (values.shape[1] != window_size):\n                    break\n                desc_instances.append(values.values.tolist())\n                desc_gt.append(groundtruth.values[0])\n\n            if subject_sensor_desc in windowed_data:\n                windowed_data[subject_sensor_desc] += desc_instances\n                gt[subject_sensor_desc] += desc_gt\n            else:\n                windowed_data[subject_sensor_desc] = desc_instances\n                gt[subject_sensor_desc] = desc_gt\n                \n            progress_bar()\n            \n    return windowed_data, gt\n\n\ndef extract_features(windowed_data):\n    featured_data = {}\n    with alive_bar(len(windowed_data.items()), title=f'Feature extraction', force_tty=True, monitor='[{percent:.0%}]') as progress_bar:\n        for subject, windows in windowed_data.items():\n            data_type = subject.split('_')[-1]\n            features = []\n            for window in windows:\n                window = np.array(window)\n                if data_type != 'fused':\n                    features.append(apply_feature_extraction(window))\n                else:\n                    part_a = window[:6,:]\n                    part_b = window[6:,:]\n\n                    features_a = apply_feature_extraction(part_a)\n                    features_b = apply_feature_extraction(part_b)\n                    features.append(np.concatenate((features_a, features_b)))\n            \n            featured_data[subject] = np.array(features)\n            progress_bar()\n    return featured_data\n        \n        \ndef store_windowed_data(windowed_data, features, ground_truth, path):\n    def store_as_npy(path, data):\n        with open(path, 'wb') as f:\n            np.save(f, np.array(data)) \n            \n    with alive_bar(len(windowed_data), title=f'Storing windowed data in {path}', force_tty=True, monitor='[{percent:.0%}]') as progress_bar:\n        for desc, data in windowed_data.items():\n            subject = desc.split('_')[0]\n            subject_path = os.path.join(path, subject)\n            if not os.path.exists(subject_path):\n                os.makedirs(subject_path)\n\n            store_as_npy(os.path.join(subject_path, f'{desc}.npy'), data)\n            store_as_npy(os.path.join(subject_path, f'{desc}_features.npy'), features[desc])\n            store_as_npy(os.path.join(subject_path, f'{desc}_gt.npy'), ground_truth[desc])\n            progress_bar()  \n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_data_path', help='Path of input data', type=str, required=True)\n    parser.add_argument('--windowed_data_path', help='Path to store windowed data', type=str, required=True)\n    args = parser.parse_args()\n    \n    raw_data = load_data(args.input_data_path)\n    \n    clean_data = clean_raw_data(raw_data)\n    clean_fused_data = fuse_sources(clean_data)\n\n    print('\\nClean data:')\n    print(count_data(clean_fused_data), '\\n')\n    print('\\n')\n\n    \n    windowed_data, gt = get_windowed_data(clean_fused_data, WINDOW_SIZE, STEP_SIZE)\n    print('\\nWindowed data:')\n    print(count_data(gt), '\\n')\n        \n    features = extract_features(windowed_data)\n    store_windowed_data(windowed_data, features, gt, args.windowed_data_path)\n\n\n\n\n\n\n\nModels’ hyperparameters optimization\nBefore training the models, their hyperparameters have to be selected, i.e., the models have to be tuned. The selected options for the hyperparameters, which have been chosen based on other works using these models, are the following:\n\nNumber of units (all models): \\(128\\), \\(256\\) and \\(512\\).\nNumber of filters (CNN and CNN-LSTM): \\(32\\), \\(64\\) and \\(128\\).\nLSTM cells (LSTM and CNN-LSTM): \\(32\\), \\(64\\), \\(128\\).\n\nOther hyperparamters (e.g., learning rate, filter sizes, number of layers) were selected based on previous experience.\nThe best hyperparameters were obtained using the Grid Search technique, where every possible combination of hyperparameters is evaluated. The process was configured to train and evaluate each combination five times using the Adam optimizer during \\(50\\) epochs with a batch size of \\(64\\) windows. To reduce the computational cost of the optimization, the process was carried out in two phases: 1) optimization of layers and learning hyperparameters and 2) optimization of the number of layers. Table 2 shows the best combination of hyperparameters per model and dataset.\n\n\n\n\n\nTable 2: Best combination of parameters for each model and dataset.\n\n\n\n\n\n    \n      \n      \n      Value\n    \n    \n      Model\n      Hyperparameter\n      \n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute this process is 02_hyperparameter-optimization.py.\n\n\nCode\n\"\"\"Hyperparameters Grid Search script.\n\nPerforms an hyperparameter Grid Search on the specified model. The selected hyperparameters for the search\ncan be found in `tuning_configuration.py`.\n\n**Example**:\n\n    $ python 02_hyperparameter-optimization.py \n        --data_dir &lt;PATH_OF_DATA&gt; \n        --model &lt;MLP,CNN,LSTM,CNN-LSTM&gt;\n        --phase &lt;initial,extra-layers&gt;\n        --batch_size &lt;BATCH_SIZE&gt;\n        --epochs &lt;EPOCHS&gt;\n        --executions &lt;EXECUTIONS&gt;\n\"\"\"\n\n\nimport argparse\nimport os\n\nimport sys\nsys.path.append(\"../../..\")\n\nfrom libs.chapter3.pipeline.data_reshapers import get_reshaper\nfrom libs.chapter3.pipeline.hyperparameters_tuning import get_model_builder, create_tuner, tune, get_tuning_summary\nfrom libs.chapter3.pipeline.tuning_configuration import get_tuning_configuration\nfrom libs.common.data_loading import load_data\nfrom libs.common.data_grouping import merge_subjects_datasets\nfrom libs.common.utils import save_json, set_seed\n\n\nTUNING_DIR = 'GRID_SEARCH_{0}'\nTUNING_SUMMARY_FILE = 'summary.json'\n\nACTIVITIES = {\"SEATED\": 0, \"STANDING_UP\": 1, \"WALKING\": 2, \"TURNING\": 3, \"SITTING_DOWN\": 4}\n\nBATCH_SIZE = 64\nEPOCHS = 50\nN_EXECUTIONS = 5\n\n\ndef tune_model(data, model_type, batch_size, epochs, n_executions, phase):\n    set_seed()\n    model_builder = get_model_builder(model_type)\n    reshaper = get_reshaper(model_type)\n    optimizing_layers = phase == 'extra-layers' \n\n    for source, (x, y) in data.items():\n        x, y = merge_subjects_datasets(x, y, list(x.keys()))\n        if reshaper is not None:\n            x = reshaper(x)\n        \n        features_dimension = x.shape[-1] if model_type in ['lstm', 'cnn-lstm'] else x.shape[1]\n        tuning_configuration = get_tuning_configuration(model_type, source if optimizing_layers else None)\n        tuning_configuration['features_dimension'] = features_dimension\n        tuning_project = f'{model_type}_{source}{\"_layers\" if optimizing_layers else \"\"}'\n        print(f'Tuning {model_type} model with {source} data')\n        tuner = create_tuner(\n            model_builder, \n            n_executions, \n            tuning_configuration, \n            TUNING_DIR.format(phase), \n            tuning_project\n        )\n\n        tuner = tune(tuner, x, y, epochs, batch_size)\n        save_tuning_summary(tuner, os.path.join(TUNING_DIR, tuning_project))\n\n\ndef save_tuning_summary(tuner, tuning_dir):\n    save_json(get_tuning_summary(tuner), tuning_dir, TUNING_SUMMARY_FILE)\n\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', help='data directory', type=str, required=True)\n    parser.add_argument('--model', help='optimize hyperparameters for selected model', type=str, choices=['mlp', 'lstm', 'cnn', 'cnn-lstm'])\n    parser.add_argument('--phase', help='tuning phase: &lt;initial&gt; to tune layer hyperparameters and &lt;extra-layers&gt; to tune number of layers' , type=str, choices=['initial', 'extra-layers'])\n    parser.add_argument('--batch_size', help='training batch size', type=int, default=BATCH_SIZE)\n    parser.add_argument('--epochs', help='training epochs', type=int, default=EPOCHS)\n    parser.add_argument('--executions', help='executions per trial', type=int, default=N_EXECUTIONS)\n    args = parser.parse_args()\n\n    use_raw_data = args.model != 'mlp'\n\n    x_sp, y_sp = load_data(args.data_dir, 'sp', use_raw_data, ACTIVITIES)\n    x_sw, y_sw = load_data(args.data_dir, 'sw', use_raw_data, ACTIVITIES)\n    x_fused, y_fused = load_data(args.data_dir, 'fused', use_raw_data, ACTIVITIES)\n    data = {\n        'sp': (x_sp, y_sp),\n        'sw': (x_sw, y_sw),\n\t    'fused': (x_fused, y_fused)\n    }\n    tune_model(data, args.model, args.batch_size, args.epochs, args.executions, args.phase)  \n    \n\n\n\n\n\n\nEvaluation procedure\n\nIncremental Leaving-One-Subject-Out (ILOSO)\nAs pointed out by Gholamiangonabadi, Kiselov, and Grolinger (2020), the right approach to evaluate HAR systems is the LOSO (cross-subject evaluation) strategy. To study the effect of the amount of training data on the performance of models, we employ an ILOSO.\nIn the ILOSO strategy, given \\(S\\) subjects, each \\(s\\) subject is individually considered as a test subject. Then, \\(n\\) subjects are randomly selected as training subjects from \\(S\\setminus s\\), where \\(n \\in [1,\\ldots,|S|-1]\\). The data from training subjects are employed to train a model and the data from the test subject is used to evaluate it. This procedure is repeated \\(R\\) times for every value of \\(n\\) with different random initialization of the model.\nThe described algorithm trains and evaluates \\(|S|*(|S|-1)*R\\) models. Since we use \\(R=10\\) and there are three datasets and four types of models, a total of \\(60720\\) combinations models are trained (\\(23*22*10*3*4\\)).\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute this process is 03_incremental-loso.py.\n\n\nCode\n\"\"\"Incremental Leaving-One-Subject-Out script.\n\nPerforms the ILOSO evaluation.\n\n**Example**:\n\n    $ python 03_incremental-loso.py \n        --data_dir &lt;PATH_OF_DATA&gt; \n        --reports_dir &lt;PATH_TO_STORE_RECORDS&gt;\n        --model &lt;MLP,CNN,LSTM,CNN-LSTM&gt;\n        --subject &lt;EVALUATION_SUBJECT&gt;\n        --batch_size &lt;BATCH_SIZE&gt;\n        --epochs &lt;EPOCHS&gt;\n        --splits &lt;SPLITS&gt;\n\"\"\"\n\nimport os\nimport traceback\nimport argparse\nimport gc\n\nimport sys\nsys.path.append(\"../../..\")\n\nfrom alive_progress import alive_bar\n\nfrom libs.chapter3.pipeline.data_grouping import generate_lno_group\nfrom libs.chapter3.pipeline.data_reshapers import get_reshaper\nfrom libs.chapter3.pipeline.training import create_trainer\nfrom libs.chapter3.pipeline.training_report import report_writer\nfrom libs.common.data_loading import load_data\nfrom libs.common.data_grouping import generate_training_and_test_sets\nfrom libs.common.ml import generate_report\nfrom libs.common.utils import set_seed\n\n\nACTIVITIES = {\"SEATED\": 0, \"STANDING_UP\": 1, \"WALKING\": 2, \"TURNING\": 3, \"SITTING_DOWN\": 4}\n\nBATCH_SIZE = 64\nEPOCHS = 50\nN_SPLITS = 10\n\n\ndef train_models(data, subjects, test_subjects, model_type, batch_size, epochs, n_splits, reports_dir, testing_mode):\n    set_seed()\n    writers = {}\n    reshaper = get_reshaper(model_type)\n\n    try:\n        for test_subject in test_subjects:\n            with alive_bar(len(subjects) - 1, dual_line=True, title=f'Evaluating models with {test_subject}', force_tty=True) as progress_bar:\n                for n in range(1, len(subjects)):\n                    for i in range(n_splits):\n                        train_subjects = generate_lno_group(subjects, n, test_subject)\n                        \n                        progress_bar.text = f'Training {i+1}th model with {n} subjects'\n                        for source, (x, y) in data.items():\n                            x_train, y_train, x_test, y_test = generate_training_and_test_sets(x, y, train_subjects, [test_subject])\n                            if reshaper is not None:\n                                x_train = reshaper(x_train)\n                                x_test = reshaper(x_test)\n\n                            trainer = create_trainer(model_type, source, batch_size, epochs)\n                            trainer(x_train, y_train, verbose=0)\n                            model, training_time = trainer(x_train, y_train, verbose=0)\n                            y_pred = model.predict(x_test, verbose=0)\n\n                            report = generate_report(y_test, y_pred, ACTIVITIES.keys())\n                            report['training time'] = training_time\n\n                            if not testing_mode:\n                                if not source in writers:\n                                    writers[source] = report_writer(os.path.join(reports_dir, f'{0}_models.csv'.format(f'{model_type}_{source}')))\n                                writers[source](test_subject, n, i+1, report)\n                            \n                            del model\n                            del x\n                            del y\n                            del x_train\n                            del y_train\n                            del x_test\n                            del y_test\n                    gc.collect()\n                    progress_bar()\n    except:\n        with open('failures.txt', 'a') as file:\n            file.write(f'Exception in test_subject:{test_subject}, n_training_subjects:{n}, model:{model_type}, iteration: {i}\\n')\n            traceback.print_exc(file=file)\n    \n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', help='data directory', type=str, required=True)\n    parser.add_argument('--reports_dir', help='directory to store reports', type=str, required=True)\n    parser.add_argument('--model', help='model to use for evaluation', type=str, choices=['mlp', 'lstm', 'cnn', 'cnn-lstm'])\n    parser.add_argument('--subject', help='evaluate only with specified subject', type=int)\n    parser.add_argument('--batch_size', help='training batch size', type=int, default=BATCH_SIZE)\n    parser.add_argument('--epochs', help='training epochs', type=int, default=EPOCHS)\n    parser.add_argument('--splits', help='models trained for each case', type=int, default=N_SPLITS)\n    parser.add_argument('--testing_script', help='Testing the script. Results not stored', action='store_true')\n    args = parser.parse_args()\n\n    use_raw_data = args.model != 'mlp'\n\n    x_sp, y_sp = load_data(args.data_dir, 'sp', use_raw_data, ACTIVITIES)\n    x_sw, y_sw = load_data(args.data_dir, 'sw', use_raw_data, ACTIVITIES)\n    x_fused, y_fused = load_data(args.data_dir, 'fused', use_raw_data, ACTIVITIES)\n    data = {\n        'sp': (x_sp, y_sp),\n        'sw': (x_sw, y_sw),\n\t    'fused': (x_fused, y_fused)\n    }\n    subjects = list(x_sp.keys())\n    test_subjects = [subjects[args.subject - 1]] if args.subject else subjects\n    train_models(data, subjects, test_subjects, args.model, args.batch_size, args.epochs, args.splits, args.reports_dir, args.testing_script)  \n    \n\n\n\n\n\n\nEvaluation metrics\nWe analyse the effect of the increasing size of training data by observing the overall and activity-wise classification performance – measured using the accuracy and F1-score metrics (defined in Evaluation metrics) – of all model combinations grouped by the amount of training data (\\(n\\)), dataset and model type.\nThen, based on the accuracy and F1-score metrics, descriptive statistics are computed for each value of \\(n\\) (i.e., the amount of training subjects), type of models and datasets to determine the effect of the increase in the training data. Finally, MWU are executed to find significant differences between the amount of data employed and the model performance, and KWH are executed to determine the best performant models and datasets (see Statistical tools).",
    "crumbs": [
      "Multidimensional Analysis"
    ]
  },
  {
    "objectID": "03_analysis-har.html#results",
    "href": "03_analysis-har.html#results",
    "title": "Multidimensional Analysis of ML and DL on HAR",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\n\n\nImpact of the amount of training data\n\n\nVisual and statistical analysis of how the amound of data impacts on the performance of models and data sources.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical comparison of data sources performance\n\n\nDetermine how the different data sources perform in the selected models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical comparison of models performance\n\n\nDetermine how the selected models perform with the different data sources.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCoskun, Doruk, Ozlem Durmaz Incel, and Atay Ozgovde. 2015. “Phone Position/Placement Detection Using Accelerometer: Impact on Activity Recognition.” In 2015 IEEE Tenth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP), 1–6. https://doi.org/10.1109/ISSNIP.2015.7106915.\n\n\nFigo, Davide, Pedro C Diniz, Diogo R Ferreira, and Joao MP Cardoso. 2010. “Preprocessing Techniques for Context Recognition from Accelerometer Data.” Personal and Ubiquitous Computing 14: 645–62. https://doi.org/10.1007/s00779-010-0293-9.\n\n\nGholamiangonabadi, Davoud, Nikita Kiselov, and Katarina Grolinger. 2020. “Deep Neural Networks for Human Activity Recognition with Wearable Sensors: Leave-One-Subject-Out Cross-Validation for Model Selection.” IEEE Access 8: 133982–94. https://doi.org/10.1109/ACCESS.2020.3010715.\n\n\nJaén-Vargas, Milagros et al. 2022. “Effects of Sliding Window Variation in the Performance of Acceleration-Based Human Activity Recognition Using Deep Learning Models.” PeerJ Computer Science 8: e1052. https://doi.org/10.7717/peerj-cs.1052.\n\n\nMatey-Sanz, Miguel, Joaquín Torres-Sospedra, Alberto González-Pérez, Sven Casteleyn, and Carlos Granell. 2024. “Analysis and Impact of Training Set Size in Cross-Subject Human Activity Recognition.” In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, 391–405. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-49018-7_28.\n\n\nSansano, Emilio et al. 2020. “A Study of Deep Neural Networks for Human Activity Recognition.” Comput. Intell. 36 (3): 1113–39. https://doi.org/10.1111/coin.12318.",
    "crumbs": [
      "Multidimensional Analysis"
    ]
  },
  {
    "objectID": "04_tug.html",
    "href": "04_tug.html",
    "title": "HAR in mHealth: TUG Test Using Smartphones and Smartwatches",
    "section": "",
    "text": "TUG test mHealth system\nThe TUG test’s main output is its total duration. Nevertheless, this measurement can be decomposed into several components (i.e., subphases) that measure different aspects of the subject’s mobility. In the literature, there seem to be different approaches to this decomposition: some authors break down the subphases of the test into stand up (from the chair), walk \\(3\\) meters, turn, walk back to the chair, another turn and sit down. Others consider the last two subphases into a single one combining the last turn and the sitting down activities. As will be shown in Splitting approach analysis, the best approach is the first one, where these subphases can be named as follows: standing_up, first_walk, first_turn, second_walk, second_turn and sitting_down.\nThese subphases can be associated with five differentiated human activities, namely SEATED, STANDING_UP, WALKING, TURNING and SITTING_DOWN. Thus, recognizing these human activities enables the automatic assessment of the TUG test. More specifically, the duration of each subphase and the total duration of the test can be obtained by calculating the boundaries between the associated recognized human activities.\nThis is the aim of the developed mHealth system depicted in Figure 2. The developed system has two operating modes:\nThe developed system collects samples from the smartwatch (C1) or the smartphone (C2). These samples are used to determine which activity a subject performs while executing the TUG test, and to measure the duration of each subphase and the total duration of the test upon completion. Sample collection is manually initiated in both configurations, but is automatically stopped during the TUG test assessment when its completion is detected. Next, the smartwatch and smartphone applications of the TUG test system are described.",
    "crumbs": [
      "HAR in mHealth: TUG test"
    ]
  },
  {
    "objectID": "04_tug.html#tug-test-mhealth-system",
    "href": "04_tug.html#tug-test-mhealth-system",
    "title": "HAR in mHealth: TUG Test Using Smartphones and Smartwatches",
    "section": "",
    "text": "C1: a WearOS smartwatch is used as a sensing device (i.e., sensor sampling) and a paired Android smartphone acts as a computing device (e.g., data processing, activity inference and results computation).\nC2: an Android smartphone acts as a sensing and computing device.\n\n\n\n\n\n\n\nFigure 2: Architecture of the developed mHealth system.\n\n\n\n\n\nSmartwatch application\nThe smartwatch application is a native Android application developed using the WearOS Sensors library (described in Data collection libraries) that runs on a WearOS-based smartwatch whose main purpose is to act as a sensing device. The smartwatch must be linked to a smartphone with the application described in the next section installed, but the phone does not need to be carried on since it is used as a computing device. This is to increase the battery life of the smartwatch.\nFigure 3 shows the application interface that allows the user to start the collection of accelerometer and gyroscope samples at \\(100\\)Hz for data collection – Figure 3 (a) – or inference – Figure 3 (b) –. After pressing the button to start the system, the smartwatch emits a vibration to indicate that the system is ready and that the collection has started. Then, collected samples are batched in groups of \\(50\\) and wirelessly sent to the paired smartphone using Bluetooth communication. When the end of the test is detected and its results are computed on the smartphone side, the smartwatch stops the data collection process, receives the results and vibrates to signal the end of the test, displaying the results (only the total duration, due to screen size constraints) on the screen – Figure 3 (c) –.\n\n\n\n\n\n\n\n\n\n\n\n(a) Data collection UI\n\n\n\n\n\n\n\n\n\n\n\n(b) TUG test assessment UI\n\n\n\n\n\n\n\n\n\n\n\n(c) TUG test results UI\n\n\n\n\n\n\n\nFigure 3: Screenshots of the smartwatch application UI.\n\n\n\n\n\n\n\n\n\nAvailability\n\n\n\nThe TUG test smartwatch application is available in GitHub and Zenodo (Matey-Sanz and González-Pérez 2022b).\n\n\n\n\nSmartphone application\nThe smartphone application is a NativeScript application that runs on Android smartphones. It has been developed using the Phone Sensors, Wear OS and ML Kit packages (see Data collection libraries and ML and DL) of the AwarNS Framework (González-Pérez et al. 2023). With the application set up and running, the smartphone can act just as a computing device (C1), with a paired smartwatch as a sensing device, or as a sensing and computing device (C2).\nOn the sensing facet, the application provides an interface to start the data collection process (Figure 4 (a)). After pressing the start button, the smartphone emits a sound to notify the user that the system is ready and the data collection has started. Like the smartwatch application, it collects accelerometer and gyroscope data at \\(100\\)Hz. On the computing facet, the application processes the collected data (either from the smartwatch or from the phone itself) by applying a temporal alignment, Min-Max scaling and data windowing. Once a data window is prepared, the bundle is used as input to an embedded ML or DL model that predicts which activity the subject is performing.\nBased on the predicted activities, the system can automatically detect the end of the TUG test and calculate the results. The end is detected when the application predicts three consecutive SEATED activities preceded by some SITTING_DOWN activities. This pattern, which requires the user to maintain a seated position after sitting down, is captured by the system approximately one second after the subject has sat down (i.e., \\(3\\) consecutive windows of \\(50\\) samples – \\(0.5\\) seconds per window – with a \\(50\\%\\) overlap – \\(0.25\\) seconds –). The application then computes the start and end time (and duration) of each subphase of the test by determining the boundaries between them using the predicted activities. Finally, with the start of the standing_up and the end of the sitting_down subphases, the duration of the entire test is determined. If for some reason the application cannot detect any of the other subphases, it will still be able to compute the total duration of the test as long as these two subphases are correctly identified and their duration calculated.\nAfter the detection of the end of the test, the data collection is automatically stopped on the sensing device (i.e., smartwatch or smartphone), and the test results are calculated and sent to the smartwatch if it is the sensing device. The smartphone application also displays a list of the executed TUG tests and their full results – Figure 4 (a) and Figure 4 (b), respectively –.\n\n\n\n\n\n\n\n\n\n\n\n(a) Main UI with TUG test assessments list\n\n\n\n\n\n\n\n\n\n\n\n(b) Detail UI of single assessment\n\n\n\n\n\n\n\nFigure 4: Screenshots of the smartphone application UI.\n\n\n\n\n\n\n\n\n\nAvailability\n\n\n\nThe TUG test smartphone application is available in GitHub and Zenodo (Matey-Sanz and González-Pérez 2022a).",
    "crumbs": [
      "HAR in mHealth: TUG test"
    ]
  },
  {
    "objectID": "04_tug.html#methodology",
    "href": "04_tug.html#methodology",
    "title": "HAR in mHealth: TUG Test Using Smartphones and Smartwatches",
    "section": "Methodology",
    "text": "Methodology\nThis section describes the methodology employed to evaluate the developed system.\n\nParticipants\nThirty healthy individuals participated in the evaluation of the system. The participants ranged from \\(21\\) to \\(73\\) years old (\\(\\mu = 43.7 \\pm 14\\)), where the ratio of male/female participants was \\(53\\%/47\\%\\).\n\n\n\nAge info: min=21.00, max=73.00, mean=43.77, std=14.08\nGender info: male=16 (53.333333333333336), female=14 (46.666666666666664)\n\n\n\n\nTable 1: Information of the participants in the evaluation.\n\n\n\n\n\n    \n      \n      Id\n      Age\n      Gender\n      Valid\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe experiment was approved by the ethics committee of the Universitat Jaume I (reference No. CD/88/2022) and carried out in accordance with the Declaration of Helsinki.\n\nThe objectives and description of the study were explained to the participants, who were informed that they were going to be video-recorded while performing the test, and their written informed consent was obtained. The experiment was approved by the ethics committee of the Universitat Jaume I (reference No. CD/88/2022) and carried out in accordance with the Declaration of Helsinki.\n\n\nDevices\nParticipants were provided with a TicWatch Pro 3 GPS (WH12018) smartwatch and two Xiaomi Poco X3 Pro (M2102J20SG) smartphones, equipped with an STMicroelectronics LSM6DSO IMU1 sensor. The smartwatch and one smartphone were used as sensing and computing devices respectively (C1), while the other smartphone was used as a sensing and computing device (C2). The participants were instructed to wear the smartwatch on the left wrist (the paired smartphone was left aside) and the smartphone on the left front pocket, without further instructions regarding orientation.\n1 IMU Specification\n\nSmartphone specs:\n\nAccelerometer: range (\\(\\pm8g\\)), accuracy (\\(\\pm0.02g\\))\nGyroscope: range (\\(\\pm1000dpi\\)), accuracy (\\(\\pm1dpi\\))\n\nSmartwatch specs:\n\nAccelerometer: range (\\(\\pm8g\\)), accuracy (\\(\\pm0.02g\\))\nGyroscope: range (\\(\\pm2000dpi\\)), accuracy (\\(\\pm1dpi\\))\n\n\nA Xiaomi Poco F2 Pro (M2004J11G) smartphone was also used to video-record the subjects while participating in the study. All devices’ internal clocks were synchronized using the NTP protocol.\n\n\nHAR classifier\nSupported by the results presented in the previous section, the CNN architecture has been chosen for activity classification. Therefore, two CNN models (i.e., one for smartphone and other smartwatch data) have been trained for HAR using the dataset presented in Smartphone and smartwatch HAR dataset and embedded in the smartphone application.\n\n\nExperimental protocol\nEach participant was instructed to execute the TUG test while wearing the smartwatch on the left wrist and the smartphone in the left front pocket, adhering to the following procedure:\n\nFrom a seated position, press the “start” button on the smartphone application, lock the device and store it in the front left pocket.\nWait for a sound emitted from the smartphone.\nPress the “start” button on the smartwatch application (already placed on the wrist).\nWait for a vibration emitted from the smartwatch.\nExecute the TUG test.\nOnce finished, the smartphone and smartwatch applications will show the test’s results.\n\nEvery participant executed the TUG test \\(10\\) times, yielding a total amount of \\(300\\) TUG test executions, whose results were extracted from both system configurations. From those \\(300\\) executions, \\(9\\) were discarded due to procedural breaches (i.e., start the TUG test before the measuring device is ready).\nThen, the recorded videos of the participants were manually analysed to determine the boundaries between the activities contained in the test to measure the duration of the test and its subphases.\nIn addition, Google’s BatteryHistorian2 tool was used to obtain the percentage of battery consumed by the system in the three devices used in the experiment.\n2 Battery Historian documentation\n\nEvaluation analyses\nTo obtain a first insight into the performance of the TUG automation system, the computed results were classified as follows:\n\nSuccess: the system was able to measure the total duration of the test and its subphases.\nPartial success: the system was able to measure the total duration of the test, but failed to measure any of its subphases.\nFailure: the system was not able to compute the total duration of the test.\n\nThen, the computed results were compared with the manually obtained results (refernce method) using several metrics:\n\nErrors: a direct comparison between a system and a manual measure to check the difference between them. The significance of the errors (i.e., different from 0) was tested using one-sample T-tests or W-tests. Then, both system configurations were also compared in terms of inter-subject RMSE, using two-sample T-tests or MWU tests to evaluate the significance of the comparison.\nBland Altman agreement: determines the agreement between measurements from C1 and C2 with the manual method (i.e., gold standard).\nICC: the ICC estimations and their \\(95\\%\\) confidence intervals based on a single rater, absolute-agreement, 2-way mixed-effects model (i.e., \\(ICC_{(2,1)}\\)) were computed comparing each system configuration with the manual approach.\n\nFinally, the energy consumption of the system was analysed using the data obtained from the Google’s BatteryHistorian tool.",
    "crumbs": [
      "HAR in mHealth: TUG test"
    ]
  },
  {
    "objectID": "04_tug.html#results",
    "href": "04_tug.html#results",
    "title": "HAR in mHealth: TUG Test Using Smartphones and Smartwatches",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\n\n\nSplitting approach analysis\n\n\nDetermines which activity splitting approach provides better accuracy results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSystem’s agreement and reliabilitty\n\n\nAnalytically validates the agreement and reliability of the developed system compared with a reference method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnergy consumption\n\n\nAnalyzes the energy consumption of the developed system.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nGonzález-Pérez, Alberto, Miguel Matey-Sanz, Carlos Granell, Laura Díaz-Sanahuja, Juana Bretón-López, and Sven Casteleyn. 2023. “AwarNS: A Framework for Developing Context-Aware Reactive Mobile Applications for Health and Mental Health.” Journal of Biomedical Informatics, 104359. https://doi.org/10.1016/j.jbi.2023.104359.\n\n\nMatey-Sanz, Miguel, and Alberto González-Pérez. 2022a. “TUG Test Smartphone Application.” Zenodo. https://doi.org/10.5281/zenodo.7456835.\n\n\n———. 2022b. “TUG Test Smartwatch Application.” Zenodo. https://doi.org/10.5281/zenodo.7457098.\n\n\nMatey-Sanz, Miguel, Alberto González-Pérez, Sven Casteleyn, and Carlos Granell. 2022. “Instrumented Timed up and Go Test Using Inertial Sensors from Consumer Wearable Devices.” In International Conference on Artificial Intelligence in Medicine, 144–54. Springer. https://doi.org/10.1007/978-3-031-09342-5\\_14.\n\n\n———. 2024. “Implementing and Evaluating the Timed up and Go Test Automation Using Smartphones and Smartwatches.” IEEE Journal of Biomedical and Health Informatics 28 (11): 6594–6605. https://doi.org/10.1109/JBHI.2024.3456169.\n\n\nPodsiadlo, Diane, and Sandra Richardson. 1991. “The Timed ‘up & Go’: A Test of Basic Functional Mobility for Frail Elderly Persons.” Journal of the American Geriatrics Society 39 (2): 142–48. https://doi.org/10.1111/j.1532-5415.1991.tb01616.x.",
    "crumbs": [
      "HAR in mHealth: TUG test"
    ]
  },
  {
    "objectID": "05_wifi-csi.html",
    "href": "05_wifi-csi.html",
    "title": "Looking into the Future: Wi-Fi CSI based HAR",
    "section": "",
    "text": "Methodology: preliminar localized HAR experiment",
    "crumbs": [
      "Wi-Fi CSI based HAR"
    ]
  },
  {
    "objectID": "05_wifi-csi.html#methodology-preliminar-localized-har-experiment",
    "href": "05_wifi-csi.html#methodology-preliminar-localized-har-experiment",
    "title": "Looking into the Future: Wi-Fi CSI based HAR",
    "section": "",
    "text": "Data collection\nA dataset is collected using a TP-Link Archer C80 router (one TX antenna) and a SparkFun Thing Plus ESP32-S2 WROOM (one RX antenna) connected to a laptop. The TX and RX were separated by \\(5\\) meter in Line of Sight (LOS) condition, with two chairs placed in between them, at \\(0.5\\) meter from each device. Although the chair partially blocks the signal, we consider the setup to be in LOS condition since no heavy obstacles (e.g., walls) are blocking the signal.\nThe TX device was configured to work with the standard IEEE 802.11n operating in the channel \\(6\\). The RX device was configured to establish a connection with the TX, send ping requests at \\(100\\)Hz, and extract the Wi-Fi CSI information from the HT-LTF subcarriers (\\(64\\), \\(56\\) non-null) of the ping responses.\nFigure 1 depicts the data collection process. It consisted of one subject moving from one chair to the other repeatedly, collecting data for the activities widely used along this thesis: SEATED, STANDING_UP, WALKING, TURNING and SITTING_DOWN|. Since the subject performed the activities in both directions (i.e., from TX to RX and vice versa), the activities were labelled accordingly (e.g., SEATED_TX/RX, WALKING_TX/RX, etc.) adding a localization component to them.\n\n\n\n\n\n\nFigure 1: Data collection environment and activities performed\n\n\n\nFigure 2 depicts the data collection strategy, which was spaced out over time to explore potential degradation of CSI data over time. The following datasets were collected:\n\n\n\n\n\n\nFigure 2: Data collection procedure\n\n\n\n\nD1: The subject performed the sequence of activities \\(20\\) times (\\(10\\) in each direction).\nD2: After \\(10\\) minutes of collecting D1, the subject performed again the sequence of activities \\(4\\) times (\\(2\\) in each direction).\nD3: After \\(20\\) minutes of collecting D2, the subject performed again the sequence of activities \\(4\\) times.\nD4: After \\(60\\) minutes of collecting D3, the subject performed again the sequence of activities \\(4\\) times.\n\nTable 1 shows the number of CSI samples collected for each activity and dataset.\n\n\n\n\n\nTable 1: Collected CSI samples fir each activity and dataset\n\n\n\n\n\n\n\n\n\n\nD1\nD2\nD3\nD4\n\n\n\n\nSEATED_RX\n2864.0\n614.0\n593.0\n569.0\n\n\nSTANDING_UP_RX\n1305.0\n293.0\n276.0\n269.0\n\n\nWALKING_TX\n2285.0\n455.0\n466.0\n469.0\n\n\nTURN_TX\n1133.0\n222.0\n238.0\n208.0\n\n\nSITTING_DOWN_TX\n1538.0\n351.0\n301.0\n315.0\n\n\nSEATED_TX\n2890.0\n415.0\n504.0\n499.0\n\n\nSTANDING_UP_TX\n1289.0\n291.0\n271.0\n267.0\n\n\nWALKING_RX\n2470.0\n503.0\n504.0\n510.0\n\n\nTURN_RX\n997.0\n194.0\n228.0\n175.0\n\n\nSITTING_DOWN_RX\n1524.0\n353.0\n304.0\n301.0\n\n\nTotal\n18295.0\n3691.0\n3685.0\n3582.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nData preparation\nFirst, from the raw CSI data, the signal amplitude values of each subcarrier where obtained using the equation \\[\namplitude_{i} = \\sqrt{real_{i}^2 + imaginary_{i}^2},\n\\] where \\(real_{i}\\) and \\(imaginary_{i}\\) are the corresponding components of the complex number associated with the \\(i^{th}\\) subcarrier. The phase of the signal was discarded.\nNext, the dataset was arranged in windows of \\(50\\) samples with a \\(50\\%\\) overlap. Then, each window was processed using the following techniques:\n\nThe DBSCAN clustering algorithm (Ester et al. 1996) was employed to detect outliers and replace them using the average value of the \\(5\\) previous and posterior values.\nA \\(2\\)-level discrete wavelet transform was used to decompose the signals, apply threshold-based filtering on the detail coefficients and reconstruct the signal with the inverse discrete wavelet transform.\n\nFigure 3 and Figure 4 depict the raw (after amplitude extraction) and processed CSI data of the first two sequences of D1.\n\n\n\n\n\n\n\n\nFigure 3: First two executions of the D1 dataset before applying the data processing techniques.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: First two executions of the D1 dataset after applying the data processing techniques.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute this process is 01_1_preliminar-dataset-processing.py with the flag --method proposed.\n\n\nCode\n\"\"\"Data preprocessing script for preliminar dataset.\n\nProcesses the raw data by: arange samples in windows and process them using 1) DBSCAN for outlier detection\nand 2-level DWT for threshold based filtering or 2) Choi et al. method.\n\n**Example**:\n\n    $ python 01_1_preliminar-dataset-processing.py \n        --input_data_path &lt;PATH_OF_RAW_DATA&gt; \n        --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;\n        --method &lt;PROCESSING_METHOD&gt;\n        --window_size &lt;WINDOW_SIZE&gt;\n        --window_overlap &lt;WINDOW_OVERLAP&gt;\n\"\"\"\n\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(\"../../..\")\n\nimport numpy as np\n\nfrom alive_progress import alive_bar\nfrom libs.chapter5.pipeline.processing import proposed_method, choi_method\nfrom libs.chapter5.pipeline.raw_data_loading import load_labelled_data\n\nWINDOW_SIZE = 50\nWINDOW_OVERLAP = 25\n\n\ndef create_windows(executions_amplitudes, executions_labels, window_size, window_overlap):\n    win = {}\n    win_labels = {}\n    for execution_id in executions_amplitudes:\n        amplitudes = executions_amplitudes[execution_id]\n        exec_labels = executions_labels[execution_id]\n\n        data = amplitudes\n        n = data.shape[1] // window_overlap\n\n        windows = []\n        windows_labels = []\n        for i in range(0, (n-1) * window_overlap, window_overlap):\n            if i+window_size &gt; data.shape[1]:\n                break\n            window_labels = exec_labels[i:i+window_size]\n            values, counts = np.unique(window_labels, return_counts=True)\n            if len(values) != 1:\n                continue\n            windows.append(data[:,i:i+window_size])\n            windows_labels.append(values[counts.argmax()])\n\n        windows = np.array(windows)\n        windows_labels = np.array(windows_labels)\n\n        win[execution_id] = windows\n        win_labels[execution_id] = windows_labels\n    return win, win_labels\n\n\ndef process_windows(executions_windows, processing_function):\n    processed_windows = {}\n    executions_ids = executions_windows.keys()\n    with alive_bar(len(executions_ids), title=f'Processing windows', force_tty=True) as progress_bar:\n        for execution_id in executions_ids:\n            proc_windows = []\n            windows = executions_windows[execution_id]\n            for window in windows:\n                proc_windows.append(processing_function(window))\n            processed_windows[execution_id] = np.array(proc_windows)\n            progress_bar()\n    return processed_windows\n\n\ndef save_windowed_data(data, labels, directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    x_file_path = os.path.join(directory, '{0}-x.npy')\n    y_file_path = os.path.join(directory, '{0}-y.npy')\n\n    for execution_id in data:\n        x = data[execution_id]\n        y = labels[execution_id]\n\n        np.save(x_file_path.format(execution_id), x)\n        np.save(y_file_path.format(execution_id), y)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_data_path', help='Path of input data', type=str, required=True)\n    parser.add_argument('--windowed_data_path', help='Path to store windowed data', type=str, required=True)\n    parser.add_argument('--method', help='Processing method', required=True, choices=['proposed', 'choi'])\n    args = parser.parse_args()\n\n    processing_function = proposed_method if args.method == 'proposed' else choi_method\n\n    for dataset in ['D1', 'D2', 'D3', 'D4']:\n        print(f'Processing dataset {dataset}')\n        executions_amp, labels = load_labelled_data(os.path.join(args.input_data_path, dataset))\n        windows, windows_labels = create_windows(executions_amp, labels, WINDOW_SIZE, WINDOW_OVERLAP)\n        windows_processed = process_windows(windows, processing_function)\n        save_windowed_data(windows_processed, windows_labels, os.path.join(args.windowed_data_path, dataset))\n\n\n\n\n\n\nHAR classifier\nSince a previous section showed that the CNN was the best-performing model from the selected ones, in this chapter we keep using a CNN architecture despite the domain of the input data being different. The Grid search technique was used to determine the best hyperparameters for the selected architecture. The process was configured to train and evaluate each combination five times using the Adam optimizer during \\(50\\) epochs with a batch size of \\(32\\) windows. The process was executed in two phases to reduce the computational cost: 1) optimization of layers and learning hyperparameters, and 2) optimization of the number of layers. Table 2 contains the best combination of hyperparameters\n\n\n\n\n\nTable 2: Best combination of hyperparameters.\n\n\n\n\n\n\n\n\n\n\nValue\n\n\n\n\nconv\n2D\n\n\ninput_cnn_filters\n128\n\n\ninput_cnn_filter_size_x\n5\n\n\ninput_cnn_filter_size_y\n25\n\n\nbatch_norm\n1\n\n\nmax_pool\n1\n\n\nextra_cnn\n0\n\n\ndropout\n0\n\n\nn_dense\n1\n\n\ndense_laye\n512\n\n\nlr\n0.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute the Grid Search is 02_hyperparameter-optimization.py with the flag --model cnn.\n\n\nCode\n\"\"\"Hyperparameters Grid Search script.\n\nPerforms an hyperparameter Grid Search on the specified model. The selected hyperparameters for the search\ncan be found in `tuning_configuration.py`.\n\n**Example**:\n\n    $ python 02_hyperparameter-optimization.py \n        --data_dir &lt;PATH_OF_DATA&gt; \n        --model &lt;MLP,CNN&gt;\n        --phase &lt;initial,extra-layers&gt;\n        --batch_size &lt;BATCH_SIZE&gt;\n        --epochs &lt;EPOCHS&gt;\n        --executions &lt;EXECUTIONS&gt;\n\"\"\"\n\n\nimport argparse\nimport os\n\nimport sys\nsys.path.append(\"../../..\")\n\nfrom libs.chapter5.pipeline.data_loading import load_data\nfrom libs.chapter5.pipeline.data_grouping import combine_windows\nfrom libs.chapter5.pipeline.hyperparameters_tuning import get_model_builder, create_tuner, tune, get_tuning_summary\nfrom libs.chapter5.pipeline.tuning_configuration import get_tuning_configuration\nfrom libs.common.data_loading import ground_truth_to_categorical\nfrom libs.common.utils import save_json, set_seed\n\nTUNING_DIR = 'GRID_SEARCH_{0}'\nTUNING_SUMMARY_FILE = 'summary.json'\n\nBATCH_SIZE = 32\nEPOCHS = 50\nN_EXECUTIONS = 5\n\nMAPPING = {\n    'SEATED_RX': 0, \n    'STANDING_UP_RX': 1, \n    'WALKING_TX': 2, \n    'TURN_TX': 3, \n    'SITTING_DOWN_TX': 4, \n    'SEATED_TX': 5, \n    'STANDING_UP_TX': 6,\n    'WALKING_RX': 7,\n    'TURN_RX': 8,\n    'SITTING_DOWN_RX': 9,\n}\n\ndef tune_model(data, model_type, batch_size, epochs, n_executions, phase):\n    set_seed()    \n    model_builder = get_model_builder(model_type)\n    optimizing_layers = phase == 'extra-layers' \n\n    for source, (x, y) in data.items():\n        features_dimension = x.shape[1]\n        tuning_configuration = get_tuning_configuration(model_type, source if optimizing_layers else None)\n        tuning_configuration['features_dimension'] = features_dimension\n        tuning_project = f'{model_type}_{source}{\"_layers\" if optimizing_layers else \"\"}'\n        print(f'Tuning {model_type} model with {source} data')\n        tuner = create_tuner(\n            model_builder, \n            n_executions, \n            tuning_configuration, \n            TUNING_DIR.format(phase), \n            tuning_project\n        )\n\n        tuner = tune(tuner, x, y, epochs, batch_size)\n        save_tuning_summary(tuner, os.path.join(TUNING_DIR, tuning_project))\n\n\ndef save_tuning_summary(tuner, tuning_dir):\n    save_json(get_tuning_summary(tuner), tuning_dir, TUNING_SUMMARY_FILE)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', help='data directory', type=str, required=True)\n    parser.add_argument('--model', help='optimize hyperparameters for selected model', type=str, choices=['mlp', 'cnn'])\n    parser.add_argument('--phase', help='tuning phase: &lt;initial&gt; to tune layer hyperparameters and &lt;extra-layers&gt; to tune number of layers' , type=str, choices=['initial', 'extra-layers'])\n    parser.add_argument('--batch_size', help='training batch size', type=int, default=BATCH_SIZE)\n    parser.add_argument('--epochs', help='training epochs', type=int, default=EPOCHS)\n    parser.add_argument('--executions', help='executions per trial', type=int, default=N_EXECUTIONS)\n    args = parser.parse_args()\n\n    d1_windows, d1_labels = load_data(args.data_dir)\n    y = ground_truth_to_categorical(d1_labels, MAPPING)    \n    x, y = combine_windows(d1_windows, y)\n    print(x.shape)\n    \n    data = {\n        'csi': (x, y)\n    }\n    tune_model(data, args.model, args.batch_size, args.epochs, args.executions, args.phase)    \n\n\n\n\n\n\nExperimental procedure\nFigure 5 depicts the three different evaluation approaches employed to determine the performance of a Wi-Fi CSI model for localized HAR and study the stability of the CSI data over time.\n\n\n\n\n\n\nFigure 5: Evaluation procedures. First, \\(10\\) K-fold cross-validation is used with D1. Then, D1 is split in D1T and D1E (\\(80\\%/20\\%\\)) maintaining temporal dependencies. Finally, datasets D2, D3 and D4 evaluate a model trained with D1T.\n\n\n\n\nK-fold cross-validation: classical procedure widely employed in the literature for model evaluation. It consists of splitting the available data into \\(K\\) parts, where each \\(k_{i}\\) part is used to evaluate a model trained with the remaining \\(K-1\\) parts. We employ this evaluation approach with the D1 dataset and \\(K=10\\).\nMaintaining the temporal dependencies: the K-fold cross-validation is not the most appropriate evaluation approach when dealing with time series since the temporality of the data is altered. To maintain that temporality, the first \\(16\\) sequences of activities (\\(80\\%\\) of data) from D1 are used for training and the remaining (last) \\(4\\) sequences (\\(20\\%\\)) for evaluation. These subsets of D1 are named D1T (training) and D1E (evaluation). This is a basic approach to investigate the stability of the data.\nEffect of time: the model trained with D1T is evaluated using the data from D2, D3 and D4. This approach allows to analyse the variation of the classification performance in different time frames (\\(10\\), \\(30\\) and \\(90\\) minutes after D1) and therefore, to determine the stability of the CSI data.\n\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute this process is 03_1_multiple-evaluations.py with the flag --model cnn.\n\n\nCode\n\"\"\"Multiple evaluation script\n\nPerforms a cross-validation and an evaluation with different subsets collected at different time frames.\n\n**Example**:\n\n    $ python 03_1_multiple_evaluations.py \n        --data_dir &lt;PATH_OF_DATA&gt; \n        --reports_dir &lt;PATH_TO_STORE_REPORTS&gt;\n        --model &lt;MLP,CNN&gt;\n\"\"\"\n\n\nimport argparse\nimport os\nimport sys\nsys.path.append(\"../../..\")\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom libs.chapter5.pipeline.data_loading import load_data\nfrom libs.chapter5.pipeline.data_grouping import combine_windows, split_train_test\nfrom libs.chapter5.pipeline.ml import cross_validation, evaluate_model\nfrom libs.common.data_loading import ground_truth_to_categorical\nfrom libs.common.utils import save_json, set_seed\n\nMAPPING = {\n    'SEATED_RX': 0, \n    'STANDING_UP_RX': 1, \n    'WALKING_TX': 2, \n    'TURN_TX': 3, \n    'SITTING_DOWN_TX': 4, \n    'SEATED_TX': 5, \n    'STANDING_UP_TX': 6,\n    'WALKING_RX': 7,\n    'TURN_RX': 8,\n    'SITTING_DOWN_RX': 9,\n}\nLABELS = ['SEATED_RX','STANDING_UP_RX','WALKING_TX','TURNING_TX','SITTING_DOWN_TX', 'SEATED_TX', 'STANDING_UP_TX','WALKING_RX','TURNING_RX','SITTING_DOWN_RX']\nNUM_CLASSES = len(LABELS)\n\nTRAIN_IDS = ['e01_rx_tx', 'e01_tx_rx', 'e02_rx_tx', 'e02_tx_rx', 'e03_rx_tx', 'e03_tx_rx', 'e04_rx_tx', 'e04_tx_rx',\n             'e05_rx_tx', 'e05_tx_rx', 'e06_rx_tx', 'e06_tx_rx', 'e07_rx_tx', 'e07_tx_rx', 'e08_rx_tx', 'e08_tx_rx']\nTEST_IDS = ['e09_rx_tx', 'e09_tx_rx', 'e10_rx_tx', 'e10_tx_rx']\n\nBATCH_SIZE = 32\nEPOCHS = 50\nFOLDS = 10\n\ndef mlp_model():\n    set_seed()\n\n    model = keras.Sequential([\n        layers.Dense(128, activation='relu', input_shape=(500,)),\n        layers.Dense(1024, activation='relu'),\n        layers.Dense(1024, activation='relu'),\n        layers.Dense(1024, activation='relu'),\n        layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n\n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0005), metrics=['accuracy'])\n    return model\n\n\ndef cnn_model():\n    set_seed()\n\n    model = keras.Sequential([\n        layers.Conv2D(filters=128, kernel_size=(5,25), input_shape=(56, 50, 1)),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.MaxPooling2D(),\n        \n        layers.Flatten(),\n        \n        layers.Dense(512, activation='relu'),\n        layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n\n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n    return model\n\n\ndef model_builder(model_type):\n    if model_type == 'cnn':\n        return cnn_model\n    return mlp_model\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', help='data directory', type=str, required=True)\n    parser.add_argument('--reports_dir', help='directory to store the generated classification reports', type=str, required=True)\n    parser.add_argument('--model', help='optimize hyperparameters for selected model', type=str, choices=['mlp', 'cnn'])\n    args = parser.parse_args()\n\n    d1_windows, d1_labels = load_data(os.path.join(args.data_dir, 'D1'))\n    d1_labels_cat = ground_truth_to_categorical(d1_labels, MAPPING)    \n    x, y = combine_windows(d1_windows, d1_labels_cat)\n    \n    print(\"Starting 10-fold cross-validation\")\n    cv_reports = cross_validation(x, y, model_builder(args.model), FOLDS, BATCH_SIZE, EPOCHS, LABELS)\n    save_json(cv_reports, args.reports_dir, 'cv_report.json')\n\n    print(\"Starting D1T training and D1E evaluation\")\n    (x_d1t, y_d1t), (x_d1e, y_d1e) = split_train_test(d1_windows, d1_labels_cat, TRAIN_IDS, TEST_IDS)\n    model = model_builder(args.model)()\n    model.fit(x_d1t, y_d1t, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0)\n    report = evaluate_model(model, x_d1e, y_d1e, LABELS)\n    save_json(report, args.reports_dir, 'd1_report.json')\n\n    print(\"Starting D2, D3 and D4 evaluation\")\n    for eval_dataset in ['D2', 'D3', 'D4']:\n        windows, labels = load_data(os.path.join(args.data_dir, eval_dataset))\n        labels_cat = ground_truth_to_categorical(labels, MAPPING)    \n        x, y = combine_windows(windows, labels_cat)\n        report = evaluate_model(model, x, y, LABELS)\n        save_json(report, args.reports_dir, f'{eval_dataset.lower()}_report.json')",
    "crumbs": [
      "Wi-Fi CSI based HAR"
    ]
  },
  {
    "objectID": "05_wifi-csi.html#investigating-the-causes-of-failure",
    "href": "05_wifi-csi.html#investigating-the-causes-of-failure",
    "title": "Looking into the Future: Wi-Fi CSI based HAR",
    "section": "Investigating the causes of failure",
    "text": "Investigating the causes of failure\nThe previous methodology resulted in non-satisfactory outcomes (see Localized HAR based on Wi-Fi CSI). The results showed a clear degradation in the classification accuracy of the employed CNN model when the evaluation took into account data collected spaced in time regarding the training data. That is, classification accuracy quickly degrades over time.\nNotwithstanding, temporal instability of CSI data is only one possible explanation for the poor obtained results. Concretely, the following factors could affect the results:\n\nThe selected methods (i.e., data preprocessing and model architecture) might not be able to properly work with CSI data, i.e., generalize from the training data. While CNN approaches have proven to provide good results working with CSI data (Ma, Zhou, and Wang 2019), most related works using the ESP32 microcontroller employ other architectures, such as the MLP.\nThe employed hardware for CSI extraction, ESP32-S2 microcontroller, might not be appropriate for such a task. Other devices, such as the Intel 5300 or Atheros NICs might be more appropriate.\nThe collected dataset might have been affected by some external interference, altering the environment and changing the CSI data.\nThe CSI data is not stable over time and therefore can not be used for real-life applications.\n\nNext, we aim to determine the cause of the bad results presented in Localized HAR based on Wi-Fi CSI. First, to determine that our method is appropriate for CSI data (1), we applied it to two public datasets and compared the results with other state-of-the-art works (Validation of employed methods). Then, to prove that alternative methods validated in the literature would have obtained similar results to our method (1), we applied the method from a related work on our collected dataset (Validation of employed methods). Finally, to verify the temporal stability of the CSI data (4), a new dataset was collected over several days to evaluate the similarity of the data across days (Temporal stability of Wi-Fi CSI data from ESP32 microcontrollers). The remaining factors could not be explored due to resource limitations (2) and the impossibility of determining the existence of external interferences while collecting the dataset (3).\n\nValidation of method on public datasets\n\nMethodology\nTwo publicly available datasets have been used to validate the methods and model employed: the StanWiFi and the Multi-environment dataset.\n\nStanWiFi: it was collected by (Yousefi et al. 2017) and made available in GitHub1. The dataset was collected with a Wi-Fi router (Tx) and an Intel 5300 NIC with three Rx antennas, both separated by \\(3\\) meter in a LOS environment. The dataset contains CSI data from \\(90\\) subcarriers sampled at \\(1000\\)Hz corresponding to \\(7\\) activities: lie, fall, walk, run, sit down, stand up and pick up. For comparison purposes, the pick up activity was removed from the dataset since other works do so.\nMulti-environment: collected in three different environments, E1 and E2 in LOS conditions and E3 in NLOS condition (Alsaify et al. 2020). The latter dataset is discarded since we focus on LOS conditions. The datasets were collected using two computers (Tx and Rx) equipped with an Intel 5300 NIC, which were separated by \\(3.7\\) meter sin E1 and \\(7.6\\) meters in E2. The CSI data was collected from \\(90\\) subcarriers at \\(320\\)Hz corresponding to \\(12\\) different activities classified in \\(6\\) groups: no movement, falling, walking, sitting/standing, turning and pick up.\n\n1 Wifi_Activity_RecognitionThe data preparation steps described in Data preparation were applied to both datasets. While for the collected dataset the windows consisted of \\(0.5\\) seconds of data, a window size of \\(1\\) seconds was employed in both public datasets since they contain a higher amount of data.\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute the process in StanWiFi dataset is 01_2_stanwifi-processing.py.\n\n\nCode\n\"\"\"Data preprocessing script for StanWiFi dataset.\n\nProcesses the raw data by processing the windows generated by the author's scripts using DBSCAN for outlier detection\nand 2-level DWT for threshold based filtering.\n\n**Example**:\n\n    $ python 01_2_stanwifi-processing.py \n        --input_data_path &lt;PATH_OF_RAW_DATA&gt; \n        --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(\"../../..\")\n\nimport numpy as np\n\nfrom cross_vali_input_data import csv_import\nfrom libs.chapter5.pipeline.processing import proposed_method\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--windowed_data_path', help='Path to store windowed data', type=str, required=True)\n    args = parser.parse_args()\n    \n    x_bed, x_fall, x_pickup, x_run, x_sitdown, x_standup, x_walk, \\\n    y_bed, y_fall, y_pickup, y_run, y_sitdown, y_standup, y_walk = csv_import()\n\n    x_subsets = [x_bed, x_fall, x_run, x_sitdown, x_standup, x_walk]\n    y_subsets = [y_bed, y_fall, y_run, y_sitdown, y_standup, y_walk]\n\n    x_proc = []\n    y_proc = []\n    for x, y in zip(x_subsets, y_subsets):\n        x_proc.append(proposed_method(x))\n        y_proc.append(np.delete(y, [0,4], axis=1))\n\n    x = np.vstack(x_proc)\n    x = np.transpose(x, axes=(0,2,1))\n    y = np.vstack(y_proc)\n\n    np.save(os.path.join(args.windowed_data_path, 'x.npy'), x)\n    np.save(os.path.join(args.windowed_data_path, 'y.npy'), y)\n\n\nThe script employed to execute the process in Multi-environment dataset is 01_3_multienvironment-processing.py.\n\n\nCode\n\"\"\"Data preprocessing script for Multi-environment dataset.\n\nProcesses the raw data by aranging samples in windows and processing them using DBSCAN for outlier detection\nand 2-level DWT for threshold based filtering\n\n**Example**:\n\n    $ python 01_3_multienvironment-processing.py \n        --input_data_path &lt;PATH_OF_RAW_DATA&gt; \n        --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;\n\"\"\"\n\nimport argparse\nimport copy\nimport os\nimport sys\nsys.path.append(\"../../..\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom alive_progress import alive_bar\nfrom libs.chapter5.pipeline.processing import proposed_method\nfrom math import sqrt\n\nACTIVITY_MAPPING = {\n    'A01': 'A1',\n    'A02': 'A2',\n    'A03': 'A1',\n    'A04': 'A1',\n    'A05': 'A2',\n    'A06': 'A3',\n    'A07': 'A5',\n    'A08': 'A3',\n    'A09': 'A5',\n    'A10': 'A4',\n    'A11': 'A4',\n    'A12': 'A6',\n}\n\ndef load_multienvironment_dataset(environment):\n    data = {}\n    subject_dirs = os.listdir(environment)\n    subject_dirs = list(filter(lambda x: x.startswith('Subject'), subject_dirs))\n    with alive_bar(len(subject_dirs), title=f'Loading data from subjects', force_tty=True) as progress_bar:\n        for subject_dir in subject_dirs:\n            subject = f'S{int(subject_dir.split(\" \")[-1]):02d}'\n            data[subject] = {}\n            subject_dir_path = os.path.join(environment, subject_dir)\n            for file in os.listdir(subject_dir_path):\n                if not file.endswith('.csv'):\n                    continue\n\n                base_activity = file.split('_')[3]\n                file_path = os.path.join(subject_dir_path, file)\n                df = pd.read_csv(file_path)\n                df = df.iloc[160:-160] #remove 0.5 sec after and before due to noise\n\n                if base_activity not in data[subject]:\n                    data[subject][base_activity] = df\n                else:\n                    data[subject][base_activity] = pd.concat([data[subject][base_activity], df])\n            progress_bar()\n    return data\n\n\ndef amplitude_from_raw_data(data):\n    amplitudes = {}\n    with alive_bar(len(data.keys()), title=f'Extracting amplitudes from subject\\'s data', force_tty=True) as progress_bar:\n        for subject in data:\n            amplitudes[subject] = {}\n            for activity in data[subject]:\n                activity_data = data[subject][activity]\n                activity_amplitudes = []\n                for index, row in activity_data.iterrows():\n                    instance_amplitudes = []\n                    for antenna in range(1,4):\n                        for subcarrier in range(1,31):\n                            csi_data = row[f'csi_1_{antenna}_{subcarrier}']\n                            real, imaginary = csi_data.split('+')\n                            real = int(real)\n                            imaginary = int(imaginary[:-1])\n\n                            instance_amplitudes.append(sqrt(imaginary ** 2 + real ** 2))\n                    activity_amplitudes.append(instance_amplitudes)\n                amplitudes[subject][activity] = np.array(activity_amplitudes)\n            progress_bar()\n    return amplitudes\n\n\ndef create_windows(amplitudes, window_size=320, window_overlap=160):\n    windows = {}\n    windows_labels = {}\n    for subject_id in amplitudes:\n        subject_windows = []\n        subject_windows_labels = []\n        for activity_id in amplitudes[subject_id]:\n            activity_amplitudes = amplitudes[subject_id][activity_id].T\n\n            n = activity_amplitudes.shape[1] // window_overlap\n            for i in range(0, (n-1) * window_overlap, window_overlap):\n                if i+window_size &gt; activity_amplitudes.shape[1]:\n                    break\n                subject_windows.append(activity_amplitudes[:,i:i+window_size])\n                subject_windows_labels.append(ACTIVITY_MAPPING[activity_id])\n\n        windows[subject_id] = np.array(subject_windows)\n        windows_labels[subject_id] = np.array(subject_windows_labels)\n    return windows, windows_labels\n\n\ndef process_windows(windows):\n    proc_windows = {}\n    with alive_bar(len(windows.keys()), title=f'Processing subject\\'s windows', force_tty=True) as progress_bar:\n        for subject_id in windows:\n            windows_copy = copy.deepcopy(windows[subject_id])\n            for i in range(len(windows_copy)):\n                windows_copy[i] = proposed_method(windows_copy[i])\n            proc_windows[subject_id] = windows_copy\n            progress_bar()\n    return proc_windows\n\n\ndef save_windowed_data(data, labels, directory):\n    for subject_id, subject_data in data.items():\n        np.save(os.path.join(directory, f'{subject_id}_x.npy'), subject_data)\n        np.save(os.path.join(directory, f'{subject_id}_x.npy'), labels[subject_id])\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_data_path', help='Path of input data', type=str, required=True)\n    parser.add_argument('--windowed_data_path', help='Path to store windowed data', type=str, required=True)\n    args = parser.parse_args()\n\n    for dataset in ['ENVIRONMENT 1', 'ENVIRONMENT 2']:\n        print(f'Processing dataset {dataset}')\n        data = load_multienvironment_dataset(os.path.join(args.input_data_path, dataset))\n        amplitudes = amplitude_from_raw_data(data)\n        windows, windows_labels = create_windows(amplitudes)\n\n        del data, amplitudes\n        \n        proc_windows = process_windows(windows)\n        save_windowed_data(proc_windows, windows_labels, args.windowed_data_path)\n\n\n\n\nAs regards the HAR classifier, the model architecture described in HAR classifier was employed, with minor adaptations in some hyperparameters due to computational limitations2. The adaptations in each dataset are the following:\n2 The higher dimensionality of both datasets (higher sampling rate and data from more subcarriers) compared with the collected one makes it unfeasible to use the previous model due to memory limitations.\nStanWiFi: \\(16\\) number of filters, \\(128\\) batch size and \\(30\\) epochs.\nMulti-environment (E1 and E2): \\(8\\) number of filters, \\(256\\) batch size and \\(30\\) epochs.\n\nFinally, the experimental procedure consisted of the \\(10\\)-fold cross-validation to evaluate the CNN model in the public datasets. The results are compared with other related works also employing a K-fold cross-validation approach.\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute this process in the StanWiFi is 03_2_cross-validation.py with the flag --dataset stanwifi. The same script was used for the Multienvironment dataset employing the flag --dataset multienvironment.\n\n\nCode\n\"\"\"Cross-validation script\n\nPerforms a cross-validation on the selected dataset.\n\n**Example**:\n\n    $ python 03_2_cross-validation.py \n        --data_dir &lt;PATH_OF_DATA&gt; \n        --reports_dir &lt;PATH_TO_STORE_REPORTS&gt;\n        --dataset &lt;stanwifi,multienvironment&gt;\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nsys.path.append(\"../../..\")\n\nimport numpy as np\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom libs.chapter5.pipeline.data_grouping import combine_windows\nfrom libs.chapter5.pipeline.ml import cross_validation\nfrom libs.common.data_loading import ground_truth_to_categorical\nfrom libs.common.utils import save_json, set_seed\n\nSTANWIFI_LABELS = ['LIE DOWN', 'FALL', 'WALK', 'RUN', 'SITDOWN', 'STANDUP']\nSTANWIFI_BATCH_SIZE = 128\n\nMULTI_ENV_LABELS = ['No movement', 'Falling', 'Walking', 'Sitting/Standing', 'Turning', 'Pick up pen']\nMULTI_ENV_MAPPING = {'A1': 0, 'A2': 1, 'A3': 2, 'A4': 3, 'A5': 4, 'A6': 5}\nMULTIENV_BATCH_SIZE = 256\n\nFOLDS = 10\nEPOCHS = 30\n\n\ndef stanwifi_model():\n    set_seed()\n\n    model = keras.Sequential([\n        layers.Conv2D(filters=16, kernel_size=(5,25), input_shape=(90, 500, 1)),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.MaxPooling2D(),\n        \n        layers.Flatten(),\n        \n        layers.Dense(512, activation='relu'),\n        layers.Dense(6, activation='softmax')\n    ])\n\n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n    return model\n    \n\ndef multienvironment_model():\n    set_seed()\n\n    model = keras.Sequential([\n        layers.Conv2D(filters=8, kernel_size=(5,25), input_shape=(90, 320, 1)),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.MaxPooling2D(),\n        \n        layers.Flatten(),\n        \n        layers.Dense(512, activation='relu'),\n        layers.Dense(6, activation='softmax')\n    ])\n\n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n    return model\n\n\ndef load_multienv_data(path, dataset_dir):\n    dataset_path = os.path.join(path, dataset_dir)\n    subjects = ['S01', 'S02', 'S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S09', 'S10'] if dataset_dir == 'E1' else ['S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S19', 'S20'] \n\n    windows = {}\n    windows_labels = {}\n    for subject_id in subjects:\n        windows[subject_id] = np.load(os.path.join(dataset_path, f'x_{subject_id}.npy'))\n        windows_labels[subject_id] = np.load(os.path.join(dataset_path, f'y_{subject_id}.npy'))\n    return windows, windows_labels\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', help='data directory', type=str, required=True)\n    parser.add_argument('--reports_dir', help='directory to store the generated classification reports', type=str, required=True)\n    parser.add_argument('--dataset', help='optimize hyperparameters for selected model', type=str, choices=['stanwifi', 'multienvironment'])\n    args = parser.parse_args()\n\n    if args.dataset == 'stanwifi':\n        x = np.load(os.path.join(args.data_dir, 'x.npy'))\n        y = np.load(os.path.join(args.data_dir, 'x.npy'))\n        \n        model_builder = stanwifi_model\n        batch_size = STANWIFI_BATCH_SIZE\n        labels = STANWIFI_LABELS\n        reports = cross_validation(x, y, stanwifi_model, FOLDS, STANWIFI_BATCH_SIZE, EPOCHS, STANWIFI_LABELS)\n        save_json(reports, args.reports_dir, 'cv_report.json')\n    else:\n        for dataset in ['E1', 'E2']:\n            windows, windows_labels = load_multienv_data(args.data_dir, dataset)\n            windows_labels_cat = ground_truth_to_categorical(windows_labels, MULTI_ENV_MAPPING)  \n            x, y = combine_windows(windows, windows_labels_cat)\n            reports = cross_validation(x, y, multienvironment_model, FOLDS, MULTIENV_BATCH_SIZE, EPOCHS, MULTI_ENV_LABELS)\n            save_json(reports, args.reports_dir, f'{dataset.lower()}-cv_report.json')\n\n\n\n\n\n\n\nValidation of alternative method in the collected dataset\nThe methods proposed by Choi et al. (2022) have been applied to the collected dataset. In their work, the authors extract a set of hand-crafted features from the CSI data and employ an MLP model for crowd counting and localization. Choi’s et al. methods have been selected since they followed an appropiate evaluation taking into account the stability of the signal and only showed a small drop in performance.\n\nMethodology\nAs in Data preparation, the amplitude is extracted from the CSI data and the dataset is arranged in windows of \\(50\\) samples with a \\(50\\%\\) overlap. Then, the methods presented by Choi et al. (2022) are applied:\n\nNoise removal: the Hampel and the Savitzky-Golay filters are applied on each subcarrier.\nFeature extraction: the extracted features to be used as input of the MLP model are the Mean, SD, Maximum, Minimum, Lower quartile, Higher quartile, IQR, Differences between adjacent subcarriers and the Euclidean distance.\n\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute this process is 01_1_preliminar-dataset-processing.py with the flag --method choi.\n\n\nCode\n\"\"\"Data preprocessing script for preliminar dataset.\n\nProcesses the raw data by: arange samples in windows and process them using 1) DBSCAN for outlier detection\nand 2-level DWT for threshold based filtering or 2) Choi et al. method.\n\n**Example**:\n\n    $ python 01_1_preliminar-dataset-processing.py \n        --input_data_path &lt;PATH_OF_RAW_DATA&gt; \n        --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;\n        --method &lt;PROCESSING_METHOD&gt;\n        --window_size &lt;WINDOW_SIZE&gt;\n        --window_overlap &lt;WINDOW_OVERLAP&gt;\n\"\"\"\n\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(\"../../..\")\n\nimport numpy as np\n\nfrom alive_progress import alive_bar\nfrom libs.chapter5.pipeline.processing import proposed_method, choi_method\nfrom libs.chapter5.pipeline.raw_data_loading import load_labelled_data\n\nWINDOW_SIZE = 50\nWINDOW_OVERLAP = 25\n\n\ndef create_windows(executions_amplitudes, executions_labels, window_size, window_overlap):\n    win = {}\n    win_labels = {}\n    for execution_id in executions_amplitudes:\n        amplitudes = executions_amplitudes[execution_id]\n        exec_labels = executions_labels[execution_id]\n\n        data = amplitudes\n        n = data.shape[1] // window_overlap\n\n        windows = []\n        windows_labels = []\n        for i in range(0, (n-1) * window_overlap, window_overlap):\n            if i+window_size &gt; data.shape[1]:\n                break\n            window_labels = exec_labels[i:i+window_size]\n            values, counts = np.unique(window_labels, return_counts=True)\n            if len(values) != 1:\n                continue\n            windows.append(data[:,i:i+window_size])\n            windows_labels.append(values[counts.argmax()])\n\n        windows = np.array(windows)\n        windows_labels = np.array(windows_labels)\n\n        win[execution_id] = windows\n        win_labels[execution_id] = windows_labels\n    return win, win_labels\n\n\ndef process_windows(executions_windows, processing_function):\n    processed_windows = {}\n    executions_ids = executions_windows.keys()\n    with alive_bar(len(executions_ids), title=f'Processing windows', force_tty=True) as progress_bar:\n        for execution_id in executions_ids:\n            proc_windows = []\n            windows = executions_windows[execution_id]\n            for window in windows:\n                proc_windows.append(processing_function(window))\n            processed_windows[execution_id] = np.array(proc_windows)\n            progress_bar()\n    return processed_windows\n\n\ndef save_windowed_data(data, labels, directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    x_file_path = os.path.join(directory, '{0}-x.npy')\n    y_file_path = os.path.join(directory, '{0}-y.npy')\n\n    for execution_id in data:\n        x = data[execution_id]\n        y = labels[execution_id]\n\n        np.save(x_file_path.format(execution_id), x)\n        np.save(y_file_path.format(execution_id), y)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_data_path', help='Path of input data', type=str, required=True)\n    parser.add_argument('--windowed_data_path', help='Path to store windowed data', type=str, required=True)\n    parser.add_argument('--method', help='Processing method', required=True, choices=['proposed', 'choi'])\n    args = parser.parse_args()\n\n    processing_function = proposed_method if args.method == 'proposed' else choi_method\n\n    for dataset in ['D1', 'D2', 'D3', 'D4']:\n        print(f'Processing dataset {dataset}')\n        executions_amp, labels = load_labelled_data(os.path.join(args.input_data_path, dataset))\n        windows, windows_labels = create_windows(executions_amp, labels, WINDOW_SIZE, WINDOW_OVERLAP)\n        windows_processed = process_windows(windows, processing_function)\n        save_windowed_data(windows_processed, windows_labels, os.path.join(args.windowed_data_path, dataset))\n\n\n\n\nAs HAR classifier, an MLP model is employed, but instead of using the same architecture as the one employed by Choi, a Grid search process was executed to determine the most appropriate hyperparameters for our dataset.\nThe Grid search was carried out as described in HAR classifier. Table 3 contains the best combination of hyperparameters.\n\n\n\n\n\nTable 3: Best combination of hyperparameters.\n\n\n\n\n\n\n\n\n\n\nValue\n\n\n\n\ninput_layer\n128.0\n\n\nn_hidden\n3.0\n\n\nhidden_layer\n1024.0\n\n\nlr\n0.0005\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute the Grid Search is 02_hyperparameter-optimization.py with the flag --model mlp.\n\n\nCode\n\"\"\"Hyperparameters Grid Search script.\n\nPerforms an hyperparameter Grid Search on the specified model. The selected hyperparameters for the search\ncan be found in `tuning_configuration.py`.\n\n**Example**:\n\n    $ python 02_hyperparameter-optimization.py \n        --data_dir &lt;PATH_OF_DATA&gt; \n        --model &lt;MLP,CNN&gt;\n        --phase &lt;initial,extra-layers&gt;\n        --batch_size &lt;BATCH_SIZE&gt;\n        --epochs &lt;EPOCHS&gt;\n        --executions &lt;EXECUTIONS&gt;\n\"\"\"\n\n\nimport argparse\nimport os\n\nimport sys\nsys.path.append(\"../../..\")\n\nfrom libs.chapter5.pipeline.data_loading import load_data\nfrom libs.chapter5.pipeline.data_grouping import combine_windows\nfrom libs.chapter5.pipeline.hyperparameters_tuning import get_model_builder, create_tuner, tune, get_tuning_summary\nfrom libs.chapter5.pipeline.tuning_configuration import get_tuning_configuration\nfrom libs.common.data_loading import ground_truth_to_categorical\nfrom libs.common.utils import save_json, set_seed\n\nTUNING_DIR = 'GRID_SEARCH_{0}'\nTUNING_SUMMARY_FILE = 'summary.json'\n\nBATCH_SIZE = 32\nEPOCHS = 50\nN_EXECUTIONS = 5\n\nMAPPING = {\n    'SEATED_RX': 0, \n    'STANDING_UP_RX': 1, \n    'WALKING_TX': 2, \n    'TURN_TX': 3, \n    'SITTING_DOWN_TX': 4, \n    'SEATED_TX': 5, \n    'STANDING_UP_TX': 6,\n    'WALKING_RX': 7,\n    'TURN_RX': 8,\n    'SITTING_DOWN_RX': 9,\n}\n\ndef tune_model(data, model_type, batch_size, epochs, n_executions, phase):\n    set_seed()    \n    model_builder = get_model_builder(model_type)\n    optimizing_layers = phase == 'extra-layers' \n\n    for source, (x, y) in data.items():\n        features_dimension = x.shape[1]\n        tuning_configuration = get_tuning_configuration(model_type, source if optimizing_layers else None)\n        tuning_configuration['features_dimension'] = features_dimension\n        tuning_project = f'{model_type}_{source}{\"_layers\" if optimizing_layers else \"\"}'\n        print(f'Tuning {model_type} model with {source} data')\n        tuner = create_tuner(\n            model_builder, \n            n_executions, \n            tuning_configuration, \n            TUNING_DIR.format(phase), \n            tuning_project\n        )\n\n        tuner = tune(tuner, x, y, epochs, batch_size)\n        save_tuning_summary(tuner, os.path.join(TUNING_DIR, tuning_project))\n\n\ndef save_tuning_summary(tuner, tuning_dir):\n    save_json(get_tuning_summary(tuner), tuning_dir, TUNING_SUMMARY_FILE)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', help='data directory', type=str, required=True)\n    parser.add_argument('--model', help='optimize hyperparameters for selected model', type=str, choices=['mlp', 'cnn'])\n    parser.add_argument('--phase', help='tuning phase: &lt;initial&gt; to tune layer hyperparameters and &lt;extra-layers&gt; to tune number of layers' , type=str, choices=['initial', 'extra-layers'])\n    parser.add_argument('--batch_size', help='training batch size', type=int, default=BATCH_SIZE)\n    parser.add_argument('--epochs', help='training epochs', type=int, default=EPOCHS)\n    parser.add_argument('--executions', help='executions per trial', type=int, default=N_EXECUTIONS)\n    args = parser.parse_args()\n\n    d1_windows, d1_labels = load_data(args.data_dir)\n    y = ground_truth_to_categorical(d1_labels, MAPPING)    \n    x, y = combine_windows(d1_windows, y)\n    print(x.shape)\n    \n    data = {\n        'csi': (x, y)\n    }\n    tune_model(data, args.model, args.batch_size, args.epochs, args.executions, args.phase)    \n\n\n\n\nThe same experimental procedure described in Experimental procedure with the three evaluation approaches is employed using the method presented by Choi in our collected dataset.\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute this process is 03_1_multiple-evaluations.py with the flag --model mlp.\n\n\nCode\n\"\"\"Multiple evaluation script\n\nPerforms a cross-validation and an evaluation with different subsets collected at different time frames.\n\n**Example**:\n\n    $ python 03_1_multiple_evaluations.py \n        --data_dir &lt;PATH_OF_DATA&gt; \n        --reports_dir &lt;PATH_TO_STORE_REPORTS&gt;\n        --model &lt;MLP,CNN&gt;\n\"\"\"\n\n\nimport argparse\nimport os\nimport sys\nsys.path.append(\"../../..\")\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom libs.chapter5.pipeline.data_loading import load_data\nfrom libs.chapter5.pipeline.data_grouping import combine_windows, split_train_test\nfrom libs.chapter5.pipeline.ml import cross_validation, evaluate_model\nfrom libs.common.data_loading import ground_truth_to_categorical\nfrom libs.common.utils import save_json, set_seed\n\nMAPPING = {\n    'SEATED_RX': 0, \n    'STANDING_UP_RX': 1, \n    'WALKING_TX': 2, \n    'TURN_TX': 3, \n    'SITTING_DOWN_TX': 4, \n    'SEATED_TX': 5, \n    'STANDING_UP_TX': 6,\n    'WALKING_RX': 7,\n    'TURN_RX': 8,\n    'SITTING_DOWN_RX': 9,\n}\nLABELS = ['SEATED_RX','STANDING_UP_RX','WALKING_TX','TURNING_TX','SITTING_DOWN_TX', 'SEATED_TX', 'STANDING_UP_TX','WALKING_RX','TURNING_RX','SITTING_DOWN_RX']\nNUM_CLASSES = len(LABELS)\n\nTRAIN_IDS = ['e01_rx_tx', 'e01_tx_rx', 'e02_rx_tx', 'e02_tx_rx', 'e03_rx_tx', 'e03_tx_rx', 'e04_rx_tx', 'e04_tx_rx',\n             'e05_rx_tx', 'e05_tx_rx', 'e06_rx_tx', 'e06_tx_rx', 'e07_rx_tx', 'e07_tx_rx', 'e08_rx_tx', 'e08_tx_rx']\nTEST_IDS = ['e09_rx_tx', 'e09_tx_rx', 'e10_rx_tx', 'e10_tx_rx']\n\nBATCH_SIZE = 32\nEPOCHS = 50\nFOLDS = 10\n\ndef mlp_model():\n    set_seed()\n\n    model = keras.Sequential([\n        layers.Dense(128, activation='relu', input_shape=(500,)),\n        layers.Dense(1024, activation='relu'),\n        layers.Dense(1024, activation='relu'),\n        layers.Dense(1024, activation='relu'),\n        layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n\n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0005), metrics=['accuracy'])\n    return model\n\n\ndef cnn_model():\n    set_seed()\n\n    model = keras.Sequential([\n        layers.Conv2D(filters=128, kernel_size=(5,25), input_shape=(56, 50, 1)),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.MaxPooling2D(),\n        \n        layers.Flatten(),\n        \n        layers.Dense(512, activation='relu'),\n        layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n\n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n    return model\n\n\ndef model_builder(model_type):\n    if model_type == 'cnn':\n        return cnn_model\n    return mlp_model\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', help='data directory', type=str, required=True)\n    parser.add_argument('--reports_dir', help='directory to store the generated classification reports', type=str, required=True)\n    parser.add_argument('--model', help='optimize hyperparameters for selected model', type=str, choices=['mlp', 'cnn'])\n    args = parser.parse_args()\n\n    d1_windows, d1_labels = load_data(os.path.join(args.data_dir, 'D1'))\n    d1_labels_cat = ground_truth_to_categorical(d1_labels, MAPPING)    \n    x, y = combine_windows(d1_windows, d1_labels_cat)\n    \n    print(\"Starting 10-fold cross-validation\")\n    cv_reports = cross_validation(x, y, model_builder(args.model), FOLDS, BATCH_SIZE, EPOCHS, LABELS)\n    save_json(cv_reports, args.reports_dir, 'cv_report.json')\n\n    print(\"Starting D1T training and D1E evaluation\")\n    (x_d1t, y_d1t), (x_d1e, y_d1e) = split_train_test(d1_windows, d1_labels_cat, TRAIN_IDS, TEST_IDS)\n    model = model_builder(args.model)()\n    model.fit(x_d1t, y_d1t, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0)\n    report = evaluate_model(model, x_d1e, y_d1e, LABELS)\n    save_json(report, args.reports_dir, 'd1_report.json')\n\n    print(\"Starting D2, D3 and D4 evaluation\")\n    for eval_dataset in ['D2', 'D3', 'D4']:\n        windows, labels = load_data(os.path.join(args.data_dir, eval_dataset))\n        labels_cat = ground_truth_to_categorical(labels, MAPPING)    \n        x, y = combine_windows(windows, labels_cat)\n        report = evaluate_model(model, x, y, LABELS)\n        save_json(report, args.reports_dir, f'{eval_dataset.lower()}_report.json')\n\n\n\n\n\n\n\n\nVerification of the stability of the CSI signal\nThis section describes the methodology to determine if the CSI data is stable over time carrying out a simple experiment. To do so, a new data collection is executed minimizing the disturbance of the environment by external factors. Then, an evaluation procedure is designed to determine the similarity of CSI samples collected in different time frames using DL classification models.\n\nMethodology\nA dataset was collected using a TP-Link Archer C80 (one Tx antenna) and a SparkFun Thing Plus ESP32-S3 WROOM (one Rx antenna) connected to a computer. The Tx and Rx were placed on a table, separated by \\(1\\) meter in LOS condition.The Tx device was configured to work with the standard IEEE 802.11n in the channel \\(6\\). The Rx device was configured to connect to the Rx and extract Wi-Fi CSI data from HT-LTF subcarriers generated by ping traffic at \\(100\\)Hz.\nThe data collection consisted of capturing CSI data from an unaltered laboratory from the university for several days: from March \\(28^{th}\\) to April \\(1^{st}\\) \\(2024\\), coinciding with the Easter holidays. During these days, no external human factors would have disturbed the environment and thus, the CSI data. The collected CSI samples were labelled regarding the day they were collected (i.e., \\(03/29\\), \\(03/29\\), \\(03/30\\), \\(03/31\\), \\(04/01\\)).\nThe data preparation steps described in Data preparation with minor adaptations were applied to the dataset. More concretely, given the amount of the collected data (\\(24\\) GB), the windowing procedure was set to arrange windows of size \\(100\\) without overlapping.\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute this process is 01_4_lodo-dataset-processing.py.\n\n\nCode\n\"\"\"Data preprocessing script for LODO dataset.\n\nProcesses the raw data by aranging samples in windows and process them using DBSCAN for outlier detection\nand 2-level DWT for threshold based filtering.\n\n**Example**:\n\n    $ python 01_4_lodo-dataset-processing.py \n        --input_data_path &lt;PATH_OF_RAW_DATA&gt; \n        --windowed_data_path &lt;PATH_TO_STORE_RESULTS&gt;\n        --window_size &lt;WINDOW_SIZE&gt;\n\"\"\"\n\n\nimport argparse\nimport copy\nimport os\nimport sys\n\nsys.path.append(\"../../..\")\n\nimport numpy as np\n\nfrom alive_progress import alive_bar\nfrom libs.chapter5.pipeline.processing import proposed_method\n\nWINDOW_SIZE = 100\n\ndef create_windows(dataset, labels, window_size=100):\n    splits = np.arange(window_size, dataset.shape[1], window_size)\n    return np.array(np.split(dataset, splits, axis=1)[:-1]), np.array(np.split(labels, splits, axis=0)[:-1])[:,0]\n\n\ndef process_windows(windows):\n    windows_copy = copy.deepcopy(windows)\n    with alive_bar(len(windows_copy), title=f'Processing windows', force_tty=True, refresh_secs=5) as progress_bar:\n        for i, window in enumerate(i, windows_copy):\n            windows_copy[i] = proposed_method(window)\n            progress_bar()\n    return windows_copy\n\n\n\ndef save_windowed_data(data, labels, directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    np.save(os.path.join(directory, '{0}_x.npy'), data)\n    np.save(os.path.join(directory, '{0}_y.npy'), labels)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_data_path', help='Path of input data', type=str, required=True)\n    parser.add_argument('--windowed_data_path', help='Path to store windowed data', type=str, required=True)\n    parser.add_argument('--window_size', help='Window size', required=True, default=WINDOW_SIZE)\n    args = parser.parse_args()\n\n    amplitude_files = ['amplitudes_03_28.npy', 'amplitudes_03_29.npy', 'amplitudes_03_30.npy', 'amplitudes_03_31.npy', 'amplitudes_04_01.npy']\n    labels_files = ['labels_03_28.npy', 'labels_03_29.npy', 'labels_03_30.npy', 'labels_03_31.npy', 'labels_04_01.npy']\n\n    for amplitude_file, label_file in zip(amplitude_files, labels_files):\n        print(f'Processing dataset {amplitude_file}')\n        name = amplitude_file.split('_', 1)[1]\n        amplitudes = np.load(os.path.join(args.input_data_path, amplitude_file))\n        labels = np.load(os.path.join(args.input_data_path, label_file))\n\n        windows, windows_labels = create_windows(amplitudes, labels, args.window_size)\n\n        del amplitudes, labels\n\n        windows_processed = process_windows(windows)\n        save_windowed_data(windows_processed, windows_labels, os.path.join(args.windowed_data_path, name))\n\n\n\n\nAs HAR classifier, the model described in HAR classifier was employed, although with minor adaptations in some hyperparameters due to computational limitations caused by the high quantity of data. More concretely, the number of filters, batch size and epochs were set to \\(8\\), \\(512\\) and \\(30\\), respectively.\nFinally, the experimental procedure consisted of a \\(5\\)-fold cross-validation with the processed dataset. Each fold of the cross-validation corresponds to the data collected in one day, which can be named as Leaving-One-Day-Out (LODO). This procedure aims to evaluate how the models classify data from an unseen day, having two possible outputs:\n\nThe samples from a specific day are classified in any of the remaining days. In other words, a specific day’s samples are similar to those of any other day. These results would imply that the CSI data is stable over time.\nThe samples from the day \\(X_i\\) are classified in the day \\(X_{i-1}\\) or \\(X_{i+1}\\). In other words, samples from a specific day are similar only to the adjacent days (i.e., samples most close in time). These results would imply that the CSI data is not stable over time.\n\n\n\n\n\n\n\nNote\n\n\n\nThe script employed to execute this process is 03_3_lodo.py.\n\n\nCode\n\"\"\"Leaving-One-Day-Out validation script\n\nPerforms a Leaving-One-Day-Out evaluation on the LODO dataset.\n\n**Example**:\n\n    $ python 03_3_lodo.py \n        --data_dir &lt;PATH_OF_DATA&gt; \n        --reports_dir &lt;PATH_TO_STORE_REPORTS&gt;\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nsys.path.append(\"../../..\")\n\nimport numpy as np\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical\n\nfrom libs.chapter5.pipeline.ml import evaluate_model\nfrom libs.common.utils import save_json, set_seed\n\n\nMAPPING = {\n    '03/28': 0,\n    '03/29': 1,\n    '03/30': 2,\n    '03/31': 3,\n    '04/01': 4,\n}\n\nLABELS = ['03/28', '03/29', '03/30', '03/31', '04/01']\nNUM_CLASSES = len(LABELS)\n\nBATCH_SIZE = 512\nEPOCHS = 30\n\n\ndef build_model():\n    set_seed()\n\n    model = keras.Sequential([\n        layers.Conv2D(filters=8, kernel_size=(5,25), input_shape=(56, 100, 1)),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.MaxPooling2D(),\n        \n        layers.Flatten(),\n        \n        layers.Dense(512, activation='relu'),\n        layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n\n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n    return model\n\n\ndef train_models(datasets, labels, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1):\n    reports = []\n    \n    for i in range(len(datasets)):\n        training_datasets = [datasets[j] for j in range(len(datasets)) if j != i]\n        training_labels = [labels[j] for j in range(len(labels)) if j != i]\n        print(f'Training with: {training_labels}')\n        print(f'Testing with: {labels[i]}')\n    \n        x_train = np.vstack(training_datasets)\n        y_train = one_hot_encoding(np.concatenate(training_labels), MAPPING)\n\n        x_test = datasets[i]\n        y_test = one_hot_encoding(labels[i], MAPPING)\n\n        model = build_model()\n        model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=verbose)\n        report = evaluate_model(model, x_test, y_test, LABELS)\n        reports.append(report)\n\n        del x_train\n        del y_train\n        del x_test\n        del y_test\n        del training_datasets\n        del training_labels\n        del model\n    \n    return reports\n\n\ndef one_hot_encoding(y, mapping):\n    return to_categorical(list(map(lambda i: mapping[i], y)), num_classes=len(mapping.keys()))    \n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', help='data directory', type=str, required=True)\n    parser.add_argument('--reports_dir', help='directory to store the generated classification reports', type=str, required=True)\n    args = parser.parse_args()\n\n    windows = []\n    labels = []\n\n    for day in ['03_28', '03_29', '03_30', '03_31', '04_01']:\n        windows.append(np.load(os.path.join(args.data_dir), f'{day}_x.npy'))\n        labels.append(np.load(os.path.join(args.data_dir), f'{day}_y.npy'))\n\n    reports = train_models(windows, labels)\n\n    save_json(reports, args.reports_dir, 'reports.json')",
    "crumbs": [
      "Wi-Fi CSI based HAR"
    ]
  },
  {
    "objectID": "05_wifi-csi.html#results",
    "href": "05_wifi-csi.html#results",
    "title": "Looking into the Future: Wi-Fi CSI based HAR",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\n\n\nLocalized HAR based on Wi-Fi CSI\n\n\nPresents the preliminary results on localized HAR using Wi-Fi CSI.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation of employed methods\n\n\nValidates the employed methods in the previous sections to discard them as causes of the poor results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal stability of Wi-Fi CSI data from ESP32 microcontrollers\n\n\nAnalyses the temporal stability of the Wi-Fi CSI data collected with ESP32 microcontrollers.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nAlsaify, Baha A, Mahmoud M Almazari, Rami Alazrai, and Mohammad I Daoud. 2020. “A Dataset for Wi-Fi-Based Human Activity Recognition in Line-of-Sight and Non-Line-of-Sight Indoor Environments.” Data in Brief 33: 106534. https://doi.org/10.1016/j.dib.2020.106534.\n\n\nChoi, Hyuckjin, Manato Fujimoto, Tomokazu Matsui, Shinya Misaki, and Keiichi Yasumoto. 2022. “Wi-CaL: WiFi Sensing and Machine Learning Based Device-Free Crowd Counting and Localization.” IEEE Access 10: 24395–410. https://doi.org/10.1109/ACCESS.2022.3155812.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. 1996. “A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.” In Kdd, 96:226–31. 34.\n\n\nMa, Yongsen, Gang Zhou, and Shuangquan Wang. 2019. “WiFi Sensing with Channel State Information: A Survey.” ACM Comput. Surv. 52 (3): 1–36. https://doi.org/10.1145/3310194.\n\n\nMatey-Sanz, Miguel, Joaquín Torres-Sospedra, and Adriano Moreira. 2023. “Temporal Stability on Human Activity Recognition Based on Wi-Fi CSI.” In 2023 13th International Conference on Indoor Positioning and Indoor Navigation (IPIN), 1–6. https://doi.org/10.1109/IPIN57070.2023.10332214.\n\n\nYousefi, Siamak, Hirokazu Narui, Sankalp Dayal, Stefano Ermon, and Shahrokh Valaee. 2017. “A Survey on Behavior Recognition Using WiFi Channel State Information.” IEEE Communications Magazine 55 (10): 98–104. https://doi.org/10.1109/MCOM.2017.1700082.",
    "crumbs": [
      "Wi-Fi CSI based HAR"
    ]
  }
]