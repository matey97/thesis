# Copyright 2024 Miguel Matey Sanz
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

'''
Provides functions to process the reports generated in the evaluation of classification models.
'''

import numpy as np
import pandas as pd


def _from_dict(report):
    accuracy = report["accuracy"]
    precision = report['classification_report']['weighted avg']['precision']
    recall = report['classification_report']['weighted avg']['recall']
    f1 = report['classification_report']['weighted avg']['f1-score']
    return accuracy, precision, recall, f1


def _from_list(reports):
    accuracies = []
    precisions = []
    recalls = []
    f1s = []

    for report in reports:
        accuracies.append(report['accuracy'])
        precisions.append(report['classification_report']['weighted avg']['precision'])
        recalls.append(report['classification_report']['weighted avg']['recall'])
        f1s.append(report['classification_report']['weighted avg']['f1-score'])

    return np.mean(accuracies), np.mean(precisions), np.mean(recalls), np.mean(f1s)


def extract_metrics(report):
    '''
    Extracts metrics of interest from a classification report

    Args:
        report (dict|list[dict]): Classification report or a list of reports.

    Returns:
        (float): Accuracy obtained by the model.
        (float): Precision obtained by the model.
        (float): Recall obtained by the model.
        (float): F1-score obtained by the model.
    '''

    return _from_dict(report) if isinstance(report, dict) else _from_list(report)


def metrics_summary(reports, labels):
    '''
    Creates a summary of the classification reports generated by the evaluation of several models.

    Args:
        reports (list[dict]): List of classification reports.

    Returns:
        (`pandas.DataFrame`): DataFrame summarizing the results.
    '''

    summary = {
        'Accuracy': [],
        'Precision': [],
        'Recall': [],
        'F1-score': []
    }
    
    for report in reports:
        accuracy, precision, recall, f1 = extract_metrics(report)
        summary['Accuracy'].append(accuracy)
        summary['Precision'].append(precision)
        summary['Recall'].append(recall)
        summary['F1-score'].append(f1)
    
    df = pd.DataFrame(summary, index=labels)
    return df

        
def _metric_increment(report_a, report_b):
    def compute_increment(old, new):
        return (new - old) / (old) * 100
    
    accuracy_a, precision_a, recall_a, f1_a = extract_metrics(report_a)
    accuracy_b, precision_b, recall_b, f1_b = extract_metrics(report_b)
    return compute_increment(accuracy_a, accuracy_b), compute_increment(precision_a, precision_b), compute_increment(recall_a, recall_b), compute_increment(f1_a, f1_b)


def metric_increment_summary(comparisons):
    '''
    Creates a summary with metrics increments/decrements in the provided reports to compare
    different approaches

    Args:
        comparisons (dict): Dict where each entry indicates which reports to compare.

    Returns:
        (`pandas.DataFrame`): DataFrame containing the comparisons.
    '''

    increments = {
        'Accuracy': [],
        'Precision': [],
        'Recall': [],
        'F1-score': []
    }
    
    for (report_a, report_b) in comparisons.values():
        acc_inc, prec_inc, rec_inc, f1_inc = _metric_increment(report_a, report_b)
        increments['Accuracy'].append(acc_inc)
        increments['Precision'].append(prec_inc)
        increments['Recall'].append(rec_inc)
        increments['F1-score'].append(f1_inc)
        
    df = pd.DataFrame(increments, index=list(comparisons.keys()))
    return df